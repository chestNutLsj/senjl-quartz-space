# 附录A　术语表

本附录定义了书中部分关键术语。

**激活函数****（activation function）** 在人工神经网络中转换神经元输出的函数，通常是为了提供非线性变换处理能力或保证将输出值限制在一定范围内（第7章）。

**无环图****（acyclic）** 没有环路的图（第4章）。

**可接受的启发****（admissible heuristic）** A*搜索算法的启发式算法，绝不高估抵达目标的成本（第2章）。

**人工神经网络****（artificial neural network）** 用计算工具仿真生物神经网络，以解决那些难以简化为传统算法适用形式的难题。请注意，人工神经网络的操作通常与生物学意义上的神经网络存在明显的差异（第7章）。

**自动结果缓存****（auto-memoization）** 在语言层级实现的结果缓存，其中保存着不会有副作用的函数调用结果，以供后续的相同调用时检索（第1章）。

**反向传播****（backpropagation）** 一种用来训练神经网络得出权重的技术，基于正确输出已知的一组输入来完成。这里用偏导数计算权重对实际结果与预期结果之误差所承担的“责任”。这些delta将用于修正后续训练中的权重（第7章）。

**回溯****（backtracking）** 在搜索问题中，碰到障碍后就回到之前的决策点（转向与前一次不同的方向）（第3章）。

**位串****（bit string）** 一种数据结构，存储的是1和0组成的序列，每个序列值用1位内存表示。有时也被称作位矢量（bit vector）或位数组（bit array）（第1章）。

**形心****（centroid）** 聚类的中心点。通常，该点每个维度的值都是其他所有点在此维度的均值（第6章）。

**染色体****（chromosome）** 在遗传算法中，种群中的个体被称为染色体（第5章）。

**聚类簇****（cluster）** 参见聚类（第6章）。

**聚类****（clustering）** 一种无监督学习技术，将一个数据集划分为由相关点构成的多个小组，这些小组被称作聚类簇（第6章）。

**密码子****（codon）** 组成氨基酸的3种核苷酸的组合（第2章）。

**压缩****（compression）** 对数据进行编码（改变格式）以减少占用空间（第1章）。

**连通****（connected）** 图的一种属性，表明任一顶点都存在到其他任何顶点的路径（第4章）。

**约束****（constraint）** 为解决约束满足问题而必须满足的条件（第3章）。

**交换****（crossover）** 在遗传算法中，将种群中的个体组合在一起创造出后代，这些后代是其父母的混合体，并将组成下一代种群（第5章）。

**CSV**　一种文本交换格式，每行数据中的值以逗号分隔，行与行之间通常由换行符分隔。CSV的意思是逗号分隔的值（comma-separated value）。CSV是从电子表格和数据库中导出的数据的常见格式（第7章）。

**环****（cycle）** 图的路径，在没有回溯的情况下同一个顶点会被访问两次（第4章）。

**解压缩****（decompression）** 压缩过程的逆操作，将数据恢复为原格式（第1章）。

**深度学习****（deep learning）** 一句流行语，任何一种用高级机器学习算法分析大数据的技术都可被认为是深度学习。最常见的深度学习是用多层人工神经网络求解大数据集应用问题（第7章）。

**delta**　表示神经网络中权重的预期值与实际值之间的差距的一个值。预期值由数据的训练和反向传播进行确定（第7章）。

**有向图****（digraph）** 参见有向图（directed graph）（第4章）。

**有向图****（directed graph）** 也称作digraph，有向图的边只能朝一个方向遍历（第4章）。

**值域****（domain）** 约束满足问题中变量的可能取值范围（第3章）。

**动态规划****（dynamic programming）** 动态规划不采用蛮力法直接解决大型问题，而是把大型问题分解为更可控的小型子问题（第9章）。

**边****（edge）** 图中两个顶点（节点）之间的连接（第4章）。

**异或****（exclusive or）** 参见XOR（第1章）。

**前馈****（feed-forward）** 一种神经网络，信号在其中朝一个方向传播（第7章）。

**适应度函数****（fitness function）** 一种评分函数，对问题可能的解进行效果评价（第5章）。

**代****（generation）** 遗传算法中的一轮计算，也用于表示一轮计算过程中受激活个体组成的种群（第5章）。

**遗传编程****（genetic programming）** 运用选择、交换和变异操作符进行自我修改的程序，以便求解解法不明显的编程问题（第5章）。

**梯度下降****（gradient descent）** 用反向传播时计算出来的delta和学习率，修改人工神经网络权重的方法（第7章）。

**图****（graph）** 一种抽象的数学结构，通过将问题划分为一组相互连通的节点来对现实世界的问题进行建模。这些节点被称为顶点，顶点间的连接被称为边（第4章）。

**贪婪算法****（greedy algorithm）** 一种在任一决策点都选择最优直接选项的算法，以期能导出全局的最优解（第4章）。

**启发式算法****（heuristic）** 一种关于问题求解路径的直觉，认为该路径指向正确的方向（第2章）。

**隐藏层****（hidden layer）** 在前馈人工神经网络中，所有位于输入层和输出层之间的层（第7章）。

**无限循环****（infinite loop）** 不会终止的循环（第1章）。

**无限递归****（infinite recursion）** 不会终止的递归调用，而是持续发起新的递归调用。这类似于无限循环。通常是因为缺少基线条件引起的（第1章）。

**输入层****（input layer）** 前馈人工神经网络的第一层，接收来自某种外部实体的输入（第7章）。

**学习率****（learning rate）** 通常是一个常数，用于根据计算得出的delta调整人工神经网络权重的修改率（第7章）。

**结果缓存****（memoization）** 一种将计算任务的结果保存起来的技术，以供后续从内存中读取，从而节省为重新生成相同结果而额外耗费的计算时间（第1章）。

**最小生成树****（minimum spanning tree）** 连接所有顶点的生成树，使得所有边的总权重最低（第4章）。

**变异****（mutate）** 在遗传算法中，当个体被放入下一代种群之前随机改变该个体的某些属性（第5章）。

**自然选择****（natural selection）** 生物优胜劣汰的进化过程。给定有限的环境资源，最善于利用这些资源的生物将会存活并繁衍。经过几代之后，就会让有利的特征在种群中扩散，由此环境约束就做出了自然选择（第5章）。

**神经网络****（neural network）** 由多个神经元构成的网络，神经元相互协同进行信息处理。这些神经元通常视作分层组织（第7章）。

**神经元****（neuron）** 神经细胞个体，正如人类大脑中的神经细胞（第7章）。

**归一化****（normalization）** 让不同类型的数据具有可比性的过程（第6章）。

**NP困难问题****（NP-hard problem）** 一类没有已知的多项式时间算法能够求解的问题（第9章）。

**核苷酸****（nucleotide）** DNA的4种碱基（腺嘌呤（A）、胞嘧啶（C）、鸟嘌呤（G）和胸腺嘧啶（T）之一的实例（第2章）。

**输出层****（output layer）** 前馈人工神经网络中的最后一层，用于对给定输入和问题确定神经网络的求解结果（第7章）。

**路径****（path）** 连接图中两个顶点的边的集合（第4章）。

**层****（ply）** 在双人游戏中的一个回合（通常可被视为一步）（第8章）。

**种群****（population）** 在遗传算法中，种群是多个个体的集合（每个种群都代表问题可能的解），这些个体相互竞争以期求解问题（第5章）。

**优先队列****（priority queue）** 基于“优先级”顺序弹出数据项的数据结构。例如，为了首先响应最高优先级的电话，优先队列可以与紧急电话数据集一起使用（第2章）。

**队列****（queue）** 一种抽象数据结构，保证先进先出（First-In-First-Out，FIFO）的顺序。队列的实现代码至少应提供压入操作和弹出操作，分别用于添加和移除元素（第2章）。

**递归函数****（recursive function）** 调用自己的函数（第1章）。

**选择****（selection）** 在遗传算法的一代运算中，为了繁殖而选择个体的过程，以创造下一代中的个体（第5章）。

**sigmoid****函数****（sigmoid function）** 流行的激活函数之一，用于人工神经网络。名为sigmoid的函数始终会返回介于0到1之间的值。它还有助于确保神经网络能把超出线性变换的结果表示出来（第7章）。

**SIMD****指令****（SIMD instruction）** 为矢量计算做过优化的微处理器指令，有时也称为矢量指令。SIMD代表单指令多数据（single instruction，multiple data）（第7章）。

**生成树****（spanning tree）** 连接图中每个顶点的树（第4章）。

**栈****（stack）** 一种抽象数据结构，保证后进先出的顺序（Last-In-First-Out，LIFO）。栈的实现代码至少应提供压入操作和弹出操作，分别用于添加和移除元素（第2章）。

**监督学习****（supervised learning）** 机器学习技术中的算法或多或少需要外部资源的指导才能得出正确解（第7章）。

**突触****（synapse）** 神经元之间的间隙，神经递质充斥其中用以传导电流。用非专业的话说，这些就是神经元之间的连接（第7章）。

**训练****（training）** 人工神经网络在训练阶段利用反向传播调整权重，用到的是某些给定输入的已知正确输出（第7章）。

**树****（tree）** 任意两个顶点之间只有一条路径的图。树是无环（acyclic）图（第4章）。

**无监督学习****（****unsupervised learning****）** 不用先验知识（foreknowledge）即可得出结论的机器学习技术，换句话说，这种技术无须指导而是自行运行（第6章）。

**变量****（variable）** 在约束满足问题的上下文中，变量是必须作为解的一部分并求出的参数。变量的可能取值范围即为值域（domain）。解必须满足一条或多条约束条件（第3章）。

**顶点****（vertex）** 图的一个节点（第4章）。

**XOR**　一种逻辑位操作，只要有一个操作数为true就返回true，但两个操作数都为true或都不为true时则返回false。此缩写表示异或。在Python语言中，用运算符“^”表示XOR（第1章）。

**z分数****（z-score）** 数据点与数据集均值之间的距离，以标准差为计数单位（第6章）。