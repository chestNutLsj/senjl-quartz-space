## Coping with UnicodeEncodeError

Most non-UTF codecs handle only a small subset of the Unicode characters. When converting text to bytes, if a character is not defined in the target encoding, `UnicodeEncodeError` will be raised, unless special handling is provided by passing an `errors` argument to the encoding method or function. The behavior of the error handlers is shown in [Example 4-5](#ex_encoding).

##### Example 4-5. Encoding to bytes: success and error handling

```
>>> 
```

[![^1]

The UTF encodings handle any `str`.

[![^2]

`iso8859_1` also works for the `'São Paulo'` string.

[![^3]

`cp437` can’t encode the `'ã'` (“a” with tilde). The default error handler—`'strict'`—raises `UnicodeEncodeError`.

[![^4]

The `error='ignore'` handler skips characters that cannot be encoded; this is usually a very bad idea, leading to silent data loss.

[![^5]

When encoding, `error='replace'` substitutes unencodable characters with `'?'`; data is also lost, but users will get a clue that something is amiss.

[![^6]

`'xmlcharrefreplace'` replaces unencodable characters with an XML entity. If you can’t use UTF, and you can’t afford to lose data, this is the only option.

###### Note

The `codecs` error handling is extensible. You may register extra strings for the `errors` argument by passing a name and an error handling function to the `codecs.register_error` function. See [the `codecs.register_error` documentation](https://fpy.li/4-6).

ASCII is a common subset to all the encodings that I know about, therefore encoding should always work if the text is made exclusively of ASCII characters. Python 3.7 added a new boolean method [`str.isascii()`](https://fpy.li/4-7) to check whether your Unicode text is 100% pure ASCII. If it is, you should be able to encode it to bytes in any encoding without raising `UnicodeEncodeError`.