## Extreme “Normalization”: Taking Out Diacritics

The Google Search secret sauce involves many tricks, but one of them apparently is ignoring diacritics (e.g., accents, cedillas, etc.), at least in some contexts. Removing diacritics is not a proper form of normalization because it often changes the meaning of words and may produce false positives when searching. But it helps coping with some facts of life: people sometimes are lazy or ignorant about the correct use of diacritics, and spelling rules change over time, meaning that accents come and go in living languages.

Outside of searching, getting rid of diacritics also makes for more readable URLs, at least in Latin-based languages. Take a look at the URL for the Wikipedia article about the city of São Paulo:

https://en.wikipedia.org/wiki/S%C3%A3o_Paulo

The `%C3%A3` part is the URL-escaped, UTF-8 rendering of the single letter “ã” (“a” with tilde). The following is much easier to recognize, even if it is not the right spelling:

https://en.wikipedia.org/wiki/Sao_Paulo

To remove all diacritics from a `str`, you can use a function like [Example 4-14](#ex_shave_marks).

##### Example 4-14. simplify.py: function to remove all combining marks

```
import
```

[![^1]

Decompose all characters into base characters and combining marks.

[![^2]

Filter out all combining marks.

[![^3]

Recompose all characters.

[Example 4-15](#ex_shave_marks_demo) shows a couple of uses of `shave_marks`.

##### Example 4-15. Two examples using `shave_marks` from [Example 4-14](#ex_shave_marks)

```
>>> 
```

[![^1]

Only the letters “è”, “ç”, and “í” were replaced.

[![^2]

Both “έ” and “é” were replaced.

The function `shave_marks` from [Example 4-14](#ex_shave_marks) works all right, but maybe it goes too far. Often the reason to remove diacritics is to change Latin text to pure ASCII, but `shave_marks` also changes non-Latin characters—like Greek letters—which will never become ASCII just by losing their accents. So it makes sense to analyze each base character and to remove attached marks only if the base character is a letter from the Latin alphabet. This is what [Example 4-16](#ex_shave_marks_latin) does.

##### Example 4-16. Function to remove combining marks from Latin characters (import statements are omitted as this is part of the simplify.py module from [Example 4-14](#ex_shave_marks))

```
def
```

[![^1]

Decompose all characters into base characters and combining marks.

[![^2]

Skip over combining marks when base character is Latin.

[![^3]

Otherwise, keep current character.

[![^4]

Detect new base character and determine if it’s Latin.

[![^5]

Recompose all characters.

An even more radical step would be to replace common symbols in Western texts (e.g., curly quotes, em dashes, bullets, etc.) into `ASCII` equivalents. This is what the function `asciize` does in [Example 4-17](#ex_asciize).

##### Example 4-17. Transform some Western typographical symbols into ASCII (this snippet is also part of simplify.py from [Example 4-14](#ex_shave_marks))

```
single_map
```

[![^1]

Build mapping table for char-to-char replacement.

[![^2]

Build mapping table for char-to-string replacement.

[![^3]

Merge mapping tables.

[![^4]

`dewinize` does not affect `ASCII` or `latin1` text, only the Microsoft additions to `latin1` in `cp1252`.

[![^5]

Apply `dewinize` and remove diacritical marks.

[![^6]

Replace the Eszett with “ss” (we are not using case fold here because we want to preserve the case).

[![^7]

Apply NFKC normalization to compose characters with their compatibility code points.

[Example 4-18](#ex_asciize_demo) shows `asciize` in use.

##### Example 4-18. Two examples using `asciize` from [Example 4-17](#ex_asciize)

```
>>> 
```

[![^1]

`dewinize` replaces curly quotes, bullets, and ™ (trademark symbol).

[![^2]

`asciize` applies `dewinize`, drops diacritics, and replaces the `'ß'`.

###### Warning

Different languages have their own rules for removing diacritics. For example, Germans change the `'ü'` into `'ue'`. Our `asciize` function is not as refined, so it may or not be suitable for your language. It works acceptably for Portuguese, though.

To summarize, the functions in _simplify.py_ go way beyond standard normalization and perform deep surgery on the text, with a good chance of changing its meaning. Only you can decide whether to go so far, knowing the target language, your users, and how the transformed text will be used.

This wraps up our discussion of normalizing Unicode text.

Now let’s sort out Unicode sorting.