# Delegating Tasks to Executors

One important advantage of Node.js over Python for asynchronous programming is the Node.js standard library, which provides async APIs for all I/O—not just for network I/O. In Python, if you’re not careful, file I/O can seriously degrade the performance of asynchronous applications, because reading and writing to storage in the main thread blocks the event loop.

In the `download_one` coroutine of [Example 21-6](#flags2_asyncio_top), I used this line to save the downloaded image to disk:

        `await` `asyncio``.``to_thread``(``save_flag``,` `image``,` `f``'{cc}.gif'``)`

As mentioned before, the `asyncio.to_thread` was added in Python 3.9. If you need to support 3.7 or 3.8, then replace that single line with the lines in [Example 21-10](#flags2_asyncio_executor_fragment).

##### Example 21-10. Lines to use instead of `await asyncio.to_thread`

```
        
```

[![^1]

Get a reference to the event loop.

[![^2]

The first argument is the executor to use; passing `None` selects the default `ThreadPoolExecutor` that is always available in the `asyncio` event loop.

[![^3]

You can pass positional arguments to the function to run, but if you need to pass keyword arguments, then you need to resort to `functool.partial`, as described in the [`run_in_executor` documentation](https://fpy.li/21-22).

The newer `asyncio.to_thread` function is easier to use and more flexible, as it also accepts keyword arguments.

The implementation of `asyncio` itself uses `run_in_executor` under the hood in a few places. For example, the `loop.getaddrinfo(…)` coroutine we saw in [Example 21-1](#blogdom_ex) is implemented by calling the `getaddrinfo` function from the `socket` module—which is a blocking function that may take seconds to return, as it depends on DNS resolution.

A common pattern in asynchronous APIs is to wrap blocking calls that are implementation details in coroutines using `run_in_executor` internally. That way, you provide a consistent interface of coroutines to be driven with `await`, and hide the threads you need to use for pragmatic reasons. The [Motor](https://fpy.li/21-23) asynchronous driver for MongoDB has an API compatible with `async/await` that is really a façade around a threaded core that talks to the database server. A. Jesse Jiryu Davis, the lead developer of Motor, explains his reasoning in [“Response to ‘Asynchronous Python and Databases’”](https://fpy.li/21-24). Spoiler: Davis discovered that a thread pool was more performant in the particular use case of a database driver—despite the myth that asynchronous approaches are always faster than threads for network I/O.

The main reason to pass an explict `Executor` to `loop.run_in_executor` is to employ a `ProcessPoolExecutor` if the function to execute is CPU intensive, so that it runs in a different Python process, avoiding contention for the GIL. Because of the high start-up cost, it would be better to start the `ProcessPoolExecutor` in the `supervisor`, and pass it to the coroutines that need to use it.

Caleb Hattingh—the author of [_Using Asyncio in Python_](https://fpy.li/hattingh) (O’ Reilly)—is one of the tech reviewers of this book and suggested I add the following warning about executors and _asyncio_.

# Caleb’s Warning about run_in_executors

Using `run_in_executor` can produce hard-to-debug problems since cancellation doesn’t work the way one might expect. Coroutines that use executors give merely the pretense of cancellation: the underlying thread (if it’s a `ThreadPoolExecutor`) has no cancellation mechanism. For example, a long-lived thread that is created inside a `run_in_executor` call may prevent your _asyncio_ program from shutting down cleanly: `asyncio.run` will wait for the executor to fully shut down before returning, and it will wait forever if the executor jobs don’t stop somehow on their own. My greybeard inclination is to want that function to be named `run_in_executor_uncancellable`.

We’ll now go from client scripts to writing servers with `asyncio`.