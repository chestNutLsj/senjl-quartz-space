
## 摘要

近年来，通过在大规模语料库上对 Transformer 模型进行预训练，人们提出了预训练语言模型（Pre-training Language Model, PLM），其在解决各种自然语言处理（Natural Language Processing, NLP）任务方面表现出强大的能力。由于研究人员发现扩展模型规模可以提高模型能力，因此他们通过将参数增加到更大的 尺寸来进一步研究该效应。有趣的是，当参数规模超过一定水平时，这些规模更大的语言模型的性能不仅得到了显著提升，而且还表现出一些小规模语言模型（例如 BERT）所不具备的特殊能力（例如上下文学习）。为了区分不同参数规模下的语言模型，研究界创造了术语——大语言模型（Large Language Model, LLM）代指大型的 PLM（如包含数百亿或数千亿个参数）。

## 发展历史

### 从 SLM 到 LLM

- **统计语言模型** (SLM)： SLMs 基于统计学习方法开发，并在 20 世纪 90 年代兴起。其基本思想是基于马尔可夫假设建立词预测模型，例如根据最近的上下文预测下一个词。具有固定上下文长度 n 的 SLM 也称为 n 元语言模型，例如 bigram 和 trigram 语言模型。SLM 已被广泛应用于提高信息 检索（IR）和自然语言处理（NLP）的任务性能。然而，它们通常受到维数灾难的困扰：由于需要估计指数级数量的转换概率，因此很难准确估计高阶语言模型。因此， 专门设计的平滑策略，如回退估计和古德图灵估计已被引入以缓解数据稀疏问题。
- **神经语言模型** (NLM)： NLM 通过神经网络，如循环神经网络（RNN），来描述单词序列的概率。作为一个显著贡献，Y. Bengio的工作[^1]引入了词的分布式表示这一概念，并在聚合上下文特征（即分布式词向量）的条件下构建词预测函数。通过扩展学习词或句子有效特征的想法，已有研究开发了一种通用神经网络方法来为各种 NLP 任务构建统一解决方案。此外，word2vec 提出了构建一个简化的浅层神经网络来学习分布式单词表示的方法，这些表示在各种 NLP 任务中被证明非常有效。这些研究开创了将语言模型用于表示学习（超越词序列建模）的应用，对 NLP 领域产生了重要影响
- **预训练语言模型** (PLM)：作为早期尝试，ELMo 被提出来通过预训练一个双向 LSTM（biLSTM）网络（而不是学习固定的词表示）来捕捉上下文感知的词表示，然后根据特定的下游任务微调 biLSTM 网络。进一步，基于自注意力机制的高度并行化 Transformer 架构，BERT 作为双向语言模型，在大规模无标签语料库上使用专门设计的预训 练任务。这些预训练的上下文感知词表示作为通用语义特征 非常有效，其极大地提高了 NLP 任务的性能。这项研究激发 了大量后续工作，==确立了“预训练和微调”学习范式==。遵循这一范式，已经建立了大量关于 PLM 的研究，这些研究引入 了不同的架构 [24, 25]（例如 GPT-2 [26] 和 BART [24]）或者 改进的预训练策略 [27–29]。在这个范式中，通常需要对 PLM 进行微调以适配不同的下游任务。
- 大语言模型 (LLM)：研究人员发现，扩展 PLM（例如 扩展模型大小或数据大小）通常会提高下游任务的模型性能（即遵循扩展法则 [30]）。许多研究通过训练越来越大的 PLM （例如 1750 亿参数的 GPT-3 和 5400 亿参数的 PaLM）来探 索性能极限。尽管扩展主要在模型大小方面进行（使用类似 的架构和预训练任务），但这些大规模的 PLM 与较小的 PLM （例如 3.3 亿参数的 BERT 和 15 亿参数的 GPT-2）表现出不同的行为，并在解决一系列复杂任务中展示了惊人的能力（称为涌现能力）。例如，GPT-3 可以通过上下文学习（in-context learning, ICL）来解决小样本任务，而 GPT-2 则表现不佳。因 此，研究界将这些大规模的 PLM 命名为“大语言模型” [31-34]。作为 LLM 的一个出色应用，ChatGPT2 将 GPT 系列的 LLM 应用于对话，展现出惊人的与人类对话的能力。

### PLM 与 LLM 之间的主要区别

1. 首先，LLM 表现出一些令人惊讶的涌现能力，这些能力 可能在以前较小的 PLM 中没有观察到。这些能力是 LM 在 复杂任务上表现的关键，它使得人工智能算法具有前所未有 的强大和有效性。
2. 其次，LLM 将彻底改变人类开发和使用人 工智能算法的方式。与小型 PLM 不同，访问 LLM 的主要方 法是通过提示接口（例如 GPT-4 API）。人们必须了解 LLM 的工作原理，并以 LLM 能够遵循的方式形式化他们的任务。
3. 第三，LLM 的发展不再明确区分研究和工程。训练 LLM 需要在大规模数据处理和分布式并行训练方面具有丰富的实践经验。为了开发出有能力的 LLM，研究人员必须解决复杂的工程问题，他们需要与工程师合作或成为工程师。

### LLM 对 AI 研究的影响

在 NLP 领域，LLM 可以在一定程度上作为通用语 言任务解决器，研究范式已经转向使用 LLM。在 IR 领域，传 统搜索引擎正受到通过 AI 聊天机器人（即 ChatGPT）搜索 新信息的挑战，而 New Bing 展示了一个初步的基于 LLM 增 强搜索结果的研究尝试。在计算机视觉（CV）领域，研究人 员试图开发类似 ChatGPT 的视觉-语言模型，以更好地为多模态对话提供服务 [41–44]，GPT-4 [45] 已经通过整合视觉信 息来支持多模态输入。这一新技术浪潮可能会带来一个基于 LLM 的实际应用的繁荣生态系统。例如，Microsoft 365 正在 利用 LLM（即 Copilot）来自动化办公工作，而 OpenAI 支持 在 ChatGPT 中使用插件来实现特殊功能。

### 关于 LLM 的疑惑

1. 首先，为什么涌现能力会出现在 LLM 中，而 不是较小的 PLM 中，仍然是难以解释的。并且，一个更普 遍的问题是研究界缺乏对 LLM 优越能力的关键因素进行深 入、详细的研究调查。因此，研究 LLM 何时以及如何获得这 些能力非常重要 [46]。尽管对这个问题已有一些有意义的讨 论 [46, 47]，但仍需要更多原则性的研究来揭示 LLM 的“秘 密”。
2. 其次，研究界很难训练出有能力的 LLM。由于计算资 源的巨大需求，为了研究训练 LLM 的各种策略的效果，进行 重复、消融研究的成本非常高。实际上，LLM 主要由工业界 训练，许多重要的训练细节（如数据收集和清理）并未向公 众透露。
3. 第三，将 LLM 与人类价值观或偏好保持一致是具有 挑战性的。LLM 尽管具有出色的能力，但是其也可能生成有 害、虚构或具有负面影响的内容。因此，需要有效和高效的控 制方法来消除使用 LLM 的潜在风险。

## 概述

### LLM 的背景

通常，LLM 是指包含数千亿（或更多）参数的 Transformer 语言模型 4，这些模型是在大规模文本数据上进行训练的 [31]， 例如 GPT-3 [55]，PaLM [56]，Galactica [34] 和 LLaMA [57]。 LLM 展现了理解自然语言和解决复杂任务（通过文本生成） 的强大能力。为了对 LLM 的工作原理有一个快速的了解，本 部分将介绍 LLM 的基本背景，包括扩展法则、涌现能力和关 键技术。

#### 扩展法则

目前，LLM 主要建立在 Transformer 架构上 [22]，其中多头注意力层堆叠在非常深的神经网络中。 现有的 LLM 采用类似的 Transformer 架构和与小型语言模型 相同的预训练目标（如语言建模）。然而，**LLM 大幅度扩展 了模型规模、数据规模和总计算量（数量级）**。大量研究表明， 扩展可以大幅提高 LLM 的模型能力 [26, 55, 56]。因此，建立 一个定量的方法来描述扩展效应是有意义的。接下来，我们 介绍两个 Transformer 语言模型的代表性扩展法则 [30, 33]。

1. **$KM$ 扩展法则**：2020 年，Kaplan 等人 [30]（OpenAI 团队）首次提出了神经语言模型的性能与模型规模（N ）、数 据集规模（D）和训练计算量（C ）之间的幂律关系。在给定 计算预算 c 的条件下，他们依据实验提出了三个基本公式来描述扩展法则：
$$
\begin{aligned}
L(N)&=\left(\frac{N_{e}}{N}\right)^{\alpha_{N}},\alpha_{N}\sim 0.076,N_{e}\sim 8.8\times10^{13}\\
L(D)&=\left(\frac{D_{e}}{D}\right)^{\alpha_{D}},\alpha_{D}\sim 0.095,D_{e}\sim 5.4\times10^{13}\\
L(C)&=\left(\frac{C_{e}}{C}\right)^{\alpha_{C}},\alpha_{C}\sim 0.050,C_{e}\sim 8.8\times10^{13}\\
\end{aligned}
$$
这里，$L (·)$ 表示用 nats 表示的交叉熵损失。这三个规律是通 过拟合模型在不同数据大小（2200 万到 230 亿个 token）、模 型大小（7.68 亿到 15 亿个非嵌入参数）和训练计算量下的性 能得出的，同时做出了一些假设（如一个因素的分析不会受 到其他两个因素的限制）。结果表明，模型性能与这三个因素 存在着强依赖关系。

2. **$Chinchilla$ 扩展法则**：作为另一代表性研究，Hoffmann 等人 [33]（Google DeepMind 团队）提出了一种扩展法则的替 代形式来指导 LLM 最优计算量的训练。他们通过变化更大范 围的模型大小（7000 万到 160 亿个参数）和数据大小（50 亿到 5000 亿个 token）进行了严格的实验，并拟合了一个类似 的扩展法则，但具有不同的系数，如下所示 [33]：
$$
L(N,D)=E+ \frac{A}{N^{\alpha}}+ \frac{B}{D^{\beta}}
$$
其中 E = 1.69, A = 406.4, B = 410.7，α = 0.34 和 β = 0.28。 通过在约束条件 C ≈ 6N D 下优化损失 L (N, D)，他们展示 了将计算预算最优地分配给模型大小和数据大小的方法（如 下）：
$$
N_{opt}(C)=G(\frac{C}{6})^\alpha,D_{opt}(C)=G^{-1}(\frac{C}{6})^{b}
$$
这里... G 是由 A、B、α 和 β 计算得出 的扩展系数。正如 [33] 中所分析的那样，随着给定计算预算 的增加，**KM 扩展法则更偏向于将更大的预算分配给模型大 小，而 Chinchilla 扩展法则则认为模型大小和数据大小应该 以相同的比例增加**，即在公式 (3)中的 a 和 b 取相近的值。
> 即，KM 法则更关注模型大小，ChinChilla 法则认为模型大小和数据大小应同比例增加。

虽然存在一些限制性的假设，这些扩展法则提供了对扩 展效应的直观理解，使得在训练过程中能够预测 LLM 的性 能 [45]。然而，一些能力（如 ICL [55]）无法根据扩展法则进 行预测，只有当模型超过一定规模时才能被观察到（如下面 讨论）。

#### 涌现能力

在文献中 [47]，LLM 的涌现能力被 正式定义为 “在小型模型中不存在但在大型模型中产生的能 力”，这是区别 LLM 与先前 PLM 的最显著特征之一。文章进 一步介绍了当涌现能力出现时的一个显著特点 [47]：当规模 达到一定水平时，性能显著提高，超出随机水平。类比而言， 这种涌现模式与物理学中的相变现象有密切联系 [47, 58]。原 则上，涌现能力可以与一些复杂任务相关联 [47, 59]，但我们 更关注可以用来解决各种任务的普遍能力。在这里，我们简 要介绍了 LLM 的三种典型涌现能力和具备这种能力的代表 性模型。

1. **上下文学习**：ICL 能力是由 GPT-3 [55] 正式引入的：==假 设已经为语言模型提供了一个自然语言指令和/或几个任务演 示，它可以通过完成输入文本的单词序列的方式来为测试实 例生成预期的输出，而无需额外的训练或梯度更新==。在 GPT 系列模型中，1750 亿的 GPT-3 模型在一般情况下表现出强大 的 ICL 能力，但 GPT-1 和 GPT-2 模型则没有。然而，这种 能力还取决于具体的下游任务。例如，130 亿参数的 GPT-3 可以在算术任务（例如 3 位数的加减法）上展现出 ICL 能力， 但 1750 亿参数的 GPT-3 在波斯语 QA 任务上甚至无法很好 地工作 [47]。
> ICL (In-Context Learning) ，称为“内置示例学习”。
> - **示例作为输入**：在ICL中，用户通过向模型提供一系列相关的示例（例如，问题-答案对）作为查询的一部分，来指示模型应如何处理一个特定的任务或问题。模型根据这些示例内部地调整其响应策略，以生成符合示例模式的输出。
> - **零次或少次学习**：ICL 使得 LLM 能够进行零次学习（zero-shot learning）和少次学习（few-shot learning）。零次学习意味着模型在没有看到任何特定任务示例的情况下就能理解并执行任务；少次学习意味着模型通过观察少量示例来学习任务。

2. **指令遵循**：通过使用自然语言描述的混合多任务数据集 进行微调（称为指令微调），**LLM 在未见过的以指令形式描述 的任务上表现出色** [28, 61, 62]。通过指令微调，LLM 能够在 没有使用显式示例的情况下遵循新的任务指令，因此它具有 更好的泛化能力。[62] 中的实验证明，当模型大小达到 680 亿 时，经过指令微调的 LaMDA-PT [63] 开始在未见过的任务上 显著优于未微调的模型，但对于 80 亿或更小的模型大小则不 会如此。最近的一项研究 [64] 发现，PaLM 至少在 620 亿的模 型大小上才能在四个评估基准（即 MMLU、BBH、TyDiQA 和 MGSM）的各种任务上表现良好，尽管较小的模型可能足 够完成某些特定任务（例如 MMLU）。
3. **逐步推理**：对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务，例如数学问题。然而，通过使用思 维链（Chain-of-Thought, CoT）提示策略 [32]，LLM 可以通 过利用包含中间推理步骤的提示机制来解决这类任务，从而 得出最终答案。这种能力可能是通过在代码上进行训练而获 得。一项实证研究 [32] 表明，当应用于模型大小大于 600 亿 的 PaLM 和 LaMDA 变体时，CoT 提示可以提高模型在算术 推理基准任务上的性能，而当模型大小超过 1000 亿时，其相 对于标准提示的优势更加明显。此外，CoT 提示的性能改进 在不同的任务上也存在差异，例如对于 PaLM 来说，GSM8K > MAWPS > SWAMP [32]。

#### LLM 应用到的关键技术

在此，我们 简要列举了几种重要的技术，这些技术（可能）是导致 LLM 成功的关键。

1. **扩展**：如前面的部分所讨论的，Transformer 语言模型 存在明显的扩展效应：更大的模型/数据规模和更多的训练计 算通常会导致模型能力的提升 [30, 33]。作为两个代表性的模型，GPT-3 和 PaLM 通过增加模型规模分别达到了 1750 亿 和 5400 亿。此外，由于计算预算通常是有限的，可以==利用扩展法则来更高效地分配计算资源==。例如，Chinchilla（具有更多的训练 token）通过在相同的计算预算下增加数据规模，优于其对应的模型 Gopher（具有更大的模型规模） [33]。然而，需要注意的是，数据扩展应该经过谨慎的清理过程，因为预 训练数据的质量在模型能力中起着关键作用。
2. **训练**：由于巨大的模型规模，成功训练一种能力强的 LLM 是非常具有挑战性的。分布式训练算法是学习 LLM 网络参数所必需的，其中通常联合使用各种并行策略。为了支 持分布式训练，已经发布了一些优化框架来促进并行算法的 实现和部署，例如 DeepSpeed [65] 和 Megatron-LM [66–68]。 此外，优化技巧对于训练稳定性和模型性能也很重要，例如 重新开始以克服训练损失激增 [56] 和混合精度训练 [69]。最 近，GPT-4 [45] 提出开发特殊的基础结构和优化方法，用更小的模型来可靠地预测大模型性能。
3. **能力引导**：在大规模语料库上预训练之后，LLM 具备 了作为通用任务求解器的潜在能力。然而，当 LLM 执行一些 特定任务时，这些能力可能不会显式地展示出来。==作为技术 手段，设计合适的任务指令或具体的 ICL 策略可以激发这些 能力==。例如，通过包含中间推理步骤，CoT 提示已被证明对 解决复杂的推理任务有效。此外，我们还可以使用自然语言 表达的任务描述对 LLM 进行指令微调，以提高 LLM 在未见 任务上的泛化能力。然而，这些技术主要对应于 LLM 的涌现 能力，可能对小语言模型的效果不同。
> 这就是 Prompt Engineering ，通过 Chain-of-Thought 对 LLM 进行逐步引导，实现在陌生场景中解决特定问题的目标。

4. **对齐微调**：由于 LLM 被训练用来捕捉预训练语料库的 数据特征（包括高质量和低质量的数据），它们可能会为人类 生成有毒、偏见甚至有害的内容。因此，有必要使 LLM 与 人类价值观保持一致，例如有用性、诚实性和无害性。为此， InstructGPT [61] 设计了一种有效的微调方法，使 LLM 能够 按照期望的指令进行操作，其中利用了基于人类反馈的强化 学习技术 [61, 70]。它将人类纳入训练循环中，采用精心设计 的标注策略。ChatGPT 实际上采用类似于 InstructGPT 的技 术，在产生高质量、无害的回答（例如拒绝回答侮辱性问题） 方面表现出很强的对齐能力。 
> “对齐”指的就是与人类伦理对齐，保证 AI 技术不用于危害人类的方向。

5. **工具操作**：从本质上讲，LLM 是基于海量纯文本语料 库进行文本生成训练的，因此在那些不适合以文本形式表达 的任务上表现不佳（例如数字计算）。此外，它们的能力也受 限于预训练数据，例如无法获取最新信息。为了解决这些问 题，最近提出了一种技术，即利用外部工具来弥补 LLM 的不足 [71, 72]。例如，LLM 可以利用计算器进行准确计算 [71]， 利用搜索引擎检索未知信息 [72]。最近，ChatGPT 已经实现 了使用外部插件（现有或新创建的应用程序）的机制 9，这类 似于 LLM 的“眼睛和耳朵”。这种机制可以广泛扩展 LLM 的 能力范围。

### ChatGPT 的演进

![[LLMs-0623-final.png]]

#### GPT-1

2017 年，Google 引入了 Transformer 模型 [22]， OpenAI 团队迅速使用这种新的神经网络架构进行语言建模 工作（曾经尝试是 RNNs，但效果不佳）。他们在 2018 年发布了第一个 GPT 模型，即 GPT1[74]，并将 GPT 作为模型名称的缩写，代表生成式预训练 （Generative Pre-Training）。GPT-1 是基于生成型的、仅解码 器（**Decoder-Only**）的 Transformer 架构开发的，并采用了无监督预训练和有 监督微调的混合方法。GPT-1 为 GPT 系列模型建立了核心 架构，并确立了对自然语言文本进行建模的基本原则，即预 测下一个单词。

#### GPT-2

GPT-2[26] 采用了与 GPT-1 类似的架构，将参 数规模增加到了 15 亿，并使用大规模的网页数据集 WebText 进行训练。正如 GPT-2 的论文所述，它旨在通过无监督语 言建模来执行任务，而无需使用标记数据进行显式微调。为 了 推 动 这 种 方 法， 他 们 引 入 了 多 任 务 求 解 的 概 率 形 式， 即 $p (output|input, task)$（类似的方法已在 [75] 中采用），它在 给定输入和任务信息的条件下预测输出。为了对该条件概率建模，自然语言文本可以自然地用作为格式化输入、输出和 任务信息的统一方式。通过这种方式，解决任务的过程可以 被视为生成解决方案文本的单词预测问题。此外，他们对这个想法提出了更正式的说明：“由于（特定任务的）有监督目标与无监督（语言建模）目标相同，但只在序列的子集上进行评估，所以无监督目标的全局最小值也是有监督目标的全局 最小值（对于各种任务）”[26]。对这个主张的基本理解是， ==每个 NLP 任务可以被视为基于世界文本的子集的单词预测 问题==。因此，如果模型训练后具有足够能力以复原世界文本，无监督语言建模可以解决各种任务。GPT-2 论文中的这些早 期讨论与 Ilya Sutskever 在接受 Jensen Huang 采访时的观点 相呼应：“神经网络学到的是生成文本的过程的某种表示。这个文本实际上是世界的投影... 你在预测下一个单词时越准确，真实度越高，你在这个过程中获得的分辨度就越高...”13。

***能力飞跃***：尽管 GPT-2 旨在成为一个“无监督的多任务学习 器”，但与监督微调的 SOTA 方法相比（如 BERT），其整体性能仍然较差。但 GPT-2 模型规模相对较小，在下游任务中得到了广泛 微调，尤其是对话任务 [76, 77]。基于 GPT-2，GPT-3 通过扩 展（几乎相同的）生成式预训练架构展现出重要的能力飞跃。

#### GPT-3

GPT-3 [55] 于 2020 年发布，将模型参数扩展 到了更大的规模，达到了 1750 亿。GPT-3 的论文正式介绍了 ICL 的概念 14，它是以小样本或零样本的方式使用 LLM。 ICL 可以指导 LLM 理解以自然语言文本的形式给出的任务。 LLM 的预训练和使用在 ICL 下有着相同的语言建模范式：预 训练预测给定上下文条件下的后续文本序列，而 ICL 预测正 确的任务解决方案，该解决方案可以被格式化为给定任务描 述和示范下的文本序列。GPT-3 不仅在各种 NLP 任务中表 现出色，而且在一些需要推理或领域适配能力的特殊设计的 任务中也表现出色。尽管 GPT-3 的论文没有明确讨论 LLM 的涌现能力，但我们观察到可能超越基本扩展法则 [30] 的巨 大性能飞跃，例如更大的模型具有显著更强的 ICL 能力（见 GPT-3 论文的原始图 1.2 [55]）。总体而言，GPT-3 可以被视 为从 PLM 到 LLM 进化过程中的一个重要里程碑。它通过实证证明，==将神经网络扩展到大的规模可以大幅增加模型的能 力==。

***能力增强***：由于其强大的能力，GPT-3 已经成为 OpenAI 开 发更强大 LLM 的基础模型。总体而言，OpenAI 探索了两种 主要方法来进一步改进 GPT-3 模型，即使用代码数据进行训 练以及与人类偏好的对齐，具体如下：
- **使用代码数据进行训练**：原始的 GPT-3 模型（在纯 文本上进行预训练）的一个主要限制在于缺乏复杂任务的推 理能力，例如完成代码和解决数学问题。为了增强这种能力， ==OpenAI 在 2021 年 7 月推出了 Codex [78]，这是一个在大量 GitHub 代码上微调的 GPT 模型==。它证明了 Codex 可以解 决非常困难的编程问题，并且在数学问题上有显著的性能提 升 [79]。此外，一种用于训练文本和代码嵌入的对比方法 [80]， 在 2022 年 1 月提出，它在一系列相关任务（例如线性探测分 类、文本搜索和代码搜索）上有所提升。实际上，GPT-3.5 模 型是在基于代码的 GPT 模型（code-davinci-002）的基础 上开发的，这表明==使用代码数据进行训练是改善 GPT 模型能 力（尤其是推理能力）的一种非常有用的实践==。此外，还有 一种猜测称使用代码数据进行训练可以极大地增加 LLM 的 CoT 提示能力 [46]，尽管这仍然需要更全面的验证。
- **与人类对齐**： OpenAI 与人类对齐的相关研究可以追 溯到 2017 年（或更早）：一篇名为 “learning from human preferences” 的文章 15 在 OpenAI 博客上发布，描述了应用强 化学习（RL）来学习由人类标注的偏好比较的工作，类似于 InstructGPT 在图 6 中的对齐算法的奖励训练步骤 [70]。在发 布这篇 RL 论文之后不久，近端策略优化 (Proximal Policy Optimization, PPO) 的论文在 2017 年 7 月发表 [81]，现在已 经成为从人类偏好中学习的基础 RL 算法 [61]。随后在 2020 年 1 月，GPT-2 通过上述 RL 算法进行了微调 [70, 81]，其利 用人类偏好来改进 GPT-2 在 NLP 任务上的性能。同年，另一 项工作 [82] 以相似的方式训练了一个用于优化人类偏好的摘要模型。基于这些先前的工作，InstructGPT[61] 在 2022 年 1 月提出，以改进 GPT-3 模型的与人类对齐能力，正式建立了 一个三阶段的基于人类反馈的强化学习（RLHF）算法。==请注 意，OpenAI 的论文和文档中似乎很少使用 “指令微调” 一词， 而是用在人类示例上有监督微调来替代==（即 RLHF 算法的第 一步 [61]）。除了提高指令遵循能力之外，RLHF 算法对于缓 解有害或有毒内容的生成问题十分有效，这对于 LLM 在实践 中的安全部署至关重要。OpenAI 在一篇技术文章中描述了他 们在对齐研究中的方法 [83]，总结了三个有前途的方向：“训 练 AI 系统使用人类反馈，协助人类评估以及做对齐研究”。 这些增强技术引出了具有更强能力的改进型 GPT-3 模 型，OpenAI 称其为 GPT-3.5 模型（请参见第 3.1 节中关于 OpenAI API 的讨论）。

#### ChatGPT

在 2022 年 11 月，OpenAI 发布了对话语言 模型 ChatGPT，它是基于 GPT 模型（GPT-3.5 和 GPT-4）开 发。正如官方博文所述 [84]，ChatGPT 是以类似 InstructGPT 的方式进行训练的（在原始文章中称为“InstructGPT 的姊 妹模型”），但专门针对对话能力进行了优化。在 ChatGPT 和 InstructGPT 的数据收集上，他们指出了一个不同之处： ChatGPT 训练数据是通过将人类生成的对话（扮演用户和 AI 两个角色）与 InstructGPT 数据集结合起来以对话形式生 成。ChatGPT 在与人类的交流中表现出卓越的能力：拥有丰 富的知识库，擅长解决数学问题，准确追踪多轮对话中的上 下文，并与人类的价值观保持一致以确保被安全使用。随后， ChatGPT 支持了插件机制，进一步通过已有工具或应用扩展 了 ChatGPT 的功能。到目前为止，它是 AI 历史上最强大的 聊天机器人。ChatGPT 的发布对未来的 AI 研究产生了重大 影响，为探索类似人类的 AI 系统提供了新的方向。

#### GPT-4

作为另一重要的进展，GPT-4 [45] 于 2023 年 3 月发布，将文本输入扩展到多模态信号。总体而言，相比 GPT-3.5，GPT-4 在解决复杂任务方面具有更强的能力，在许 多评估任务上展现出大幅度的性能提升。最近的一项研究 [40] 通过开展人类生成问题的定性测试来研究 GPT-4 的性能，其 中的测试涵盖了各种困难任务，结果显示 GPT-4 具有比之前 的 GPT 模型如 ChatGPT 更出色的性能。此外，由于经过 为期六个月的迭代对齐（在 RLHF 训练中加入了额外的安全 奖励信号），GPT-4 对于具有恶意或挑衅的提问的响应更加 安全。在技术报告中，OpenAI 强调了如何安全开发 GPT-4， 并采用了多种干预策略来减轻语言模型的可能问题，如幻觉、 隐私和过度依赖。例如，他们引入了一种称为红队评估（red teaming）的机制 [85] 来减少有害或生成有毒内容的可能性。 作为另一个重要方面，GPT-4 是在成熟的深度学习基础上开 发的，采用了改进的优化方法。他们引入了一种称为可预测扩展（predictable scaling）的新机制，可以使用模型训练期间 一小部分的计算量来准确预测最终性能。

## LLM 资源获取

### 公开可用的 Model Checkpoint 和 API

训练良好的模型检查点对于 研究组织开展 LLM 的研究和开发至关重要。由于参数规模是 使用 LLM 时需要考虑的关键因素，为了帮助用户根据其资源 预算确定适当的研究内容，我们将这些公开模型分为两个规 模级别（百亿参数量级别和千亿参数量级别）。此外，也可以 直接使用公开的 API 执行推理任务，而无需在本地运行模型。 接下来，我们对公开可用的模型检查点和 API 进行介绍。

- **百亿参数量级别的模型**：这类模型的参数规模除了 LLaMA （最大版本 650 亿参数）和 NLLB（最大版本 545 亿参数）， 大多在 100 亿至 200 亿之间。这一参数范围内的模型包 括 mT5 [88]、PanGu-α [89]、T0 [28]、GPT-NeoX-20B [92]、 CodeGen [91]、UL2 [94]、Flan-T5 [64] 和 mT0 [98] 等。其中， Flan-T5（110 亿版本）可以作为研究指令微调的首选模型，因 为它从三个方面探索了指令微调 [64]：增加任务数量、扩大模 型规模和使用 CoT 提示数据进行微调。CodeGen（11B）是 一个为生成代码设计的自回归语言模型，可用作探索代码生 成能力的候选模型。它提出了一个新的基准测试 MTPB [91]， 专门用于多轮程序合成，由 115 个专家生成的问题组成。为 了解决这些问题，需要 LLM 获得足够的编程知识（例如数 学、数组操作和算法）。对于多语言任务，mT0（13B）可能 是一个比较好的候选模型，因为它在多语言任务中使用多语 言提示进行微调。此外，对于中文的下游任务，PanGu-α [89] 具有较好的表现，特别是在零样本或小样本的设置下，该模 型基于深度学习框架 MindSpore [117] 开发，拥有多个参数版 本（最大版本 2000 亿参数），而最大的公开版本只有 130 亿 参数。此外，作为最近发布的模型，LLaMA（65B） [57] 在与 指令遵循相关的任务中展现了卓越的性能。由于其开放性和有效性，LLaMA 引起了研究界的广泛关注，许多工作 [118121] 致力于微调或继续预训练其不同的模型版本以实现新模 型或工具的开发。百亿参数量级别的模型通常需要数百甚至 上千个 GPU 或 TPU。例如，GPT-NeoX-20B 使用了 12 个微 服务器，每个服务器配备了 8 个 NVIDIA A100-SXM4-40GB GPU，LLaMA 使用了 2048 个 A100-80G GPU。为了准确估 计所需的计算资源，我们还是建议使用衡量涉及计算量的指标，例如计算 FLOPS（每秒浮点数运算次数） [30]。
- **千亿参数量级别的模型**：在这类模型中，只有少数几个模型进 行了公开发布。其中，OPT [95]、OPT-IML [99]、BLOOM [69] 和 BLOOMZ [98] 的参数量几乎与 GPT-3（175B）大致相同， 而 GLM [97] 和 Galactica [34] 的参数数量分别为 1300 亿和 1200 亿。其中，OPT（175B）专注于复现和开源，旨在使研 究人员能够进行大规模可重复研究。对于跨语言泛化研究，可 以将 BLOOM（176B）和 BLOOMZ（176B）用作基础模型， 因为其在多语言语言建模任务中具有较好的能力。在这些模 型中，OPT-IML 进行了指令微调，是研究指令微调效果的 较好选择。千亿参数量级别的模型通常需要数千个 GPU 或 TPU 进行训练。例如，OPT（175B）使用了 992 个 A10080GB GPU，GLM（130B）使用了 96 个 NVIDIA DGX-A100 （8x40G）GPU 节点集群。
- **大语言模型的公共 API**：相较于直接使用模型副本，API 提供了一种更方便的方式供普通用户使用 LLM，使得用 户 无 需 在 本 地 运 行 模 型。 作 为 使 用 LLM 的 代 表 性 接 口， GPT 系列模型的 API [45, 55, 61, 78] 已经广泛应用于学 术界和工业界 16。OpenAI 提供了七个主要的 GPT-3 系列 模型接口：ada、babbage、curie、davinci（GPT-3 系列 中最强大的版本）、text-ada-001、text-babbage-001 和 text-curie-001。其中前四个接口可以在 OpenAI 的主机 服务器上进一步进行微调。babbage、curie 和 davinci 分 别对应于 GPT-3 (1B)、GPT-3 (6.7B) 和 GPT-3 (175B) 模 型 [55]。此外，还有两个与 Codex 有关的 API，分别称为 code-cushman-001（Codex (12B) 的强大多语言版本 [78]） 和 code-davinci-002。GPT-3.5 系 列 包 括 一 个 基 础 模 型 code-davinci-002 和三个增强版本，即 text-davinci-002、 text-davinci-003 和 gpt-3.5-turbo-0301。 值 得 注 意 的 是，gpt-3.5-turbo-0301 是调用 ChatGPT 的接口。最近， OpenAI 还 发 布 了 与 GPT-4 相 应 的 API， 包 括 gpt-4、 gpt-4-0314、gpt-4-32k 和 gpt-4-32k-0314。总体而言，API 接口的选择取决于具体的应用场景和响应需求。详细的用法 可以在它们的项目网站上找到 17。

### 语料库

与早期的小型 PLM 不同，LLM 有着规模极大的参数量，需 要更大量且内容广泛的训练数据。为满足这种需求，越来越 多的可用于研究的训练数据集被发布到公共社区中。在本节， 我们将简要总结一些常用于训练 LLM 的语料库。基于它们的 内容类型，我们将这些语料库分为六个组别进行介绍：Books、 CommonCrawl、Reddit Links、Wikipedia、Code、Others。

1. **Books**： BookCorpus [122] 是之前小规模模型（如 GPT [74] 和 GPT-2 [26]）中常用的数据集，包括超过 11,000 本电子书， 涵盖广泛的主题和类型（如小说和传记）。另一个大规模的书 籍语料库是 Gutenberg [123]，它有超过 70,000 本文学作品， 包括小说、散文、诗歌、戏剧、历史、科学、哲学和其他公共 领域的作品。它是目前最大的开源书籍集合之一，被用于训 练 MT-NLG [110] 和 LLaMA [57]。至于在 GPT-3 [55] 中使 用的 Books1 [55] 和 Books2 [55]，它们比 BookCorpus 要大得 多，但目前尚未公开发布。
2. **CommonCrawl**： CommonCrawl [132] 是最大的开源网络 爬 虫 数 据 库 之 一， 能 力 达 到 了 百 万 亿 字 节 级 别， 已 经 被 广 泛运用于训练 LLM。由于整个数据集非常大，因此现有的 研究主要在特定时间段内从中提取网页子集。然而，由于网 络数据中存在大量的噪音和低质量信息，因此使用前需要进 行数据预处理。目前有四个较为常用的基于 CommonCrawl 的过滤数据集：C4 [87]，CC-Stories [124]，CC-News [27]， 和 RealNews [125]。C4 包括五个变种 18，即 en （806G）， en. noclean （6T），realnewslike （36G），webtextlike （17G） 和 multilingual （38T）。其中，en 版本被用于预训练 T5 [87]， LaMDA [63]，Gopher [59]，和 UL2 [94]。而 multilingual C4， 又被称为 mC4，被用于 mT5 [88] 的预训练。CC-Stories（31G） 是由 CommonCrawl 数据的子集组成，内容以故事的形式 展示。然而，CC-Stories 的原始数据现在已不可用，因此在表 2 中是一个复现版本 CC-Stories-R [133]。此外，还有两个 从 CommonCrawl 中提取的两个新闻语料库：REALNEWS （120G）和 CC-News（76G），也常用作预训练数据。
> ![[LLM-Survey-data-sources.png]]

3. **Reddit Links**：Reddit 是一个社交媒体平台，用户可以在上 面提交链接和帖子，其他人可以通过“赞同”或“反对”投票。高 赞的帖子通常被认为对多数用户是有帮助的，可以用来创建高 质量的数据集。WebText [26] 就是一个著名的基于 Reddit 的 语料库，它由 Reddit 上高赞的链接组成，但尚未公开。作为替 代，有一个易于获取的开源替代品叫做 OpenWebText [126]。 另一个从 Reddit 中提取的语料库是 PushShift. io [127]，它是 一个实时更新的数据集，包括自 Reddit 创建以来的历史数据。 Pushshift 不仅提供每月的数据转储，还提供有用的实用工具， 支持用户搜索、总结和对整个数据集进行初步统计分析。这 使得用户可以轻松地收集和处理 Reddit 数据。
4. **Wikipedia**： Wikipedia [128] 是一个在线百科全书，包含大 量高质量的文章，涵盖各种主题。其中大部分文章都采用解 释性写作风格（并支持引用），覆盖了多种不同语言和广泛的 知识领域。通常来说，Wikipedia 英语版本被广泛应用于大多 数 LLM (例如 GPT-3 [55]，LaMDA [63] 和 LLaMA [57])。它 还提供多种语言版本，因此可以在多语言环境下使用。
5. **Code**：为了收集代码数据，现有工作主要是从互联网上爬取 有开源许可证的代码。代码数据有两个主要来源：包括开源 许可证的公共代码库（例如 GitHub）和与代码相关的问答平 台（例如 StackOverflow）。Google 公开发布了 BigQuery 数据 集 [129]，其中包括各种编程语言的大量开源许可证代码片段， 是一个典型的代码数据集。CodeGen 使用的 BIGQUERY [91] 是 BigQuery 数据集的一个子集，用于训练多语言版本的 CodeGen（CodeGen-Multi）。
6. **Others**： The Pile [130] 是一个大规模、多样化、开源的文 本数据集，有超过 800GB 数据，内容包括书籍、网站、代码、 科学论文和社交媒体平台等。它由 22 个多样化的高质量子集 构成。The Pile 数据集被广泛应用于不同参数规模的模型，如 GPT-J（6B）[134]、CodeGen（16B）[91] 和 Megatron-Turing NLG（530B） [110]。此外，ROOTS [131] 由各种较小的数据 集（总共 1.61 TB 的文本）组成，覆盖 59 种不同的语言（包 括自然语言和编程语言），它被用于训练 BLOOM [69]。 实际上，为了预训练 LLM，通常需要混合使用不同的数 据源（见图 2），而不是单一的语料库。因此，现有的研究通 常混合几个现成的数据集（如 C4、OpenWebText 和 the Pile 等），然后进行进一步的处理以获取预训练语料库。此外，为 了训练适用于特定应用的 LLM，从相关源（如 Wikipedia 和 BigQuery）提取数据以丰富预训练数据中的相应信息也很重 要。

### 代码库

在这部分，我们简要介绍了一些可用于开发 LLM 的代码库。 

1. Transformers [135] 是一个使用 Transformer 架构构 建模型的开源 Python 库，由 Hugging Face 开发和维护。它具有简单和用户友好的 API，方便使用和定制各种预训练模 型。它是一个功能强大的库，拥有庞大而活跃的用户和开发 者社区，他们定期更新和改进模型和算法。
2. DeepSpeed [65] 是由 Microsoft 开发的深度学习优化 库（与 PyTorch 兼容），已用于训练多个 LLM，例如 MTNLG[110] 和 BLOOM [69]。它提供了各种分布式训练优化技 术的支持，例如内存优化（ZeRO 技术、梯度检查点）和管道 并行。
3. Megatron-LM [66–68] 是由 NVIDIA 开发的深度学习库，用于训练 LLM。它提供了丰富的分布式训练优化技术， 包括模型和数据并行、混合精度训练和 FlashAttention。这些 优化技术可以大大提高训练效率和速度，并实现 GPU 间的高 效分布式训练。
4. JAX [136] 是由 Google 开发的用于高性能机器学习算 法的 Python 库，允许用户在带有硬件加速（例如 GPU 或 TPU）的情况下进行数组的高效运算。它可以在各种设备上 进行高效计算，还支持自动微分和即时编译等特色功能。
5. Colossal-AI [137] 是由 HPC-AI Tech 开发的用于 训练大规模人工智能模型的深度学习库。它基于 PyTorch 实 现， 并 支 持 丰 富 的 并 行 训 练 策 略。 此 外， 它 还 可 以 使 用 PatrickStar[138] 提出的方法优化异构内存管理。最近，使 用 Colossal-AI 基于 LLaMA [57] 开发的类 ChatGPT 模型 ColossalChat [121]（7B 和 13B 版本）已经公开发布。
6. BMTrain [139] 是由 OpenBMB 开发的用于以分布式 方式训练大规模参数模型的高效库，强调代码简洁、低资源占 用和高可用性。BMTrain 已经将一些常见的 LLM（如 FlanT5[64] 和 GLM [97]）迁移到到其 ModelCenter 中，用户可以 直接使用这些模型。
7. FastMoE [140] 是一种专门用于 MoE（即混合专家） 模型的训练库。它基于 PyTorch 开发，注重效率和用户友好性。FastMoE 简化了将 Transformer 模型转换为 MoE 模型的 过程，并支持数据并行和模型并行训练。

除了上述的库资源外，其他深度学习框架（例如 PyTorch [141]，TensorFlow [142]，MXNet [143]，PaddlePaddle [144]，MindSpore [117] 和 OneFlow [145]）也提供了并行算法支持，这些算法通常用于训练大规模模型。

## 预训练

### 数据收集

#### 数据来源

相比小规模语言模型，LLM 更需要高质量数据来预训练模型， 并且它们的模型能力很大程度上依赖于预训练语料库及其预 处理方式。在这一部分，我们讨论预训练数据的收集和处理， 包括数据来源、预处理方法以及对预训练数据如何影响 LLM 的性能的重要分析。

预训练语料库的来源可以广义地分为两种类型：**通用文 本数据**和**专用文本数据**。通用文本数据，如网页、书籍和对 话文本等，其由于规模大、多样性强且易于获取的特点，被 大多数 LLM 所利用 [55, 56, 95]，这可以增强 LLM 的语言 建模和泛化能力。鉴于 LLM 所展现出的惊人泛化能力，也 有研究将预训练语料库扩展到更专用的数据集，如多语言数 据、科学数据和代码等，以此来赋予 LLM 解决专用任务的能 力 [34, 56, 91]。接下来，我们将描述这两种类型的预训练数据 来源以及它们对 LLM 的影响。关于常用语料库的详细介绍， 可以参考第 3.2 节。

***通用文本数据***

![[LLM-Survey-data-sources-for-LLM.png]]

如图 Fig. 6 所示，绝大多数的 LLM 采用了通用的 预训练数据，比如网页、书籍和对话文本等，这些数据源提供 了丰富的文本资源，并且涉及了多种主题。接下来，我们简要 总结三种重要的通用文本数据。
- 网页：随着互联网的普及，多种多样的数据被创造 出来，这些丰富的数据使得 LLM 能够获得多样化的语言 知识并增强 LLM 的泛化能力 [26, 87]。为了方便使用这些 数 据 资 源， 之 前 的 工 作 从 网 络 中 爬 取 了 大 量 的 数 据， 如 CommonCrawl [132]。然而，这些爬取的网络数据往往同时 包含高质量的文本，如维基百科，和低质量的文本，如垃圾邮 件，因此过滤和处理网页以提高数据质量非常重要。
- 对话文本：对话数据可以增强 LLM 的对话能力 [95]， 并可能改善 LLM 在问答任务上的表现 [56]。研究人员可以利 用公共对话语料库的子集（例如 PushShift. io Reddit 语料库） [127, 146]，或从在线社交媒体收集对话数据。由于在线对话 数据通常涉及多个参与者之间的讨论，因此一种有效的处理 方式是将对话转换成树形结构，其中每句话与回应它的话语 相连。通过这种方式，多方之间的的对话树可以被划分为预 训练语料库中的多个子对话。然而，过度引入对话数据来训 练 LLM 可能会导致一个潜在的风险 [95]：陈述性指令和直接 疑问句被错误地认为是对话的开始，从而导致指令的有效性 下降。
- 书籍：与其他语料库相比，书籍提供了更正式的长文 本，这对于 LLM 学习语言知识、建模长期依赖关系以及生成 叙述性和连贯的文本具有潜在的好处。为了获得开源书籍数 据，现有的研究通常采用 Books3 和 Bookcorpus2 数据集，这 些数据集可以在 Pile 数据集中获得 [130]。

***专用文本数据***

专用数据集对于提高 LLM 在特定下游任务中 的能力非常有用。接下来，我们介绍三种专用数据类型。

- 多语言文本：除了在单目标语言上进行训练外，整合多 语言语料库可以增强模型的多语言的理解和生成能力。例如， BLOOM [69] 和 PaLM [56] 在其预训练语料库中收集了包含 46 种和 122 种语言的多语言数据。这些模型在多语言任务中 展现出了出色的性能，例如翻译、多语言摘要和多语言问答， 并且它与在目标语言上微调的最先进的模型具有可比性甚至 有更好的性能。
- 科学文本：科学出版物的不断增长见证了人类对科学的 探索。为了增强 LLM 对科学知识的理解 [34, 147]，可以将科学语料库纳入模型的预训练语料 [34, 147]。通过在大量科学 文本上进行预训练，LLM 可以在科学和推理任务中取得出色 的性能 [148]。为了构建科学语料库，现有的工作主要收集了 arXiv 论文、科学教材、数学网页和其他相关的科学资源。由 于科学领域数据的复杂性，例如数学符号和蛋白质序列，通 常需要特定的标记化和预处理技术来将这些不同格式的数据 转换为可以被语言模型处理的统一形式。
- 代码：程序编写在学术界得到了广泛的研究 [78, 149152]，特别是对于在代码上进行训练的 PLM 的应用 [134, 153]。然而，对于这些 PLM（例如 GPT-J [134]），生成高 质量和准确的程序仍然具有挑战性。最近的研究 [78, 152] 发 现，在大量的代码语料库上预训练 LLM 可以显著提高编写程 序的质量。编写的程序可以成功通过专家设计的单元测试用 例 [78] 或解决竞赛编程问题 [111]。一般来说，常用于预训练 LLM 的代码语料库有两种来源。第一种来源是编程问答社区 （如 Stack Exchange）[154, 155]。第二种来源是开源软件仓库， 例如 GitHub [78, 91, 152]，它们收集了代码数据（包括注释 和文档字符串）以供利用。与自然语言文本相比，代码以编程 语言的格式呈现，对应着长距离依赖和准确的执行逻辑 [156]。 最近的一项研究 [46] 还推测，训练代码可能是复杂推理能力 （例如 CoT 能力 [32]）的来源。此外，将推理任务格式化为代 码的形式还可以帮助 LLM 生成更准确的结果 [156, 157]。

#### 数据预处理

在收集大量文本数据后，对数据进行预处理，特别是消除噪 声、冗余、无关和潜在有害的数据 [56, 59]，对于构建预训练 语料库是必不可少的，因为这些数据可能会极大地影响 LLM 的能力和性能。在这部分中，我们将细致地回顾提高收集数 据质量的数据预处理策略 [59, 69, 109]。预处理 LLM 的预训 练数据的典型流程已在图 Fig. 7 中说明。

![[LLM-Survey-data-preparing.png]]

1. 质量过滤：为删除收集到的语料库中的低质量数据，现有的工作通常采用两种方法：（1）基于分类器的方法，和（2）基于启发 式的方法。前一种方法基于高质量文本训练选择分类器，并利 用它来识别和过滤低质量数据。通常，这些方法 [55, 56, 109] 使用高质量数据（例如维基百科页面）作为正样本，采样候选 数据作为负样本来训练二元分类器，并预测衡量每个数据的 质量的分数。然而，一些研究 [59, 109] 也发现，基于分类器的 方法可能会删除方言、口语和社会语言的高质量文本，从而可 能导致有偏的预训练语料库，并减少语料库的多样性。对于 第二种方法，一些研究，如 BLOOM [69] 和 Gopher [59]，采 用基于启发式的方法，通过设计一组精心设计的规则来消除 低质量文本，这些规则可以总结如下： • 基于语言的过滤：如果 LLM 主要用于某项语言的任务中， 那么其他语言的文本可以被过滤掉。 • 基于度量的过滤：可以利用生成文本的评估度量，例如困 惑度（perplexity），来检测和删除不自然的句子。 • 基于统计的过滤：可以利用语料库的统计特征，例如标点 符号分布、符号与单词比率和句子长度，来衡量文本质量 并过滤低质量数据。 • 基于关键词的过滤：基于特定的关键词集合，可以识别和 删除文本中的噪声或无用元素，例如 HTML 标签、超链 接、模板和攻击性词语。
2. 去重：现有的研究 [158] 发现，语料库中的重复数据会降低语 言模型的多样性，可能导致训练过程不稳定，从而影响模型 性能。因此，需要对预训练语料库进行去重处理。具体来说， 可以在句子级、文档级和数据集级等不同粒度上去重。首先， 在句子级别上，应删除包含重复单词和短语的低质量句子，因 为它们可能会在语言建模中引入重复模式 [159]。在文档级别 上，现有研究主要依靠文档之间的表层特征（例如单词和 n 元的重叠）的重叠比率来检测和删除包含相似内容的重复文 档 [57, 59, 69, 160]。此外，为了避免数据集污染问题，还必须 通过从训练集中删除测试集可能出现的重复文本，来防止训 练集和评估集之间的重叠 [56]。现有研究已经证明，这三个级 别的去重都有助于改善 LLM 的训练 [56, 161]，在实践中应该 共同使用这三个级别的去重。
3. 隐私去除：大多数预训练文本数据来自网络来源，包括涉及敏感或个人信息的用户生成内容，这可能增加隐私泄露的风 险 [162]。因此，需要从预训练语料库中删除可识别个人信息 （PII）。一种直接有效的方法是采用基于规则的方法，例如 关键字识别，来检测和删除 PII，例如姓名、地址和电话号 码 [131]。此外，研究人员还发现，LLM 在隐私攻击下的脆弱 性可能归因于预训练语料库中存在的重复 PII 数据 [163]。因 此，去重也可以在一定程度上降低隐私风险。
4. 分词：分词也是数据预处理的关键步骤。它的目的是将原始 文本分割成词序列，随后用作 LLM 的输入。虽然直接利用 已有的分词器是方便的（例如 OPT [95] 和 GPT-3 [55] 利 用了 GPT-2 [26] 的分词器），但是使用专门为预训练语料 库设计的分词器可能会更加有效 [69]，特别是对于由多种领 域、语言和格式组成的语料库。因此，最近的几个 LLM 使用 SentencePiece [164] 为预训练语料库训练定制化的分词器。同 时利用字节级的 Byte Pair Encoding (BPE) 算法 [165] 来确保 分词后的信息不会丢失 [56, 59]。然而需要注意的是，BPE 中 的归一化技术，例如 NFKC [166]，可能会降低分词性能 [33, 59, 69]。

#### 预训练数据对 LLM 的影响

与小规模的 PLM 不同，由于对计算资源的巨大需求，通常不 可能对 LLM 进行多次预训练迭代。因此，在训练 LLM 之前 构建一个准备充分的预训练语料库尤为重要。在这一部分中， 我们将讨论预训练语料库的质量和分布如何潜在地影响 LLM 的性能。

- **混合来源**：正如前面所讨论的，来自不同领域或场景的预训练 数据具有不同的语言特征或语义知识。通过在来自不同来源 的文本数据上进行预训练，LLM 可以获得广泛的知识，并可 能会展现出强大的泛化能力。当混合不同来源的数据时，需要 仔细设置预训练数据的分布，因为这也可能会影响 LLM 在下 游任务上的性能 [59]。Gopher [59] 对数据分布进行了消融实 验，以检验混合来源对下游任务的影响。它在 LAMBADA 数 据集 [167] 上的实验结果表明，增加书籍数据的比例可以提高 模型从文本中捕捉长期依赖的能力，增加 C4 数据集 [87] 的 比例则会提升它在 C4 验证数据集 [59] 上的性能。然而，单独 训练过多的某个领域的数据会影响 LLM 在其他领域的泛化能力 [34, 59]。因此，建议研究人员应仔细确定预训练语料库 中来自不同领域的数据的比例，以开发更符合其特定需求的 LLM。读者可以参考图 2，了解和比较不同 LLM 的数据来源。
- **预训练数据的*数量***：为了预训练一个有效的 LLM，收集足够 的高质量数据以满足 LLM 的数据数量需求是很重要的。现 有研究发现，随着 LLM 参数规模的增加，也需要更多的数据 来训练模型 [33, 57]：在模型性能方面，数据大小也观察到与 模型大小类似的扩展法则。Chinchilla [33] 表明，许多现有的 LLM 由于缺乏充足的预训练数据而遭受次优训练的问题。通 过进行广泛的实验，一些研究进一步表明，在给定的计算预 算下，采用相等规模的模型参数和训练 token 是必要的。最 近，LLaMA [57] 表明，使用更多的数据和进行更长时间的训 练，较小的模型也可以实现良好的性能。因此，建议研究人员 在充分训练模型时，尤其是在调节模型参数时，应该更加关 注高质量数据的数量。
- **预训练数据的*质量***：现有的研究表明，对低质量的语料库进 行预训练时，例如噪声、有害和重复的数据，可能会损害模型 的性能 [59, 158, 160, 163]。为了开发表现良好的 LLM，收集 的训练数据的数量和质量都是至关重要的。最近的研究，例 如 T5 [87]、GLaM [109] 和 Gopher [59]，已经研究了数据质 量对下游任务性能的影响。通过比较在过滤和未过滤的语料 库上训练的模型的性能，它们得到了相同的结论，即在清理 后的数据上预训练 LLM 可以提高性能。更具体地说，数据 的重复可能会导致“双下降现象”（指性能最初恶化，随后得 到改善） [158, 168]，甚至可能会使训练过程不稳定 [158]。此 外，已经有研究表明重复的数据会降低 LLM 从上下文中复制 的能力，这可能进一步影响 LLM 在 ICL 中的泛化能力 [158]。 因此，正如现有研究 [56, 59, 69] 所建议的，研究人员有必要 仔细地对预训练语料库进行预处理（如在第 4.1.2 节中所示）， 来提高训练过程的稳定性并避免其对模型性能的影响。

### 架构

![[LLM-Survey-LLMs-list.png]]

#### 主流架构

由于 Transformer 架 构 的 出 色 并 行 性 能 和 任 务 完 成 能 力， Transformer 架构已成为开发各种 LLM 的标准骨干，使得 语言模型能够扩展到数百亿或数千亿个参数 [22]。一般来说， 现有 LLM 的主流架构可以大致分为三种类型，即编码器-解 码器、因果解码器和前缀解码器, 正如图 Fig. 9 所示。

![[LLM-Survey-attention-patterns.png]]

1. **编码器-解码器架构**：传统 Transformer 模型是建立在编码器解码器架构上 [22]，由两个 Transformer 块分别作为编码器和 解码器。==编码器采用堆叠的多头自注意层对输入序列进行编 码以生成其潜在表示，而解码器对这些表示进行交叉注意并自回归地生成目标序列==。编码器-解码器 PLM（例如 T5 [87] 和 BART [24]）能有效完成各种 NLP 任务。目前，只有少数 LLM 是基于编码器-解码器架构构建的，例如 Flan-T5 [64]。 有关架构选择的详细讨论将在第 4.2.4 节中进行。
2. **因果解码器架构**：因果解码器架构==采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身==。输入 和输出 token 通过解码器以相同的方式进行处理。作为这种 架构的代表性语言模型，GPT 系列模型 [26, 55, 74] 是基于因 果解码器架构开发的。特别地，GPT-3 [55] 成功展示了这种 架构的有效性，同时也展示了 LLM 惊人的 ICL 能力。有趣 的是，GPT-1 [74] 和 GPT-2 [26] 没有展现出与 GPT-3 相同 的卓越能力，表明了模型规模的扩大在增加这种模型架构的 能力方面起到了重要作用。至今，因果解码器已被广泛用作 为各种现有 LLM 的体系结构，例如 OPT [95]、BLOOM [69] 和 Gopher [59]。注意，接下来讨论的因果解码器和前缀解码 器都属于仅解码器体系架构。当提到“仅解码器架构”时，除 非另有说明，否则主要是指现有文献中的因果解码器架构。
3. **前缀解码器架构**：前缀解码器架构（也称非因果解码器架构） 修正了因果解码器的掩码机制，以使其==能够对前缀 token 执 行双向注意力，并仅对生成的 token 执行单向注意力==。这样，与编码器-解码器架构类似，前缀解码器可以双向编码前缀序列并自回归地逐个预测输出 token，其中在编码和解码过程中共享相同的参数。实用的建议是不从头开始进行预训 练，而是继续训练因果解码器，然后将其转换为前缀解码器以 加速收敛 [29]，例如 U-PaLM [115] 是从 PaLM [56] 演化而来。 基于前缀解码器架构的现有代表性 LLM 包括 GLM-130B [97] 和 U-PaLM [115]。 对于这三种类型的架构，我们也可以考虑通过混合专家（MoE）扩展它们，其中每个输入的一小部分神经网络权重被稀疏激活，例如 Switch Transformer [25] 和 GLaM [109]。已经证明，通过增加专家的数量或总参数大小，性能会有显著 的改进 [170]。

#### 详细配置

自 Transformer [22] 推出以来，已经提出了各种改进方法来提 高其训练稳定性、性能和计算效率。在这部分中，我们将讨论 Transformer 的四个主要部分的相应配置，包括标准化、位置编码、激活函数、注意力和偏置。为了使这份综述更加独立完 整，我们在表 6 中呈现了这些配置的详细公式。

![[LLM-Survey-transformer-config-detail.png]]

1. **标准化**：训练不稳定是预训练 LLM 的一个难题。为了缓解 这个问题，层标准化 (Layer Norm, LN) [172] 被广泛应用于 Transformer 架构中。LN 的位置对 LLM 的性能至关重要。 ==虽然最初的 Transformer [22] 使用后置 LN，但大多数 LLM 采用前置 LN 以实现更稳定的训练，尽管会带来一定的性能 损失== [181]。基于前置 LN，Sandwich-LN [171] 在残差连接之 前添加额外的 LN，以避免数值爆炸。然而，已有研究发现 Sandwich-LN 有时无法稳定 LLM 的训练，可能导致训练崩 溃 [97]。最近，一些高级标准化技术被提出以作为 LN 的替代 方案。由于 RMS Norm 在训练速度和性能方面的优越性 [173]， 其在 Gopher [59] 和 Chinchilla [33] 中被采用。与 LN 相比， DeepNorm [174] 已经表现出更好的训练稳定性，和后标准化 一起被 GLM-130B 采用。此外，在嵌入层后添加额外的 LN 也可以稳定 LLM 的训练。然而，这往往会导致显著的性能下 降 [182]，在一些最近的 LLM 中已经被移除 [69]。
2. **激活函数**：为了获得良好的性能，在前馈网络中也需要设置合适的激活函数。在==现有的 LLM 中，广泛使用 GeLU 激活函数== [183]。此外，在最新的 LLM (例如 PaLM 和 LaMDA) 中， 也使用了 GLU 激活函数的变体 [178, 184]，特别是 SwiGLU 和 GeGLU 变体，在实践中通常可以获得更好的性能 [185]。然而，与 GeLU 相比，它们在前馈网络中需要额外的参数（约 50%）。
>[!note] ReLU vs. GeLU
>***ReLU*** (Rectified Linear Unit) 是目前最流行的激活函数之一，特别是在深度学习网络中。它的数学表达式非常简单：$f (x) = max (0, x)$ 。这意味着如果输入 $x$ 是正数，则输出就是\(x\)；如果输入是负数，则输出为 0。
>**优点**：
>- **计算简单**：ReLU 函数及其梯度都非常简单，这使得网络的前向和反向传播更加高效。
>- **缓解梯度消失问题**：对于正输入，ReLU 的导数是常数（即 1），这有助于缓解深层网络中的梯度消失问题。
>- **稀疏激活**：由于 ReLU 会将负值置为 0，它在一定程度上引入了激活的稀疏性，有助于提高网络的计算效率。
>**局限**：
>- **不可导点**：在\(x=0\)处 ReLU 不可导，虽然在实践中通常不会造成问题。
>- **死亡 ReLU 问题**：如果一个神经元的输出总是负数，则它的梯度将永远是 0，这意味着在训练过程中它不会更新，这被称为“死亡 ReLU”问题。
>
>***GeLU*** (Gaussian Error Linear Unit) 是一种相对较新的激活函数，由 Hendrycks 和 Gimpel 在 2016 年提出。它在 Transformer 模型（如 GPT 和 BERT）中得到了广泛应用。GeLU 的数学表达式为：\[f (x) = xP (X \leq x)\]，其中\(P (X \leq x)\)是标准正态分布的累积分布函数（CDF）。一个近似的、更常用的 GeLU 表达式是：\[f (x) = 0.5x (1 + \tanh (\sqrt{2/\pi}(x + 0.044715x^3)))\]。
>**优点**：
>- **非线性和连续性**：GeLU 是一个非线性的连续函数，它允许小的和负的信号以一种控制的方式通过，同时保持 ReLU 的一些优点，如非饱和的正区域。
>- **自适应门控机制**：GeLU 通过其自适应门控机制，可以根据输入的大小调整其梯度，这被认为有助于学习复杂的模式。
>**局限**：
>- **计算复杂度**：与 ReLU 相比，GeLU 的计算更复杂，尽管这通常不是现代深度学习库的问题。
>- **理论支持**：虽然 GeLU 在实践中表现出色，但对其工作原理的理解和理论分析相对于 ReLU 还不那么充分。
>
>总结而言，ReLU 因其简单和高效而广泛用于各种神经网络中，尤其是在 CNNs 中。GeLU 则因其优异的性能和灵活性，成为了 Transformer 架构中的首选激活函数。选择哪种激活函数通常取决于特定任务的需求和网络架构的特点。

3. **位置编码**：由于 Transformer 中的自注意模块具有置换不变性，因此需要使用位置编码来注入绝对或相对位置信息以建模序列。==在经典的 Transformer [22] 中有两种绝对位置编码的 变体，即正弦函数和学习的位置编码，后者通常在 LLM 中使 用==。与绝对位置编码不同，相对位置编码根据键和查询之间的 偏移量生成嵌入 [87]，因此它可以在比训练序列更长的序列上表现良好，即外推 [180]。ALiBi [180] ==使用基于键和查询之间距离的惩罚来偏置注意力分数。实证结果表明，它比其他 位置编码具有更好的零样本泛化能力和更强的外推能力== [29]。 此外，通过基于绝对位置设置特定的旋转矩阵，RoPE [179] 中的键和查询之间的分数可以使用相对位置信息计算，这对于建模长序列是有用的。因此，RoPE 已经被广泛应用于一些 最新的 LLM [56, 57, 97]。
4. **注意力机制和偏置**：除了原始 Transformer 中的全自注意力 机制 [22]，GPT-3 采用了更低计算复杂度的稀疏注意力机制，即分解注意力 [55, 186]。为了有效且高效地建模更长的序列， 研究者们尝试引入特殊的注意力模式 [187, 188] 或考虑显存访问（即 FlashAttention [189]）。此外，与原始 Transformer 一样，大多数 LLM 在每个线性层和层标准化中保留了偏置。然而，在 PaLM [56] 和 Galactica [34] 中，偏置被移除。研究 表明，==对于 LLM 来说，去除偏置可以增强训练的稳定性== [56]。

综合上述讨论，我们总结了现有文献中的详细配置建议。 为了有更强的泛化能力和训练稳定性，建议选择前置的 RMS 进行层标准化，并选择 SwiGLU 或 GeGLU 作为激活函数。此外，在位置编码方面，RoPE 或 ALiBi 是更好的选择，因为它 们在长序列上表现更好。

#### 预训练任务

预训练在将大规模语料库中的通用知识编码到巨大的模型参数中起着关键作用。对于训练 LLM，有两个常用的预训练任务，即语言建模和去噪自编码。 

- **语言建模**：语言建模任务（LM）是预训练仅包含解码器的 LLM（如 GPT3 [55] 和 PaLM [56]）最常用的目标。给定一个 token 序列 x = {x1, . . . , xn}，LM 任务旨在基于序列中前面 的 $\text{token } x_{<i}$，自回归地预测目标 $\text{token }x_i$ 。通常的训练目标是最大化以下似然函数：$\mathcal{L}_{LM}(\mathbf{x})=\sum\limits_{i=1}^{n}\log P(x_{i}|x_{<i}).$ 
	- ==由于大多数语言任务可以转换为基于输入的预测问题来解决，因此这些仅包含解码器的 LLM 可能具有优势，可以隐 式地学习如何以统一的 LM 方式完成这些任务==。
	- 一些研究还表明，==仅包含解码器的 LLM 可以通过自回归地预测下一个 token 而自然地迁移到某些任务中，而无需微调== [26, 55]。LM 的一个重要变体是前缀语言建模任务，它是为预训练具有前 缀解码器架构的模型设计的。在计算前缀语言模型的损失时， 将不使用随机选择的前缀内的 token。由于模型预训练涉及的 序列中 token 较少，因此在使用相同数量的预训练 token 时， 前缀语言模型的性能往往略低于传统语言模型任务 [29]。

- **去噪自编码**：除了传统的 LM 之外，去噪自编码任务（DAE, Denoising AutoEncoding） 也被广泛用于 PLM [24, 87]。DAE 任务的输入 $x_{\text{\\}\tilde{x}}$ 是一些有 随机替换区间的损坏文本。然后，训练语言模型以恢复被替 换的 $\text{token }\tilde{x}$。形式上，DAE 的训练目标如下：$\mathcal{L}_{DAE}(\mathbf{x})=\log P(\tilde{\mathbf{x}}|\mathbf{x}_{\text{\\}\tilde{\mathbf{x}}})$ 然而，DAE 任务在实现上似乎比 LM 任务更为复杂。因 此，它并没有被广泛用于预训练 LLM。采用 DAE 作为预训 练目标的现有 LLM 包括 T5 [87] 和 GLM-130B [97]。这些模 型主要通过自回归地恢复替换区间来进行训练。

### 模型训练

#### 优化参数设置

为了进行 LLM 的参数优化，我们介绍了批量训练、学习率、 优化器和训练稳定性的常用设置。

1. **批量训练**：对于语言模型的预训练，现有的研究通常将批量 大小设置为较大的数字（如 2,048 个例子或 400 万个 token）， 以提高训练的稳定性和吞吐量。像 GPT-3 和 PaLM 这样的 LLM 引入了一种新的策略，即==在训练过程中动态增加批量大小，最终达到百万级别==。具体而言，GPT-3 的批量大小从 3.2 万逐渐增加到 320 万个 token。实证结果表明，==动态调整批量 大小的策略可以有效地稳定 LLM 的训练过程== [56]。
2. **学习率**：现有的 LLM 通常在预训练过程中采用类似的学习 率调整策略，包括预热（warm-up）和衰减（decay）。具体而言，==在训练的初始 0.1% 到 0.5% 的步骤中，采用线性预热策 略逐渐增加学习率到最大值==，这个最大值通常在 $5 × 10^{−5}$ 到 $1 × 10^{−4}$ 之间（例如 GPT-3 的学习率为 $6 × 10^{−5}$）。然后，在==后续步骤中采用余弦衰减策略，逐渐将学习率降低到其最 大值的约 10%，直到训练损失的收敛==。
3. **优化器**： Adam 优化器 [190] 和 AdamW 优化器 [191] 被广泛 应用于 LLM（例如 GPT-3）的训练中，这些优化器使用了基于一阶梯度优化的低阶矩自适应估计。通常，它的超参数设 置如下：β1 = 0.9，β2 = 0.95 和 ε = 10−8。同时，Adafactor 优化器 [192] 也被用于训练 LLM（例如 PaLM 和 T5），它是一种 Adam 优化器的变体，经过特殊设计以在训练过程中 节省显存。Adafactor 优化器的超参数设置如下：β1 = 0.9， β2 = 1.0 − k−0.8，其中 k 表示训练步骤的数量。
4. **稳定训练**：在 LLM 的预训练过程中，常常会遇到训练不稳定 的问题，这可能会导致模型崩溃。为了解决这个问题，==通常会广泛使用权重衰减（weight decay）和梯度裁剪（gradient clipping）==。现有的研究 [55, 69, 95, 97, 110] 通常将梯度裁剪的 阈值设置为 1.0，将权重衰减率设置为 0.1。然而，随着 LLM 规模的扩展，训练损失的突增也更容易发生，导致训练不稳 定。为了缓解这个问题，PaLM [56] 和 OPT [95] 使用了一种 简单的策略，即从发生突增之前的一个检查点重新开始训练 过程，并跳过可能导致问题的数据。此外，GLM [97] 发现嵌 入层的异常梯度通常会导致突增，因此提出缩减嵌入层梯度 以缓解这个问题。

#### 可扩展的训练技术

随着模型和数据规模的增加，有限的计算资源下高效地训练 LLM 变得具有挑战性。尤其是两个主要的技术问题需要解决， 即**提高训练吞吐量**和**加载更大模型到显存**中。在本部分中，我 们回顾了已有工作中广泛用于解决上述两个挑战的几种方法， 即 3D 并行 [66, 193, 194]，ZeRO [195] 和混合精度训练 [196]， 并就如何利用它们进行训练提供了一般性的建议。

1. 3D 并行： 3D 并行实际上是三种常用并行训练技术的组合， 即数据并行、流水线并行 [193, 194] 和张量并行 [66]19。我们 接下来介绍这三种并行训练技术。 • 数据并行：数据并行是提高训练吞吐量的最基本方法 之一。它将模型参数和优化器状态复制到多个 GPU 上，然后 将整个训练语料库分配到这些 GPU 上。这样，每个 GPU 只 需要处理分配给它的数据，并执行前向和反向传播以获取梯 度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量 的梯度，以更新所有 GPU 上的模型。这样，由于梯度的计算 在不同 GPU 上是独立进行的，数据并行机制具有高度可扩展 性，可以通过增加 GPU 数量来提高训练吞吐量。此外，该技 术的实现简单，大多数现有的流行深度学习库已经实现了数 据并行，例如 TensorFlow 和 PyTorch。 • 流水线并行：流水线并行旨在将 LLM 的不同层分配到 多个 GPU 上。特别地，在 Transformer 模型中，流水线并行 将连续的层加载到同一 GPU 上，以减少在 GPU 之间传输已 经计算的隐藏状态或梯度的成本。然而，流水线并行的朴素实 现可能导致 GPU 利用率降低，因为每个 GPU 必须等待前一 个 GPU 完成计算，从而导致不必要的气泡开销 [193]。为了减 少流水线并行中的这些气泡，GPipe [193] 和 PipeDream [194] 提出了填充多个数据批量和异步梯度更新技术，以提高流水 线效率。 • 张量并行：张量并行也是一种常用的技术，旨在为多 GPU 加载而分解 LLM。与流水线并行不同，张量并行专注 于分解 LLM 的张量（参数矩阵）。对于 LLM 中的矩阵乘法 操作 Y = XA，参数矩阵 A 可以按列分成两个子矩阵 A1 和 A2，从而将原式表示为 Y = [XA1, XA2]。通过将矩阵 A1 和 A2 放置在不同的 GPU 上，矩阵乘法操作将在两个 GPU 上并行调用，并且可以通过跨 GPU 通信将两个 GPU 的输出 组合成最终结果。目前，张量并行已经在几个开源库中得到 支持，例如 Megatron-LM [66]，并且可以扩展到更高维度的 张量。此外，Colossal-AI 还为更高维度的张量实现了张量并 行 [197–199]，并特别针对序列数据提出了序列并行 [200]，可 以进一步分解 Transformer 模型的注意力操作。
2. **ZeRO**：ZeRO（Zero Redundancy Optimizer）技术，由 DeepSpeed [65] 库提出，专注于解决数据并行中的内存冗余问题。 如前所述，数据并行需要每个 GPU 存储 LLM 的相同副本， 包括模型参数、模型梯度和优化器参数。然而，并非所有上 述数据都需要在每个 GPU 上保留，这将导致内存冗余问题。 为了解决这个问题，ZeRO 技术旨在仅在每个 GPU 上保留部 分数据，而当需要其余数据时可以从其他 GPU 中检索。具 体而言，根据三个数据部分具体的存储方式，ZeRO 提供了三 种解决方案，即优化器状态分区、梯度分区和参数分区。实 证结果表明，前两种解决方案不会增加通信开销，而第三种 解决方案会增加约 50% 的通信开销，但可节省与 GPU 数量 成比例的内存。PyTorch 实现了与 ZeRO 类似的技术，称为 FSDP [201]。
3. **混合精度训练**：以前的 PLM（例如 BERT [23]）主要使用 32 位浮点数（FP32）进行预训练。近年来，为了预训练极大的语 言模型，一些研究 [196] 开始利用 16 位浮点数（FP16），以减 少内存使用和通信开销。此外，由于流行的 NVIDIA GPU（例 如 A100）具有的 FP16 计算单元是 FP32 的两倍，FP16 的计 算效率可以进一步提高。然而，现有的研究发现，FP16 可能 导致计算精度的损失 [59, 69]，影响最终的模型性能。为了缓 解这个问题，一种替代方案称为 Brain Floating Point (BF16) 已被用于训练，它比 FP16 分配更多的指数位和更少的有效 位。对于预训练而言，BF16 通常比 FP16 在表示准确性方面 表现更好 [69]。

***整体训练建议***：在实践中，上述训练技术，特别是 3D 并行 技术，通常会联合使用以提高训练吞吐量和大模型加载。例 如，研究人员已经将 8 路数据并行、4 路张量并行和 12 路流 水线并行纳入到 BLOOM [69] 的 384 个 A100 GPU 上进行 训练。目前，开源库如 DeepSpeed [65]、Colossal-AI [137] 和 Alpa [202] 可以很好地支持这三种并行训练方法。为了减少内 存冗余，可以使用 ZeRO、FSDP 和激活 c 重计算（activation recomputation）技术 [68, 203] 来训练 LLM，这些技术已经集 成到 DeepSpeed、PyTorch 和 Megatron-LM 中。此外，混合 精度训练技术，如 BF16，也可以利用来提高训练效率和减少 显存使用，但需要硬件的必要支持（如 A100 GPU）。由于训 练大模型是一个耗时的过程，因此在早期阶段预测模型性能 并检测异常问题将非常有用。为此，GPT-4 [45] 最近引入了 一种基于深度学习堆栈的新机制，称为可预测扩展，可以使 用更小的模型对大模型进行性能预测，这对于开发 LLM 可能 非常有用。在实践中，人们还可以进一步利用主流深度学习 框架所支持的训练技术。例如，PyTorch 支持完全分片数据并 行训练算法 FSDP [201]（即 fully sharded data parallel），如 果需要，可以将部分训练计算卸载到 CPU 上。

#### 推理加速

除了上述训练策略，提高使用 LLM 的推理速度也很重 要。通常，量化技术被广泛用于减少 LLM 推理阶段的时间和 空间开销 [204]。虽然会损失一些模型性能，但量化语言模型 具有更小的模型大小和更快的推理速度 [97, 205, 206]。对于 模型量化，INT8 量化是一个流行的选择 [205]。此外，一些 研究工作尝试开发更激进的 INT4 量化方法 [97]。最近，包括 BLOOM20、GPT-J21 和 ChatGLM22 在内的公开可用的语言 模型，已经将量化模型副本发布到 Hugging Face 上。

## 适配微调

本节中，我们将介绍两种适配预 训练后的 LLM 的方法：指令微调（instruction tuning）和对 齐微调（alignment tuning）。前一种方法旨在增强（或解锁） LLM 的能力，而后一种方法旨在将 LLM 的行为与人类的价 值观或偏好对齐。进一步，我们还将讨论用于模型快速适配 的高效微调方法。接下来，我们将详细介绍这两种方法。

### 指令微调

本质上，指令微调是在自然语言格式的实例（instance）集合 上微调预训练后的 LLM 的方法 [62]。这种方法与有监督微 调 [61] 和多任务提示训练 [28] 密切相关。为了进行指令微调， 我们首先需要收集或构建指令格式（instruction-formatted） 的实例。然后，我们使用这种格式的实例以有监督的方式微调 LLM（例如使用序列到序列的损失进行训练）。==指令微调后， LLM 可以展现出泛化到未见过任务的卓越能力 [28, 62, 64]， 即使在多语言场景下也能有不错表现== [98]。

最近的一篇综述 [213] 系统性地概述了指令微调。相比之 下，我们主要关注在 LLM 上指令微调的效果，并提供详细的 实例收集和模型微调的方法和策略。此外，我们还讨论了用 指令微调来满足用户实际需求的方法，这在现有的 LLM 中被 广泛应用，例如 InstructGPT [61] 和 GPT-4 [45]。

#### 格式化实例的构建

![[LLM-Survey-prompt.png]]

通常情况下，一个指令格式的实例包括一个任务描述（称为 指令）、一对输入-输出以及少量示例（可选）。作为重要的公 共资源，现有的研究已经发布了大量带标注的自然语言格式 的数据（可见表 6 中的可用资源列表）。接下来，我们将介绍 两种主要的构建格式化实例的方法（可见图 5 中的插图），然 后讨论几个构建实例关键因素。

1. **格式化已有数据集**：在指令微调被提出之前，几项早期的研 究 [210, 212, 214, 215] 通过收集来自不同领域（例如文本摘 要、文本分类和翻译）的实例来创建有监督的多任务训练数 据集。作为指令微调实例的一种重要来源，用自然语言的任务描述来格式化这些多任务训练数据集是相当方便的。具体 来说，最近的工作 [28, 61, 62, 93] 使用人类撰写的任务描述 来增广带标注的数据集，这些描述通过解释任务目标来指导 LLM 理解任务。例如，在图 5（b）中，每个问答任务的实 例都添加了一个任务描述“请回答下列问题”。在指令微调之 后，LLM 可以通过遵循任务描述很好地泛化到其他未见过的 任务上 [28, 62, 64]。特别地，==指令被证明是影响 LLM 任务泛 化能力的关键因素== [62]。为了更好地为指令微调生成标注实 例，一种名为 PromptSource 的众包平台 [209] 被提出，可以 有效地创建、共享和验证不同数据集的任务描述。此外，一些 研究 [28, 212, 216] 还尝试通过为指令微调特殊设计的任务描 述，来反转已有实例的输入-输出对。例如，对于一个已有的 问题-答案对，我们可以通过基于以问题预测答案的方式来创 建一个新实例（例如，“请基于以下答案生成一个问题：”）。此 外，还有一些工作 [217] 利用启发式任务模板将大量无标注的 文本转换为带标注的实例。
2. **格式化人类需求**：尽管大量的训练实例已经通过添加指令进 行格式化，但它们主要来自公共的 NLP 数据集，任务描述缺乏多样性或与人类真实需求不匹配 [61]。为了解决这个问题，InstructGPT [61] 建议采用真实用户提交给 OpenAI API 的 查询作为任务描述。==用户查询以自然语言表示，很适合引导出 LLM 遵循指令的能力。此外，为了丰富任务的多样性，标注 者还要为真实生活中的任务编写指令，包括开放式生成、开 放式问答、头脑风暴和聊天等。然后让另一组标注人员直接 按照将这些指令作为输出进行回答。最后，将指令（即采集的 用户查询）和期望的输出（即人工编写的答案）配对作为一 个训练实==例。值得注意的是，InstructGPT 还将这些以自然语 言格式化的真实世界任务用于对齐微调（在第 5.2 节中讨论）。 进一步地，GPT-4 [45] 还设计了潜在高风险的指令，并监督 微调模型拒绝这些指令以确保安全。此外，为减轻人工标注 的负担，几种半自动化的方法 [218–220] 提出将现有实例输入 到 LLM 中生成多样的任务描述和实例来构建实例。
3. **构建实例的关键因素**：指令实例的质量对模型的性能有重要 影响。在此，我们讨论了一些实例构建中的关键因素。
	- *增加指令*：大量研究已经证明扩大任务数量可以极大地 提高 LLM 的泛化能力 [28, 62, 93]。==随着任务数量的增加，模 型性能最初呈现连续增长的趋势；但任务数量达到一定水平 时，模型性能的提升变得微不足道== [64, 93]。一个合理的猜测 是，一定数量的代表性任务可以提供相对充足的知识，而添 加更多的任务可能不会带来额外的收益 [64]。此外，==从例如长 度、结构和创造力等多个方面增强任务描述的多样性也是有 益的== [28]。至于每个任务所需的实例数量，已有研究发现少量 实例通常可以使模型的泛化性能达到饱和 [62, 64]。然而，==将某些任务的实例数量进一步增加（例如数百个）可能会潜在 地导致过拟合并影响模型性能== [93]。
	- *设计格式*：指令的格式设计也是影响 LLM 泛化性能的 一个重要因素 [93]。通常来说，==我们可以向现有数据集的输 入-输出对添加任务描述和可选的示例，其中任务描述是 LLM 理解任务 [93] 的最关键部分。此外，使用适当数量的示例作 为示范 [64]，对模型可以产生实质性的改进==，这也减轻了其 对指令工程的敏感性 [62, 64]。然而，将其他部分（例如避免 事项、原因和建议）添加到指令中对 LLM 的性能提升十分轻 微，甚至会产生不利的影响 [93, 207]。为了引出 LLM 的逐步 推理能力，一些最近的工作 [64] ==建议包含面向推理数据集的 CoT 实例，例如算术推理。已经有研究表明，同时使用包含 和不包含 CoT 的样本微调 LLM，可以在各种下游任务中取 得良好的性能==，包括需要多级推理能力的任务（例如常识问 答和算术推理），以及不需要多级推理的任务（例如情感分析 和抽取式问答） [64, 99]。

总的来说，指令多样性似乎比实例数量更重要，因为表现 良好的 InstructGPT [61] 和 Alpaca [220] 使用的指令（或实 例）比 Flan 系列的 LLM [62, 64] 数量更少但更加多样化。此 外，邀请标注者构建人类真实需求的任务比使用特定数据集 的任务更有用。但是，目前仍然缺乏如何标注来满足人类需求指令的指南，使得任务构建在某种程度上更具启发性。为 减少人力成本，我们可以重用现有的格式化数据集（表 6），或 使用现有的 LLM 自动构建指令 [218]。

#### 指令微调策略

与预训练不同，因为只需要使用较少数量的实例进行训练，指令微调通常更加高效。==指令微调可以被视为一个有监督的训 练过程，其优化过程与预训练有一些不同 [64]，比如训练目标函数（如序列到序列的损失）和优化参数设置（如更小的批量 大小和学习率）==。这些细节需要在实践中特别注意。除了这些 优化参数设置，指令微调还需要考虑两个重要方面：
- **平衡数据分布**：由于指令微调涉及多种任务的混合，因此在微 调过程中平衡不同任务的比例非常重要。一种广泛使用的方 法是实例比例混合策略 [87]，即将所有数据集合并，然后从混 合数据集中按比例采样每种实例。此外，根据最近的研究发 现 [64, 99]，提高高质量数据集（例如 FLAN [62] 和 P3 [209]） 的采样比例通常可以带来性能提升。==同时，在指令微调期间通 常会设置一个最大容量，以限制数据集中可以包含的最大实 例数 [87]，这是为了防止较大的数据集挤占整个采样集合 ==[87, 99]。在实践中，根据不同的数据集，最大容量通常设置为几 千或几万个实例 [62, 64]。
- **结合指令微调和预训练**：为了使微调过程更加有效和稳定， OPT-IML [99] 在指令微调期间加入了预训练数据，==这可以看作是对模型的正则化==（regularization）。此外，一些研究并没有使用单独的两阶段训练过程（预训练和指令微调），而是尝试混合使用预训练数据（即纯文本）和指令微调数据（即指令 格式数据），用多任务学习的方式从头训练模型 [87, 210]。具 体而言，GLM-130B [97] 和 Galactica [34] 将指令格式数据集 作为预训练语料库的一小部分来预训练 LLM，这有可能同时 获得预训练和指令微调的优势。

#### 指令微调效果

在这部分中，我们讨论了指令微调对 LLM 的两个主要方面的 影响。

1. **性能改进**：尽管指令微调仅在有限数量的数据上进行了微调， 但已成为改进或发掘 LLM 能力的重要方式 [64]。最近的研究 在多个规模上（从 7700 百万到 5400 亿不等）对 LM 进行了实 验，表明不同规模的模型都可以从指令微调中受益 [64, 216]， 随着参数规模的增加，性能也得到了提升 [98]。此外，经过 指令微调的较小模型甚至可以比未经微调的较大模型表现更 好 [28, 64]。除了模型规模外，指令微调在不同的模型架构、 预训练目标和模型适配方法上都展现出稳定的改进效果 [64]。 在实践中，指令微调向现有 LM（包括小型 PLM）的能力提 升提供了一种通用的方法 [64]。此外，与预训练相比，因为 LLM 所需的指令数据数量明显较少于预训练数据，指令微调 成本较低。
2. **任务泛化性**：指令微调鼓励模型理解用于完成任务的自然语 言指令。==它赋予 LLM 遵循人类指令执行特定任务的能力（通 常被视为一种涌现能力），即使在未见过的任务上也能够执 行== [64]。大量研究已经证实了指令微调在已见过和未见过的 任务上都有卓越的性能表现 [99, 216]。此外，指令微调还被证 明可以帮助缓解 LLM 的一些弱点（例如生成重复内容或补全 输入但不完成相应任务）[61, 64]，从而使 LLM 具有更强的解 决现实世界任务的能力。此外，使用了指令微调训练的 LLM 可以泛化到其他语言的相关任务上。例如，BLOOMZ-P3 [98] 基于 BLOOM [69] 在纯英文 P3 任务集合 [209] 上进行微调。 有趣的是，在多语言的句子补全任务中，BLOOMZ-P3 相比 于 BLOOM 仍有超过 50% 的性能提升。==这表明指令微调可 以帮助 LLM 从仅纯英文数据集中获取通用的解决任务能力， 并将这些能力传递到其他语言上== [98]。此外，研究还发现，在 多语言的任务中，仅使用英文指令就可以产生令人满意的结果 [98]，从而减少针对特定语言的指令工程的工作量。

### 对齐微调

#### 对齐微调的背景和标准

***背景***

LLM 在多个自然语言处理任务上展示出了惊人的能 力 [55, 56, 62, 95]。但是, 这些模型有时可能表现出预期之外 的行为，例如编造虚假信息、追求不准确的目标，以及产生 有害的、误导性的和有偏见的表达 [61, 221]。对于 LLM 而言, 模型参数的预训练使用了语言建模的目标，即用单词预测进 行预训练，但这没有考虑到人类的价值观或偏好。为了避免这 些预期外的行为，一些研究 [61, 113] 提出了人类对齐，使得 LLM 的行为能够符合人类期望。但是, ==与原先的预训练和适配微调（例如指令微调）相比, 对齐微调需要考虑的标准（例 如有用性, 诚实性和无害性）十分不同==。已有研究表明对齐微 调可能会在某种程度上损害 LLM 的通用能力，这在相关研究 中被称为对齐税 [61, 222, 223]。

***对齐的标准***

近期，越来越多的研究致力于为规范 LLM 的行 为制定多样化的标准。在此，我们选取三个具有代表性的对 齐标准（即有用性、诚实性、无害性）作为要讨论的例子，这 些标准已在现有文献中得到广泛采纳 [61, 221, 222]。除此以 外, 从不同视角出发，针对 LLM 还有其他在本质上相似、或 使用相似的对齐技术的对齐标准，涵盖了其行为、意图、激励 和模型内在层面 [221]。根据不同需求，修改上述三个对齐标 准也是切实可行的，例如将诚实性替换为正确性 [113]，或者 关注某些特定的标准 [223]。接下来，我们将对上述三个代表 性的对齐标准给出简要的解释：
- *有用性*：==LLM 为了达到有用性，应当尽其所能以简明 扼要且高效的方式帮助用户解决任务或回答问题==。当需要进 一步阐明问题时，更高水平的 LLM 应展示出通过提出恰当 的问题来获取额外相关信息的能力，并表现出合适的敏感度、洞察力和审慎度 [222]。对齐微调实现 LLM 的有用性具有挑 战性，因为准确定义和衡量用户的意图很困难 [221]。
- *诚实性*：在基本层面上，诚实的 LLM 应该向用户提供 准确的内容，而不会捏造信息。更进一步，==LLM 在输出时传 达适当程度的不确定性至关重要，以避免任何形式的欺骗或 信息误传==。这需要模型了解其能力和知识水平（例如“知道自 己不知道什么”）。根据过去的研究 [222], 与有用性和无害性 相比，诚实性是一个更客观的标准，因此诚实性对齐依赖的 人力可能更少。
- *无害性*：无害性要求模型生成的语言不得是冒犯性或 歧视性的。==在最大限度地发挥其能力的前提下，模型应能够 检测到隐蔽的为恶意目的而发送的请求==。理想情况下，当模 型被诱导去执行危险行为（如犯罪）时，LLM 应礼貌地拒绝。 然而，哪些行为被认为是有害的以及在多大程度上有害因不 同的个人或社会而不同 [222]，这在很大程度上取决于谁在使 用 LLM、提出的问题类型以及使用 LLM 的背景（如时间）。

我们可以看到的，这些对齐的标准相当主观，是基于人 类认知制定的。因此，很难将它们直接制定为 LLM 的优化目 标。但是，在现有的研究中，有许多方法可以在对齐微调时达 到这些标准。一个有前景的技术是红队攻防 [85, 224]，它以对 抗性的方式手动或自动地探测 LLM，使之生成有害输出，然 后再更新模型防止此类输出。

#### 人类反馈的收集

在预训练阶段，LLM 使用大规模语料库，以语言建模为训练 目标进行训练。然而，这样的训练目标缺乏人类对 LLM 输出 的主观和定性评估（在本综述中称为人类反馈）。**高质量的人 类反馈对于将 LLM 向人类的偏好和价值观对齐非常重要**。在本部分，我们将讨论如何筛选出优秀的人类标注者来进行反 馈数据收集。

- **标注人员的选择**：在现有的工作中，==生成人类反馈数据的主要 方法是人工标注== [61, 113, 225]。这凸显了人类标注者在反馈 收集的重要作用。为了提供高质量的反馈数据，标注人员应 具有合格的教育水平和优秀的英语能力。例如，Sparrow [113] 要求标注人员在英国出生，母语为英语，并且至少获得本科学历。此外，在 [223] 中，高优先级任务中的标注人员约一半 是从 Amazon Mechanical Turk 平台上招募的拥有硕士学位 的美国本土人员。即便如此，一些研究 [82, 225] 发现标注人 员与研究人员的意图仍然会不匹配，这可能使得人类反馈质 量下降并导致 LLM 产生预期外的输出。为了解决这个问题， InstructGPT [61] 进一步实施了筛选过程，通过评估标注人员 与研究人员之间意图的一致性来选择标注人员。具体而言，==研 究人员首先标注少量的数据，然后衡量他们自己和标注人员 之间的标注一致性。选择一致性最高的标记者继续后续的标 注工作==。一些其他的工作 [226] 使用一组“优秀标注者”来确 保人类反馈的高质量。研究人员评估标注人员的表现，并选 择一组表现良好的人类标注员（例如高一致性）作为优秀标 注者。优秀标注者将优先与研究人员合作后续的研究。在标 注人员进行标注的过程中，提供详细的标注指令与实时的指 导是有帮助的 [82]，可以进一步规范标注结果。
- **人类反馈的收集**：在现有的工作中，主要有==三种方法从人类 标注者中收集反馈和偏好数据==。
	1. *基于排序的方法*：在早期的工作中 [225, 227]，==标注人 员通常以较为粗略的方式（即只选择最佳的候选结果）评估模 型生成的输出结果，而不考虑更精细的对齐标准==。然而，不同 的标注者可能对最佳候选结果的选择持有不同的意见；同时， 这种方法忽略了未被选中的样本。这可能导致不准确或不完 整的人类反馈。为了解决这个问题，随后的研究 [113, 223] 引 入了 Elo 评分系统 ，通过一一比较所有候选输出结果来生成 一个偏好排序。候选输出的排序将用于调整模型更倾向的输 出，从而产生更可靠和更安全的结果。
	2. *基于问题的方法*：此外==，通过回答研究人员设计的特定 问题，标注人员可以提供更详细的反馈 [72]，这些问题能够覆 盖不同的对齐标准以及其他对 LLM 的约束条件==。特别地，在 WebGPT [72] 中，为了帮助模型从检索到的文档中过滤和利 用相关信息，标注人员需要回答关于检索到的文档对于回答 给定输入是否有帮助的选择题。
	3. *基于规则的方法*：此外，在许多研究中，基于规则的 方法也被用来提供更详细的人类反馈。作为一个经典例子， Sparrow [113] ==不仅选择了标注人员挑选的最佳回复，还设计 了一系列规则来测试模型生成的回复是否符合有用、正确和 无害的对齐标准==。通过这种方式，研究人员可以获得两种类型 的人类反馈数据：（1）通过一一比较模型输出结果的质量来获 得偏好反馈，和（2）通过收集人类标注者的评估（即针对输出结果违反规则的程度打分）来获得规则反馈。此外，GPT4 [45] 利用一组（基于 GPT-4 本身的）零样本分类器作为基 于规则的奖励模型，可以自动地确定模型生成的输出是否违 反了一组人类编写的规则。

接下来，我们将重点关注一种被广泛应用于 LLM（如 ChatGPT）中的技术，即基于人类反馈的强化学习（RLHF）。 在下面，我们将介绍如何通过学习人类针对 LLM 回答用户请 求的反馈，来实现在第 5.2.1 节中介绍的对齐标准。

#### 基于人类反馈的强化学习

为了使 LLM 与人类价值观保持一致，人们提出了 RLHF (Reinforcement Learning from Human Feedback) ，使用收集到的人类反馈数据对 LLM 进行微调，有助于 改进对齐的指标（例如，有用性，诚实性和无害性）。RLHF 采用强化学习（RL）算法（例如，近端策略优化（Proximal Policy Optimization, PPO） [81]）通过学习奖励模型使 LLM 适配人类反馈。这种方法将人类纳入训练的循环中来开发对 齐得良好的 LLM，如 InstructGPT [61]。

**基于人类反馈的强化学习系统**：==RLHF 系统主要包括三个关 键组件：要对齐的 PLM、从人类反馈中学习的奖励模型，以 及训练 LM 的 RL 算法==。
- 具体来说，PLM 通常是一个生成模 型，它使用现有的 PLM 参数进行初始化。例如，OpenAI 在其 第一个主流的 RLHF 模型 InstructGPT [61] 中使用 1750 亿 参数量的 GPT-3，而 DeepMind 在其 GopherCite 模型 [226] 中使用 2800 亿参数模型 Gopher [59]。
- 此外，奖励模型（RM） 提供（学习得到的）指导信号，==这些信号反映了人类对 LM 生成的文本的偏好==，通常以标量值的形式表示。奖励模型通 常具有两种形式：经过微调的 LM 或使用人类偏好数据重新 训练的 LM。现有工作通常采用与要对齐的 LM [61, 226] 具有不同参数尺度的奖励模型。例如，OpenAI 使用 60 亿参数 量的 GPT-3，DeepMind 使用 70 亿参数量的 Gopher 作为 奖励模型。
- 最后，为了使用来自奖励模型的信号优化 PLM， 我们设计了一种特定的 RL 算法用于大规模模型的微调。具 体地，PPO [81] 是一种在现有工作中广泛使用的 RL 对齐算 法 [61, 113, 226]。

**基于人类反馈的强化学习的关键步骤**：图 6 说明了 RLHF 的 整个三步过程 [61, 82]，具体如下所述。
1. **监督微调**：为了使 LM 具有初步执行所需行为的能力，通常需要收集一个包含输入提示（指令）和所需输出的监督数 据集，以对 LM 进行微调。这些提示和输出可以在确保任务 多样性的情况下由人工标注人员针对某些特定任务编写。例 如，InstructGPT [61] 要求人工标注者编写提示（例如，“列 出五个关于我如何重拾对职业热情的想法”）和一些生成式任 务（如开放域问答、头脑风暴、聊天和重写）的期望输出。请 注意，第一步在特定的场景中是可选的。
2. **训练奖励模型**：第二步是使用人类反馈的数据训练 RM。 具体来说，我们向 LM 中输入采样的提示（来自监督数据集 或人类生成的提示），以生成一定数量的输出文本，然后邀请 人工标注员为这些输入-输出对标注偏好。标注过程可以以多 种形式进行，常见的做法是对生成的候选文本进行排序标注， 这样可以减少因标注者不同带来的差异。最后，训练 RM 预 测人类偏好的输出。在 InstructGPT 中，标注员将模型生成 的输出从最好到最差进行排名，然后训练 RM（即 60 亿参数 量的 GPT-3）来预测排名。
3. **强化学习微调**：在这一步骤中，LM 的对齐微调可以被 形式化为 RL 问题。在这种情况中，RL 问题的策略（policy） 由 PLM 给出（将提示作为输入并返回输出文本），行动空间 （action space）是 LM 的词表，状态（state）是目前生成的 token 序列，奖励（reward）则由 RM 提供。为了避免 LM 显 著偏离初始（微调前）的模型，通常在奖励函数中纳入一项 惩罚项。例如，InstructGPT 在使用 PPO 算法对抗 RM 来 优化 LM 时，对于每个输入提示，InstructGPT 计算当前 LM 和初始 LM 生成的结果之间的 KL 散度作为惩罚项。值得注 意的是，可以通过多次迭代第二步和最后一步来更好地对齐 LLM。

### 高效微调

在上文中，我们讨论了指令微调和对齐微调的方法，以使 LLM 适应特定的目标。由于 LLM 包含大量的模型参数，进行全参 数微调将会有较大开销。在本节中，我们将讨论如何对 LLM 进行高效微调。我们首先回顾几种用于 Transformer 语言模 型的代表性参数高效微调方法，然后总结现有关于参数高效 微调 LLM 的工作。

#### 参数高效微调方法

在现有文献中，参数高效微调（parameter-eﬀicient fine-tuning） [228, 228–231] 是一个重要的课题，旨在减少可训练参数的数 量，同时尽可能保持良好的性能。接下来，我们将简要回顾四 种用于 Transformer 语言模型的参数高效微调方法，包括适 配器微调（adapter tuning）、前缀微调（prefix tuning）、提示 微调（prompt tuning）和低秩适配（LoRA）。

1. ***适配器微调***：适配器微调在 Transformer 模型中引入了小型 神经网络模块（称为适配器） [232]。为了实现适配器模块， 在 [232, 233] 中提出了一个瓶颈架构，它首先将原始特征向量 压缩到较小的维度（然后进行非线性变换），然后将其恢复到 原始维度。==适配器模块将被集成到每个 Transformer 层中，通 常使用串行插入的方式，分别在 Transformer 层的两个核心 部分（即注意力层和前馈层）之后==。另外，在 Transformer 层中也可以使用并行适配器 [234]，其将两个适配器模块与注意 力层和前馈层并行放置。==在微调过程中，适配器模块将根据特定的任务目标进行优化，而原始语言模型的参数将在此过 程中保持不变==。通过这种方式，我们可以在微调过程中有效 地减少可训练参数的数量。
2. ***前缀微调***：前缀微调 [229] 在语言模型的每个 Transformer 层 前添加了一系列前缀，这些前缀是一组可训练的连续向量。==这 些前缀向量具有任务的特异性，可以视为虚拟的 token 嵌入==。 为了优化前缀向量，文章 [229] 提出了一种重参数化技巧，即 学习一个将较小矩阵映射到前缀参数矩阵的 MLP 函数，而不 是直接优化前缀。经证明，该技巧对于稳定训练很有帮助。==优化后，映射函数将被舍弃，只保留派生的前缀向量以增强与特 定任务相关的性能==。由于只有前缀参数会被训练，因此可以实 现参数高效的模型优化。类似于前缀微调，p-tuning v2 [235] 特别为自然语言理解而在 Transformer 架构中引入了逐层提 示向量，并且还利用多任务学习来联合优化共享的提示。==已 经证明，它能有效提高不同参数规模的模型在自然语言理解 任务上的性能==。
3. ***提示微调***：与前缀微调不同，提示微调 [230, 236] 主要是在输入 层中加入可训练的提示向量 24。
	- 基于离散提示方法 [238, 239]， 它通过包含一组软提示 token（以自由形式 [236] 或前缀形 式 [230]）来扩充输入文本，然后将扩充后的输入用于解决特 定的下游任务。==在实现中，任务特定的提示嵌入与输入文本 嵌入相结合，然后输入到语言模型中==。P-tuning [236] 提出了 一种自由形式来组合上下文、提示和目标 token，适用于自然 语言理解和生成的架构。他们还通过双向 LSTM 学习了软提示 token 的表示。
	- 另一种称为提示微调的代表性方法 [230] ， 直接在输入前加入前缀提示。在训练过程中，只有提示嵌入 会根据特定任务的监督进行学习。然而，==由于该方法在输入 层只包含少量可训练参数，已发现其性能高度依赖底层语言 模型的能力== [230]。
> 很好理解，LLMs 的能力越强，相同提示词能够获得的反馈就越好。

4. ***低秩适配***：Low Rank Adaptation [231] ==通过添加低秩约束来近 似每层的更新矩阵，以减少适配下游任务的可训练参数==。考虑 优化参数矩阵 W 的情况。更新过程可以写成一般形式：W ← W + ∆W。LoRA 的基本思想是冻结原始矩阵 W ∈ Rm×n， 同时通过低秩分解矩阵来近似参数更新矩阵 ∆W = A · B⊤， 其中 A ∈ Rm×k 和 B ∈ Rn×k 是用于任务适配的可训练参数， r ≪ min (m, n) 是降低后的秩。==LoRA 的主要优点是可以大大 节省内存和存储使用==（例如 VRAM）。此外，人们可以只保留 一个大型模型副本，同时保留多个用于适配不同下游任务的 特定低秩分解矩阵。此外，还有几项研究讨论了如何以更有 原则的方法设置秩，例如基于重要性分数的分配 [240] 和无需 搜索的最优秩选择 [241]。

除了上述方法，还有大量关于 Transformer 语言模型的 高效微调的研究。然而，对于高效微调的更全面讨论超出了 本文的范围，可以在相关论文中找到 [228, 234]。

#### LLM 上的参数高效微调

随着 LLM 的兴起，高效微调吸引了越来越多的研究关注，以 开发一种更轻量级的下游任务适配方法。

特别地，LoRA [231] 已广泛应用于开源 LLM（如 LLaMA 和 BLOOM） 以 实 现 参 数 高 效 微 调。 在 这 些 研 究 尝 试 中， LLaMA 及其变体因其参数高效微调而受到了广泛关注。例 如，Alpaca-LoRA [242] 是通过 LoRA 训练出的 Alpaca [220] （一个经过微调的 70 亿 LLaMA 模型，包含 5.2 万个人类指示 遵循演示）的轻量级微调版本。在不同语言或模型大小方面，都有对 Alpaca-LoRA 广泛的探索，可以在收集页面找到 25。此外，LLaMA-Adapter [243] 将可学习的提示向量插入到每 个 Transformer 层中，其中提出了零初始化的注意力，通过减轻欠拟合提示向量的影响以改善训练。此外，他们还将此方 法扩展到多模态设置，如视觉问答。

此外，一项实证研究 [233] 检验了不同微调方法对语言模 型的影响。他们比较了四种高效微调方法，包括串行适配器微 调 [232]、并行适配器微调 [234, 244] 和 LoRA [231]，在三个 开源 LLM（GPT-J（6B）、BLOOM（7.1B）和 LLaMA（7B）） 上进行评估。根据在六个数学推理数据集上的实验结果，他 们发现这些高效微调方法在困难任务上表现不如参考基准模 型 GPT-3.5，但在简单任务上表现相当。==总体而言，LoRA 在 这些比较方法中表现相对较好，同时使用的可训练参数明显 较少==。

作为重要资源，PEFT 代码库 [245]（代表参数高效微调） 已在 GitHub 上发布 26。它包括了几种广泛使用的高效微调方 法，包括 LoRA [231]/AdaLoRA [240]、前缀微调 [229, 235]、PTuning [236] 和提示微调 [230]。此外，它支持多个语言模型，如 GPT-2 和 LLaMA，还涵盖了几个代表性的视觉 Transformer 模型（如 ViT 和 Swin Transformer）。正如在第 5.3.1 节中讨论的，现有文献中提出了大量高效 微调方法。然而，大多数方法都是在小型 PLM 上进行测试的， 而不是在 LLM 上进行的。到目前为止，有关不同高效微调方 法对不同设置或任务下 LLM 的影响仍缺乏深入的研究。

## 使用

经过预训练或适配微调之后，使用 LLM 的主要方法是为解决 各种任务设计适当的提示策略。一种典型的提示方法是将任 务描述和（或）示范（demonstration）以自然语言文本的形 式表达的上下文学习（in-context learning, ICL） [50, 55]。此 外，采用思维链提示（chain-of-thought prompting） [32] 可以 通过将一系列中间推理步骤加入提示中来增强 ICL。接下来， 我们将详细介绍这两种技术的细节。

![[LLM-Survey-ICL-vs-CoT.png]]

### 上下文学习

作为一种特殊的提示形式，ICL 在 GPT-3 [55] 中首次被提出， 而后成为使用 LLM 的典型方法。

#### 上下文学习的形式

根据 [55] 中的描述，==ICL 使用一种由任务描述和（或）作为 示范的几个任务样例构成的自然语言提示==。图 7 是 ICL 的示意 图。
- 首先，以任务描述作为开始，从任务数据集中选择一些样例作为示范。
- 然后，以特别设计的模板形式将它们按照特定 的顺序组合成自然语言提示。
- 最后，将测试样例添加到 LLM 的输入中以生成输出。基于任务示范，LLM 可以在没有显式 梯度更新的情况下识别和执行新任务。

形式上，设 $D_{k} = {f (x_1, y_1), ... , f (x_k, y_k)}$ 代表由 k 个 样例组成的一组示范，其中 $f (x_k, y_k)$ 是将第 k 个任务样例转 换为自然语言提示的函数。给定任务描述 I、示范 $D_k$ 以及新 的输入查询 $x_{k+1}$，LLM 生成的输出 $\hat{y}_{k+1}$ 的预测可以用如下公式表示 27:
$$
\text{LLM}(I,\underbrace{f(x_{1},y_{1}),...,f(x_{k},y_{k})}_{\text{demonstrations}},f(\underbrace{x_{k+1}}_{\text{input}},\underbrace{\_\_\_}_{\text{answer}}))\to\hat{y}_{k+1}
$$
其中，真实的答案 $y_{k+1}$ 留白，来由 LLM 预测。因为 ICL 的 性能非常依赖示范，在提示中合理地设计它们是一个重要的问题。根据式 (6)的构建过程，我们着眼于提示中示范格式化 的三个主要方面，包括：选择组成示范的样例，用函数 $f (·)$ 将每个样例格式化为提示，以及用合理的顺序排列示范。

综述论文 [50] 对 ICL 进行了综合的回顾，我们建议读者 参考它以获得关于该主题更加普遍和详细的讨论。和这篇综 述相比，我们特别关注应用 ICL 使用 LLM 时的两个主要方 面，即示范设计以及 ICL 背后的机制。此外，ICL 还与指令微 调（在 5.1 中已讨论）有着密切的联系，因为它们都将任务或 样例转化为自然语言的形式。然而，指令微调需要微调 LLM 来增强适配，而 ICL 仅仅是以提示的方式来使用 LLM。此外， 指令微调可以提高 LLM 执行目标任务的 ICL 能力，尤其是 在零样本设置时（仅使用任务描述）[64]。

#### 示范设计

多项研究表明，ICL 的有效性在很大程度上受到示范设计的 影响 [246–248]。根据 6.1.1 节中的讨论，我们将主要从三个方 面介绍 ICL 的示范设计，即示范选择、格式和顺序。 示范选择： 根据 [249]，不同的示范对于 ICL 的性能影响非 常大；因此，选择一个能够有效发挥 LLM 的 ICL 能力的样 例子集很重要。关于示范选择的主要方法有两种，被称作启 发式方法和基于 LLM 的方法。 • 启发式的方法：由于其简单性和低成本，现有工作广泛 采用启发式方法来选择示范。一些研究采用基于 k-NN 的检 索器来选择与查询语义相关的样例 [249, 250]。然而，他们只 是针对每一个样例进行单独选择，而不是对整个样例集合进 行评估。为了解决这个问题，基于多样性的选择策略被提出， 来选择对于特定任务最具代表性的样例集合 [251, 252]。此外， [253] 在选择样例时同时考虑了相关性和多样性。 • 基于大语言模型的方法：另一部分工作利用 LLM 来选择示例。例如，LLM 可以直接根据添加样例后的性能 提升 [254] 评估每个样例的信息量，从而进行选择。此外， EPR [255] 提出了一种两阶段检索方法，首先使用无监督方 法（例如 BM25）召回相似的样例，然后使用密集检索器（使 用 LLM 标记的正负样例训练）对它们进行排名。作为一种替 代方法，可以将示范选择任务建模为一个 RL 问题，其中 LLM 作为奖励函数，为训练策略模型提供反馈 [256]。因为 LLM 在 文本标注方面表现良好 [257]，最近的一些研究用 LLM 本身 作为没有人工干预的示范生成器 [258, 259]。 总而言之，正如 [260] 中所讨论的，对于上述两种选择方 法，ICL 中所选择的示范样例应该包含足够的有关待解决任 务的信息，并且与测试查询相关。

示范格式：在选择任务示范样例后，下一步是将它们整合及格 式化为对 LLM 的自然语言提示。一种直接的方法是用相应的 输入输出对来实例化预定义的模板 [35]。为了构建更具信息 量的模板，一些研究 [64] 考虑添加任务描述，或者通过 CoT 提示 [32] 来增强 LLM 的推理能力。例如，在 [207] 中，作者 收集了一个包含人工编写的任务描述的大规模数据集。使用 这个数据集微调后，对已见任务的性能提升，LLM 也可以在 一定程度上泛化到未见任务上。为了降低标注成本，在 [218] 中，作者提出了一种半自动化方法，通过使用由人工编写的 任务描述组成的种子集合来指导 LLM 为新任务生成任务描 述。由于人工标注不同任务的示范格式成本较高，一些工作研 究了如何自动生成高质量的示范格式。作为两种典型的方法， Auto-CoT [261] 利用 LLM 使用零样本提示 “Let’s think step by step” 来生成中间推理步骤，而 least-to-most 提示 [262] 首 先询问 LLM 来执行问题分解，然后利用 LLM 根据先前解决 的中间答案依次解决子问题。

示范顺序： LLM 有时会受到顺序偏差的影响，例如会倾向于 重复示范结尾附近的答案 [248]。因此，以合理的顺序排列示 范（例如任务样例）非常重要。早期的工作提出了一些启发 式方法来快速地找到一个良好的顺序。例如，可以直接根据 在嵌入空间中示范与查询的相似度来排列 [249]：相似度越高， 距离结尾越近。此外，全局和局部熵度量可以用来给不同的 示范顺序打分 [247]。受到信息论的启发，一些最近的研究提 出最小化压缩和传输任务标签所需的码长来整合更多的任务 信息 [263]。然而，这些方法需要额外的标记数据作为用来评 估特定示范顺序性能的验证集。为了消除这种需要，在 [247] 中，作者提出从 LLM 本身采样获得验证数据。

#### 底层机制

经过预训练，LLM 可以在不更新的情况下表现出令人惊艳的 ICL 能力。在接下来的内容中，我们将讨论与 LLM 的 ICL 能 力有关的两个关键问题，即“预训练如何影响 ICL 能力”和 “LLM 如何在推理阶段执行 ICL”。

***预训练如何影响上下文学习***?

ICL 首次在 GPT-3 [55] 中提出， 其表明 ICL 的能力随着模型规模的增大而增强。然而，一些 研究表明，小规模的 PLM 也可以通过特别设计的训练任务表 现出强大的 ICL 能力（例如学习由任务实例和查询组成的输 入来预测标签），甚至可能超越规模更大的模型 [264]。这表 明，训练任务的设计是影响 LLM 的 ICL 能力的一个重要因 素。除了训练任务之外，近期的一些研究还探索了 ICL 与预训 练语料之间的关系 [260, 265, 266]；研究表明，ICL 的性能主 要取决于预训练语料的来源而非规模 [266]。另一项研究 [265] 深入分析了训练数据分布的影响；他们发现，当训练数据可 以被聚类成许多不常见的类别，而不是均匀分布时，模型会 表现出 ICL 的能力。此外，在 [260] 中，作者从理论上给出了 解释，认为 ICL 是在具备长程连贯性的文档上进行预训练的 产物。

***大语言模型如何实现上下文学习***?

在推理阶段，因为 ICL 不 涉及显式的学习或更新，研究人员侧重分析在给定示范的情 况下 ICL 的能力如何实现。他们通常从梯度下降的角度进行 分析，并将 ICL 视为隐式微调 [60, 267]。在这一框架下，ICL 可以解释为：通过前向计算，LLM 生成关于示范的元梯度， 并通过注意力机制隐式地执行梯度下降。实验也表明，LLM 中的某些注意力头能够执行与 ICL 能力密切相关的任务无关 的原子操作（例如复制和前缀匹配） [268, 269]。为了进一步 探索 ICL 的工作机制，一些研究将 ICL 抽象为一种算法学习 过程 [270–272]。具体来说，在 [271] 中作者发现，在预训练阶 段，LLM 本质上通过其参数对隐式模型进行了编码。而在前 向计算阶段，通过 ICL 中提供的示例，LLM 可以实现诸如梯 度下降的学习算法，或者直接计算出闭式解以更新这些模型。 基于这一解释框架，LLM 能够通过 ICL 有效地学习简单的线 性函数，甚至是一些复杂函数（如决策树） [270–272]。

### 思维链提示

思维链（Chain-of-Thought，CoT）[32] 是一种改进的提示策 略，旨在提高 LLM 在复杂推理任务中的性能，例如算术推理 [273–275]，常识推理 [276, 277] 和符号推理 [32]。==不同于 ICL 中仅使用输入输出对来构造提示，CoT 将可以导出最终输出 的中间推理步骤纳入提示中==。下面我们将详细介绍使用 CoT 进行 ICL 的方法，并讨论 CoT 提示什么时候起作用及为什么 起作用。

#### 使用 CoT 的上下文学习

通常情况下，CoT 可以在小样本（few-shot）和零样本（zeroshot）设置这两种主要设置下与 ICL 一起使用。 小样本思维链：小样本 CoT 是 ICL 的一个特例，它通过加 入 CoT 推理步骤将每个示范 ⟨ 输入，输出 ⟩ 扩充为 ⟨ 输入， CoT，输出 ⟩。为了应用此策略，接下来我们讨论两个关键问 题，即如何设计合适的 CoT 提示以及如何利用生成的 CoT 推导出最终答案。 • 思维链提示设计：设计合适的 CoT 提示对于有效引出 LLM 的复杂推理能力至关重要。一种直接的方法是使用多样 的 CoT 推理路径（即对于每个问题的多个推理路径），这可 以有效增强性能 [278]。另一个基于直觉的想法是，具有复杂 推理路径的提示更有可能引出 LLM 的推理能力，这可以提 高生成正确答案的准确性 [279]。然而，这两种方法都需要标 注 CoT，这限制了它们在实践中的使用。为了克服这个限制， Auto-CoT [261] 提出了利用 Zero-shot-CoT [280]（详见 “零样 本 CoT” 部分）通过特别提示 LLM 来生成 CoT 推理路径，从 而消除了人工操作。为了提高性能，Auto-CoT 进一步将训练 集中的问题分成不同的簇，并选择最接近每个簇质心的问题， 它们应该可以很好地代表整个训练集中的问题。尽管小样本 CoT 可以被视为 ICL 的一种特殊提示情况，但相比于 ICL 中 的标准提示，示范的顺序似乎对性能影响相对较小：在大多 数任务中，重新排序样例导致的性能变化仅仅少于 2% [32]。 • 增强的思维链策略：除了丰富上下文信息外，CoT 提示 还提供了更多推断答案的选项。现有研究主要关注如何生成 多个推理路径，并尝试在得到的答案中寻找一致性 [281–283]。 例如，self-consistency[281] 提出了一种在生成 CoT 和最终答 案时新的解码策略。其首先用 LLM 生成多个推理路径，然后 对所有答案进行集成 （例如通过在这些路径中进行投票来选 择最一致的答案）。Self-consistency 极大地提高了 CoT 推理 的性能，甚至可以改善一些 CoT 提示的效果差于标准提示的 任务（例如闭卷问答和自然语言推理）。此外，[282] 中的作者 将 self-consistency 策略扩展为更通用的集成框架（延伸至提 示的集成），他们发现多样化的推理路径是 CoT 推理性能提 高的关键。上述这些方法可以很容易地集成到 CoT 提示中以 提高性能，而无需进行额外的训练。相反地，其他研究则通过 训打分模型来衡量生成的推理路径的可靠性 [278]，或者持续地使用 LLM 自己生成的推理路径进行训练 [284, 285] 以提高 性能。 零样本思维链：与小样本 CoT 不同，零样本 CoT 没有在提示 中加入人工标注的任务示范。相反，它直接生成推理步骤，然 后利用生成的 CoT 来得出答案。零样本 CoT 最初是在 [280] 中被提出的；其中，首先通过用“Let’s think step by step”提 示 LLM 来生成推理步骤，然后通过用“Therefore, the answer is”提示来得出最终答案。他们发现，这种策略在模型规模 超过一定大小时可以显著提高性能，但在小规模的模型中效 果不佳，这是涌现能力的重要表现。为了在更多任务上解锁 CoT 能力，Flan-T5 和 Flan-PaLM [64] 进一步使用 CoT 进 行了指令调整，有效增强了在未见任务上的零样本性能。

#### 关于思维链的进一步讨论

在这部分中，我们将讨论两个与 CoT 相关的基本问题，即 “CoT 何时适用于 LLM ”和“LLM 为什么能够进行 CoT 推 理”。 思维链何时适用于大语言模型? 由于 CoT 是一种涌现能 力 [47]，它只能有效增强有 100 亿或更多参数的足够大的 模型 [32]，而对小模型无效。此外，由于 CoT 通过中间推理 步骤增强了标准提示，它的效果主要体现在需要逐步推理的 任务 [32]，例如算术推理、常识推理和符号推理。然而，对于 不依赖于复杂推理的其他任务，它可能会比标准提示表现更 差 [282]，例如 GLUE 数据集 [176] 中的 MNLI-m/mm、SST-2 和 QQP。有趣的是，CoT 提示带来的性能提升似乎只有在标 准提示表现较差的情况下才会较为显著 [32]。 大语言模型为什么能够进行思维链推理? 我们将从以下两个 方面讨论 CoT 的基本机制。 • 思维链能力的来源：关于 CoT 能力的来源，研究人员 普遍假设其可以归因于使用代码进行训练，因为在代码数据 上训练过的模型表现出了强大的推理能力 [46, 286]。直观上， 代码数据具有规范的算法逻辑和编程流程，这可能有助于提 高 LLM 的推理性能。然而，这个假设仍然缺乏公开的消融 实验作为证据（有和没有在代码上的训练）。此外，指令微调 似乎不是获得 CoT 能力的关键原因，因为有实验表明，在非 CoT 数据上进行指令微调不会提高模型使用 CoT 完成任务 的性能 [64]。 • 提示中组成部分的影响： CoT 提示与标准提示之间的 主要区别在于在最终答案之前加入了推理路径，因此，一些 研究人员调查了推理路径中不同组成部分的影响。具体而言， 最近的一项研究首先定义了 CoT 提示中的三个关键组成部 分，即符号（symbols）（例如算术推理中的数值量）、模式 （patterns）（例如算术推理中的方程）和文本（text）（即不是符 号或模式的其余 token）[287]。结果表明，后两部分（即模式 和文本）对模型的性能至关重要，去除其中任何一部分都会 导致性能显著下降。然而，符号和模式的正确性似乎并不关键。此外，文本和模式之间存在共生关系：文本有助于 LLM 生成有用的模式，而模式则可以帮助 LLM 理解任务并生成额 外文本以帮助解决任务 [287]。 总的来说，CoT 提示提供了一种通用而灵活的方法来引 出 LLM 的推理能力。还有一些工作尝试将这种技术扩展至 多模态任务 [288] 和多语言任务 [289]。除了直接通过 ICL 和 CoT 使用 LLM，最近还有一些研究探索了如何将 LLM 的能 力专业化于特定任务 [290–292]，这被称为模型专业化（model specialization）[293]。例如，在 [293] 中，研究人员用 LLM 生 成推理路径，然后再用这些推理路径微调小规模的语言模型 Flan-T5 [64]，从而将 LLM 的数学推理能力专业化。模型专 业化可以应用于解决各种任务，如问答 [294]、代码生成 [295] 和信息检索 [296]。

## 能力评估

为了检验 LLM 的有效性和优越性，已有研究采用了大量的任 务和基准数据集来进行实证评估和分析。首先，我们会介绍 LLM 在语言生成和语言理解方面的三种基本评估任务。然后 会介绍 LLM 在几种更复杂的设定或目标下的高级任务。最 后，会讨论现有的基准和实证分析。

### 基础评测任务

在本部分中，我们主要关注 LLM 的三种评估任务，即**语言生 成**、**知识利用**和**复杂推理**。需要注意的是，我们并不打算对所 有相关任务进行完整覆盖，而是只关注 LLM 领域中最广泛讨 论或研究的任务。接下来，我们将详细介绍这些任务。

#### 语言生成

#### 知识利用

#### 复杂推理

### 高级能力评估

除了上述基本评测任务外，LLM 还展现出一些需要特殊考虑 的高级能力。在本节中，我们将讨论几种有代表性的高级能 力及其相应的评测方法，包括与人类对齐、与外部环境的互 动、工具操作等。接下来，我们将详细讨论这些高级能力。

#### 与人类对齐

与人类对齐（human alignment）指的是让 LLM 能够很好地符 合人类的价值和需求，这是在现实世界应用中广泛使用 LLM 的关键能力。

#### 与外部环境的互动

#### 利用外部工具

### 公开基准与经验性分析

## 展望

[^1]: Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, “A neural probabilistic language model,” J. Mach. Learn. Res., vol. 3, pp. 1137–1155, 2003.