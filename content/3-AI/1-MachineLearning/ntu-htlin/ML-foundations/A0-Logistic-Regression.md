---
publish: "true"
tags:
  - æœºå™¨å­¦ä¹ 
  - æ—è½©ç”°
  - ML
date: 2024-01-14
---
## Logistic Regression Problem

è€ƒè™‘è¿™æ ·çš„æƒ…å½¢ï¼šåœ¨åŒ»é™¢ä¸­åŒ»ç”Ÿéœ€è¦æ ¹æ®ç—…äººçš„æƒ…å†µï¼Œå¯¹å…¶æ¥ä¸‹æ¥æ‚£å¿ƒè„ç—…çš„æ¦‚ç‡ä½œå‡ºé¢„æµ‹ï¼Œç»™å‡ºæ‚£ç—…çš„æ¦‚ç‡ã€‚è¿™ç§æƒ…å†µä¸æ˜¯ç®€å•çš„äºŒå…ƒåˆ†ç±»ï¼Œä¹Ÿä¸æ˜¯çº¿æ€§å›å½’ï¼Œå› ä¸ºå…¶è¾“å‡ºçš„å€¼æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œå¤„äº 0~1 ä¹‹é—´ã€‚

è¿™ç§é—®é¢˜é€‚åˆä½¿ç”¨ ***Logistic Regression*** ï¼Œå…¶å­¦ä¹ æµç¨‹å¦‚ä¸‹ï¼š
- ![[A0-Logistic-Regression-learning-flow.png]]
- æœ‰æ—¶ç§°ä¹‹ä¸ºâ€œsoftâ€äºŒå…ƒåˆ†ç±»ï¼Œå…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š$f(\mathbf{x})=P(y=+1|\mathbf{x})\in[0,1]$ ï¼›

>[!warning] Why don't translate it as "é€»è¾‘å›å½’"?
>å‘¨å¿—åè¥¿ç“œä¹¦ï¼ˆPage 58ï¼‰ä¸ŠæŒ‡å‡ºï¼Œä¸­æ–‡çš„é€»è¾‘ä¸æ­¤å¤„çš„ logistic å«ä¹‰ç›¸å»ç”šè¿œï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªç®—æ³•ä¸­å–å¯¹æ•°æ“ä½œå æ®äº†å¾ˆé‡è¦çš„åœ°ä½ï¼Œå› æ­¤å»ºè®®ç§°ä¹‹ä¸ºâ€œå¯¹æ•°å‡ ç‡å›å½’â€ï¼Œæˆ–è€…å¹²è„†ä¸è¯‘ã€‚

### Feature of Data

Logistic Regression çš„è®­ç»ƒæ•°æ®æ¯”è¾ƒéš¾å¾—ï¼Œå› ä¸ºæˆ‘ä»¬**æ— æ³•è·å–ä»¥æ¦‚ç‡æ ‡å¥½çš„æ ·æœ¬**ï¼Œå› ä¸ºæˆ‘ä»¬åªèƒ½çŸ¥é“ä¸€ä¸ªç—…äººæ˜¯å¦æ‚£äº†å¿ƒè„ç—…ï¼Œè€Œä¸æ˜¯çŸ¥é“å…¶æ‚£ç—…æ¦‚ç‡ï¼š
-  ![[A0-Logistic-Regression-soft-binary-classify.png]]
- å®é™…çš„æ•°æ®æ˜¯**æœ‰å™ªéŸ³çš„**ï¼Œå› ä¸ºé«˜æ¦‚ç‡æ‚£ç—…çš„äººå®é™…ä¸Šå¹¶ä¸ä¸€å®šæ‚£ç—…ï¼Œä½æ¦‚ç‡æ‚£ç—…çš„äººå´æœ‰å¯èƒ½æ‚£ç—…ï¼Œè¿™å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§æœä»æ­£æ€åˆ†å¸ƒçš„é‡‡æ ·ã€‚

### Logistic Function

å¯¹äº Logistic Regression ï¼Œå…¶è¾“å…¥æ ·æœ¬çš„ç‰¹å¾å‘é‡ä¸º $\mathbf{x}=(x_0,x_1,x_2,...,x_d)$ ï¼Œ
- ä¸ºå„ç»´åˆ†é…æƒé‡åç›¸ä¹˜ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªåˆ†å€¼ $s=\sum\limits_{i=0}^{d}w_{i}x_{i}=\mathbf{w}^{T}\mathbf{x}$ ï¼Œå¦‚æœèƒ½å¤Ÿå°†ç»“æœ $s$ æ˜ å°„åˆ° $[0,1]$ åŒºé—´å†…ï¼Œå°±å¾—åˆ°äº†æ¦‚ç‡ï¼›
- å®ç°è¿™ä¸€æ˜ å°„çš„å‡½æ•°ç§°ä¸º Logistic Function ï¼Œå®ƒçš„å½¢å¼æ˜¯ $\theta(s)=\frac{e^{s}}{1+e^{s}}=\frac{1}{1+e^{-s}}$ ï¼Œè¿™æ˜¯ä¸€ä¸ª**å…‰æ»‘çš„**ã€**å•è°ƒé€’å¢çš„**ã€**S å½¢çš„**ï¼ˆsigmoidï¼‰å‡½æ•°ï¼š![[A0-Logistic-Regression-logistic-func.png]]
- å¯ä»¥çœ‹å‡ºï¼Œè¿™ä¸ªå‡½æ•°åœ¨ $|s|$ è¾ƒå¤§æ—¶å˜åŒ–å¹³ç¼“ï¼Œè€Œåœ¨ $|s|\rightarrow 0$ æ—¶å˜åŒ–å‰§çƒˆï¼Œå‡½æ•°å…³äº $(0,\frac{1}{2})$ å¯¹ç§°ï¼Œå› æ­¤æœ‰ $\theta(-s)=1-\theta(s)$ ï¼›

å¦å¤–ï¼Œæˆ‘ä»¬è€ƒè™‘ $1-\theta=1-\frac{1}{1+e^{-s}}=\frac{e^{-s}}{1+e^{-s}}$ ï¼Œåˆ™å–å¯¹æ•°åæœ‰ $\ln(\frac{\theta}{1-\theta})=s$ ï¼Œå¦‚æœæŠŠ $\theta(\mathbf{x})$ çœ‹ä½œ $\mathbf{x}$ ä¸ºæ­£ä¾‹çš„å¯èƒ½æ€§ï¼Œåˆ™ $1-\theta$ å°±æ˜¯ $\mathbf{x}$ ä¸ºåä¾‹çš„å¯èƒ½æ€§ï¼Œ$\frac{\theta}{1-\theta}$ å°±æ˜¯ $\mathbf{x}$ ä½œä¸ºæ­£ä¾‹çš„**ç›¸å¯¹å¯èƒ½æ€§**ï¼Œç§°ä¸º**å‡ ç‡**ï¼ˆoddsï¼‰ï¼Œå–å¯¹æ•°åå°±èƒ½åŒ–ä¸ºçº¿æ€§å…³ç³»ï¼Œè¿™å°±æ˜¯è¥¿ç“œä¹¦ä¸­è®¤ä¸ºå®ƒåº”è¯¥å«ä½œå¯¹æ•°å‡ ç‡çš„ç”±æ¥ï¼›*ä¸è¿‡è¿™é‡Œæ—è€å¸ˆæ²¡æœ‰æåŠï¼Œä»…ä½œè¡¥å……äº†è§£*

ç»¼åˆèµ·æ¥ï¼Œæˆ‘ä»¬å¯¹ Logistic Regression çš„å‡è®¾ä¸º $h(\mathbf{x})=\theta(\mathbf{w}^{T}\mathbf{x})=\frac{1}{1+e^{-\mathbf{w}^{T}\mathbf{x}}}$ ï¼Œå®ƒæ˜¯å¯¹ç›®æ ‡å‡½æ•° $f(\mathbf{x})=P(y=+1|\mathbf{x})$ çš„è¿‘ä¼¼ã€‚

### ç»ƒä¹ ï¼šç†è§£ Logistic Regression ä¸äºŒå…ƒåˆ†ç±»

![[A0-Logistic-Regression-quiz-logistic-vs-binary.png]]

## Logistic Regression Error

å›æƒ³ä¸€ä¸‹å­¦è¿‡çš„ä¸‰ç§çº¿æ€§æ¨¡å‹ï¼Œå®ƒä»¬çš„ scoring function éƒ½æ˜¯ä¸€æ ·çš„ï¼š$s=\mathbf{w}^{T}\mathbf{x}$ ï¼Œåˆ†æä¸‰è€…çš„ç‰¹ç‚¹æˆ‘ä»¬å°è¯•==æ¨å¯¼ Logistic Regression çš„é”™è¯¯ä¼°è®¡==ï¼š
- ![[A0-Logistic-Regression-linear-models.png]]

Logistic Regression çš„ç›®æ ‡å‡½æ•°ä¸º $f(\mathbf{x})=P(y=+1|\mathbf{x})$ ï¼Œå†™æˆç›®æ ‡åˆ†å¸ƒçš„å½¢å¼ï¼Œç­‰ä»·äº 
$$
P(y|\mathbf{x})=\begin{cases}f(\mathbf{x})&\text{for }y=+1 \\ 1-f(\mathbf{x})&\text{for }y=-1\end{cases}
$$
### Cross-Entropy

è€ƒè™‘è¿™æ ·çš„æ ·æœ¬æ•°æ®ï¼š$\mathcal{D}=\{(\mathbf{x}_{1},\circ),(\mathbf{x}_{2},\times),...,(\mathbf{x}_{N},\times)\}$ ï¼Œè¦ä½¿ç”¨ç›®æ ‡å‡½æ•°ç”Ÿæˆè¿™æ ·çš„æ•°æ®ï¼Œå…¶æ¦‚ç‡ï¼ˆprobabilityï¼‰ä¸º 
$$
P(\mathbf{x}_{1})f(\mathbf{x}_{1})\times P(\mathbf{x}_{2})(1-f(\mathbf{x}_{2}))\times...\times P(\mathbf{x}_{N})(1-f(\mathbf{x}_{N}))
$$
ï¼Œç±»ä¼¼çš„ï¼Œæˆ‘ä»¬çš„å‡è®¾ $h$ è¦åŒæ ·ç”Ÿæˆè¿™æ ·çš„æ•°æ®ï¼Œå…¶å¯èƒ½æ€§ï¼ˆlikelihoodï¼‰ä¸º 
$$
P(\mathbf{x}_{1})h(\mathbf{x}_{1})\times P(\mathbf{x}_{2})(1-h(\mathbf{x}_{2}))\times...\times P(\mathbf{x}_{N})(1-h(\mathbf{x}_{N}))
$$
ï¼Œå› æ­¤å¦‚æœå‡è®¾ä¸ç›®æ ‡æ¥è¿‘ï¼Œé‚£ä¹ˆåº”æœ‰ 
$$
h\approx f \Longleftrightarrow \text{likelihood}(h)\approx \text{probability of using }f
$$

ç›®æ ‡å‡½æ•° $f$ æ˜¯ç†æƒ³çš„ï¼Œå› æ­¤å…¶äº§ç”Ÿå¯¹åº”æ ·æœ¬çš„æ¦‚ç‡æ˜¯å¾ˆå¤§çš„ï¼Œå› æ­¤æœ€ä½³ä¼°è®¡ *g* åº”å½“å–è‡ªæ‰€æœ‰å‡è®¾ä¸­å¯èƒ½æ€§æœ€é«˜è€…ï¼š$g=\underset{h}{arg} \max\{\text{likelihood}(h)\}$ ï¼Œè€Œå¯¹äº Logistic Regression çš„å‡è®¾å‡½æ•° $h(\mathbf{x})=\theta(\mathbf{w}^{T}\mathbf{x})$ ï¼Œç”±å…¶å¯¹ç§°æ€§å¯å¾—ï¼š
$$
\begin{aligned}\text{likelihood}(h)&=P(\mathbf{x}_{1})h(\mathbf{x}_{1})\times P(\mathbf{x}_{2})(1-h(\mathbf{x}_{2}))\times...P(\mathbf{x}_{N})(1-h(\mathbf{x}_{N}))\\ &=P(\mathbf{x}_{1})h(\mathbf{x}_{1})\times P(\mathbf{x}_{2})h(-\mathbf{x}_{2}))\times...P(\mathbf{x}_{N})h(-\mathbf{x}_{N}))\end{aligned}
$$
ï¼Œä»è€Œå¾—çŸ¥å‡è®¾ $h$ çš„å¯èƒ½æ€§ä¸å‡è®¾åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„ç»“æœçš„è¿ä¹˜æˆæ­£æ¯”ï¼š
$$
\text{likelihood}(\text{logistic }h)\propto\prod\limits_{n=1}^{N}h(y_{n}\mathbf{x}_{n})
$$

æˆ‘ä»¬å–æ‰€æœ‰ Logistic Regression å‡è®¾çš„æœ€å¤§è€…ï¼Œå³æœ‰ 
$$
\underset{h}{\max }\{\text{likelihood}(\text{logistic }h)\}\propto\prod\limits_{n=1}^{N}h(y_{n}\mathbf{x}_{n})
$$ 
ï¼Œä»¥æƒé‡å‘é‡æ›¿æ¢å…¶ä¸­çš„å‡è®¾ $h$ï¼Œåˆ™å¾—åˆ° 
$$
\underset{\mathbf{w}}{\max }\{\text{likelihood}(\mathbf{w})\}\propto\prod\limits_{n=1}^{N}\theta(y_{n}\mathbf{w}^{T}\mathbf{x}_{n})
$$ 
ï¼Œå¯¹å…¶å–å¯¹æ•°ï¼Œå¾— 
$$
\underset{\mathbf{w}}{\max }\{\ln\prod\limits_{n=1}^{N}\theta(y_{n}\mathbf{w}^{T}\mathbf{x}_{n})\}=\underset{\mathbf{w}}{\max }\sum\limits_{n=1}^{N}\ln\theta(y_{n}\mathbf{w}^{T}\mathbf{x}_{n})=\underset{\mathbf{w}}{\min}\sum\limits_{n=1}^{N}-\ln\theta(y_{n}\mathbf{w}^{T}\mathbf{x}_{n})
$$ 
ï¼Œå°† $\theta(s)=\frac{1}{1+e^{-s}}$ ä»£å…¥å¹¶å¢æ·»ä¸€ä¸ªå–å‡å€¼çš„æ“ä½œï¼ˆä¸ºäº†ä¸ä¹‹å‰çš„é”™è¯¯ä¼°è®¡åœ¨å½¢å¼ä¸Šç»Ÿä¸€ï¼‰ï¼Œå¾—åˆ° 
$$
\underset{\mathbf{w}}{\min} \underbrace{\frac{1}{N}\sum\limits_{n=1}^{N}\ln(1+e^{-y_{n}\mathbf{w}^{T}\mathbf{x}_{n}})}_{E_{in}(\mathbf{w})}=\underset{\mathbf{w}}{\min} \frac{1}{N}\sum\limits_{n=1}^{N}\text{err}(\mathbf{w},\mathbf{x}_{n},y_{n})
$$
æ­¤æ—¶ï¼Œæˆ‘ä»¬ç§° $\text{err}(\mathbf{w},\mathbf{x},y)=\ln(1+e^{-y\mathbf{w}^{T}\mathbf{x}})$ ä¸º [cross-entropy error](https://en.wikipedia.org/wiki/Cross-entropy?useskin=vector) ï¼Œè¿™å°±æ˜¯ Logistic Regression çš„é”™è¯¯ä¼°è®¡å‡½æ•°ã€‚

### ç»ƒä¹ ï¼šç†è§£äº¤å‰ç†µé”™è¯¯ä¼°è®¡

![[A0-Logistic-Regression-quiz-cross-entropy.png]]

## Gradient of Logistic Regression Error

æ¥ä¸‹æ¥æˆ‘ä»¬æ€è€ƒå¦‚ä½•ä½¿ $E_{in}(\mathbf{w})$ å–å¾—æœ€å°ï¼š
- æ€è€ƒä¹‹å‰çš„æƒ…å†µï¼Œ$E_{in}(\mathbf{w})$ å¦‚æœæ˜¯è¿ç»­ã€å¯å¾®ã€å‡¸çš„ï¼ˆçŸ©é˜µçš„äºŒé˜¶å¯¼æ•°å½¢å¼å¦‚æœæ˜¯æ­£å®šçš„ï¼Œé‚£ä¹ˆå°±æ˜¯å‡¸çš„ï¼‰ï¼Œé‚£ä¹ˆå°±å¾ˆå®¹æ˜“æ‰¾åˆ° $\nabla E_{in}(\mathbf{w})=0$ ï¼Œæ­¤å¤„è‡ªç„¶æ˜¯æœ€å°å€¼ç‚¹ï¼›
- å› æ­¤å¯¹å…¶æ±‚å¯¼ï¼š![[A0-Logistic-Regression-min-Ein.png]]

ä½†æ˜¯ï¼Œåœ¨ä»¤ $\nabla E_{in}(\mathbf{w})=0$ æ—¶ï¼Œå¹¶ä¸åƒä¹‹å‰é‚£ä¹ˆè½»æ¾ï¼šæˆ‘ä»¬å¯ä»¥å°† $\nabla E_{in}(\mathbf{w})= \frac{1}{N}\sum\limits_{n=1}^{N}\theta(-y_{n}\mathbf{w}^{T}\mathbf{x}_{n})(-y_{n}\mathbf{x}_{n})$ çœ‹ä½œæ˜¯å¯¹ $-y_{n}\mathbf{x}_{n}$ æ±‚ $\theta-\text{weighted}$ çš„åŠ æƒå¹³å‡ï¼Œè¦ä½¿å…¶ä¸ºé›¶ï¼Œæœ‰ä¸¤ç§å¯èƒ½ï¼š
1. æ‰€æœ‰ $\theta(-y_{n}\mathbf{w}^{T}\mathbf{x}_{n})=0$ ï¼Œå›æƒ³ Logistic Function çš„å®šä¹‰ï¼Œæ­¤æ—¶ç›¸å½“äº $-y_{n}\mathbf{w}^{T}\mathbf{x}_{n}\rightarrow -\infty$ ï¼Œè¿™æ„å‘³ç€æ¯ä¸ª $y_{n}$ éƒ½ä¸è¾“å…¥æ ·æœ¬åŠ æƒåçš„ $\mathbf{w}^{T}\mathbf{x}_{n}$ æ˜¯åŒå·çš„ï¼Œå³**è¾“å…¥æ ·æœ¬ $\mathcal{D}$ æ˜¯çº¿æ€§å¯åˆ†çš„**ï¼›ä½†è¿™ç§æƒ…å†µå¯é‡è€Œä¸å¯æ±‚ğŸ˜¢
2. æ‰€ä»¥ä¸å¾—ä¸æ±‚è§£ $\nabla E_{in}(\mathbf{w})=0$ ï¼Œè¿™åˆæ˜¯ä¸€ä¸ªéçº¿æ€§çš„æ–¹ç¨‹ï¼Œæ²¡æ³•åƒçº¿æ€§å›å½’é‚£æ ·ä¸€æ­¥ç™»å¤©åœ°æ‰¾åˆ°ç¡®åˆ‡çš„è§£ã€‚äºæ˜¯ï¼Œæˆ‘ä»¬æ±‚åŠ©äº *PLA* çš„æ€è·¯ï¼š
	- PLA ä¸­æˆ‘ä»¬è¿­ä»£åœ°æ›´æ–° $\mathbf{w}_{t}$ï¼Œå…¶è§„åˆ™æ˜¯è‡ª $\mathbf{w}_{0}$ èµ·å§‹ï¼Œæ¯è½®ä¿®æ­£ä¸€ä¸ªé”™è¯¯ï¼š$\mathbf{w}_{t+1}\leftarrow \mathbf{w}_{t}+\underbrace{1}_{\eta}\cdot\underbrace{([\text{sign}(\mathbf{w}_{t}^{T}\mathbf{x}_{n})\ne y_{n}]\cdot y_{n}\mathbf{x}_{n})}_{\mathbf{v}}$ ï¼Œå½“ä¸å†çŠ¯é”™æ—¶å°±å¾—åˆ°æœ€ä½³è¿‘ä¼¼ *g* ï¼›
	- PLA çš„æ€è·¯æ˜¯ä¸€ç§ iterative optimization approach ï¼Œæˆ‘ä»¬ä¸‹ä¸€éƒ¨åˆ†è®²çš„æ¢¯åº¦ä¸‹é™æ³•ä¹Ÿæ˜¯å¦‚æ­¤ã€‚

### ç»ƒä¹ ï¼šç†è§£æ ·æœ¬å¯¹æ¢¯åº¦çš„å½±å“

![[A0-Logistic-Regression-quiz-gradient.png]]
- æœ€å°çš„ $y_{n}\mathbf{w}^{T}\mathbf{x}_{n}$ ä»£è¡¨é¢„æµ‹æ˜¯é”™è¯¯çš„ï¼Œè€ŒçŠ¯é”™åœ¨è¿­ä»£å¼ä¼˜åŒ–çš„æ–¹æ³•é‡Œæƒé‡æ›´é«˜ï¼›

## Gradient Descent

### Understand Gradient

è¿­ä»£å¼ä¼˜åŒ–çš„æ­¥éª¤å¯ä»¥å†™ä½œï¼š$\mathbf{w}_{t+1}\leftarrow \mathbf{w}_{t}+\eta\mathbf{v}$ ï¼Œå¯¹äº *PLA* ï¼Œè¿™é‡Œçš„ $\mathbf{v}$ æ¥è‡ªäºçŠ¯é”™ï¼Œè€Œå¯¹ Logistic Regression ï¼Œä»æ¢¯åº¦çš„è§†è§’æ¥çœ‹ï¼Œç›¸å½“äºä»å±±é¡¶ï¼ˆæˆ–å±±å¡çš„æŸä¸ªé«˜åº¦ï¼‰ä¸‹é™åˆ°å±±è°·ï¼š
- ![[A0-Logistic-Regression-gradient-down.png]]
- $\mathbf{v}$ ä»£è¡¨ç€æ–¹å‘ï¼ˆå•ä½å‘é‡ï¼‰ï¼Œ$\eta$ ä»£è¡¨ç€æ­¥é•¿ï¼ˆé€šå¸¸ä¸ºæ­£ï¼‰ï¼Œè¦æ‰¾åˆ° $E_{in}$ çš„æœ€å°å€¼ï¼Œé€šå¸¸ä½¿ç”¨è´ªå¿ƒç­–ç•¥â€”â€”æ¯ä¸€æ­¥éƒ½èµ°åˆ°å½“å‰æœ€ä½å¤„ï¼š$\underset{||\mathbf{v}||=1}{\min}E_{in}(\mathbf{w}_{t}+\eta\mathbf{v})$ ï¼Œ
- ä¸è¿‡è¿™ä¸ªå½¢å¼ä»ç„¶ä¸æ˜¯çº¿æ€§çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»å¾®å…ƒçš„è§’åº¦â€”â€”==æ¯æ¬¡éƒ½èµ°ä¸€å°æ­¥ï¼Œåˆ™æ¯ä¸€å°æ­¥éƒ½æ˜¯çº¿æ€§çš„==â€”â€”æ¥è¿‘ä¼¼æˆçº¿æ€§ï¼š$E_{in}(\mathbf{w}_{t}+\eta\mathbf{v})\approx E_{in}(\mathbf{w}_{t})+\eta\mathbf{v}^{T}\nabla E_{in}(\mathbf{w}_{t})$ ï¼Œè¿™é‡Œ $\eta\rightarrow 0$ ï¼Œå®é™…ä¸Šæ˜¯æ³°å‹’å±•å¼€åœ¨é«˜ç»´ä¸­çš„åº”ç”¨ï¼›

å› æ­¤æœ€å°çš„ $E_{in}$ å¯ä»¥é€šè¿‡**è´ªå¿ƒç­–ç•¥**è¿‘ä¼¼åœ°è½¬æ¢ä¸ºçº¿æ€§å…³ç³»ï¼š
$$
\underset{||\mathbf{v}||=1}{\min}\{ \underbrace{E_{in}(\mathbf{w}_{t})}_{\text{known}}+\underbrace{\eta}_{\text{given positive}}\mathbf{v}^{T}\underbrace{\nabla E_{in}(\mathbf{w}_{t})}_{\text{known}}\}
$$
ï¼Œè¦ä½¿å…¶æœ€å°ï¼Œåˆ™ $\mathbf{v}$ ä¸ $\nabla E_{in}(\mathbf{w}_{t})$ æ–¹å‘ç›¸åï¼Œå³ $\mathbf{v}=-\frac{\nabla E_{in}(\mathbf{w}_{t})}{||\nabla E_{in}(\mathbf{w}_{t})||}$ ï¼›

ç»¼åˆèµ·æ¥ï¼Œæ¢¯åº¦ä¸‹é™çš„è¿­ä»£æ­¥éª¤å°±æ˜¯ï¼Œå¯¹è¶³å¤Ÿå°çš„ $\eta$ ï¼Œ$\mathbf{w}_{t+1}\leftarrow \mathbf{w}_{t}-\eta\frac{\nabla E_{in}(\mathbf{w}_{t})}{||\nabla E_{in}(\mathbf{w}_{t})||}$ ï¼›

### Fixing Learning Rate

ç°åœ¨æ¥çœ‹çœ‹ $\eta$ çš„å–å€¼ï¼š
-  ![[A0-Logistic-Regression-eta.png]] 
- æ˜¾ç„¶å®ƒè¿‡å°æ—¶æ¢¯åº¦ä¸‹é™ç¼“æ…¢ï¼Œè¿‡å¤§æ—¶ç”šè‡³å¯èƒ½å‡ºé”™ï¼Œè€Œæœ€ä½³çš„æ–¹æ³•åº”å½“æ˜¯åœ¨æ¢¯åº¦å¤§æ—¶è¾ƒå¤§ï¼Œæ¢¯åº¦å°æ—¶è¾ƒå°ï¼ŒåŠ¨æ€åœ°å˜åŒ–ï¼Œå³ä¸ $||\nabla E_{in}(\mathbf{w}_{t})||$ æ­£ç›¸å…³ï¼Œ
- ä¸å¦¨è®°å…¶ç›¸å…³æ¯”ä¾‹ä¸º $\eta^{'}$ï¼Œåˆ™æœ‰ $\mathbf{w}_{t+1}\leftarrow \mathbf{w}_{t}-\eta\frac{\nabla E_{in}(\mathbf{w}_{t})}{||\nabla E_{in}(\mathbf{w}_{t})||}=\mathbf{w}_{t}-\eta^{'}\nabla E_{in}(\mathbf{w}_{t})$ ï¼Œåœ¨ ML ä¸­ $\eta^{'}$ ç§°ä¸º ***fixing learning rate*** .

å› æ­¤ï¼Œå®Œæ•´çš„ Logistic Regression Algorithm çš„æµç¨‹æ˜¯è¿™æ ·ï¼š
0. åˆå§‹è®¾ç½®ä¸€ä¸ª $\mathbf{w}_{0}$ ï¼Œæ¯ä¸€è½®è¿­ä»£ï¼Œéƒ½åšå¦‚ä¸‹æ­¥éª¤ï¼š
1. è®¡ç®— $\nabla E_{in}(\mathbf{w}_{t})= \frac{1}{N}\sum\limits_{n=1}^{N}\theta(-y_{n}\mathbf{w}_{t}^{T}\mathbf{x}_{n})(-y_{n}\mathbf{x}_{n})$ 
2. æ¯è½®é€šè¿‡æ¢¯åº¦ä¸‹é™æ³• $\mathbf{w}_{t+1}\leftarrow \mathbf{w}_{t}-\eta^{'}\nabla E_{in}(\mathbf{w}_{t})$ è¿›è¡Œæ›´æ–°ï¼Œç›´åˆ° $\nabla E_{in}(\mathbf{w}_{t+1})= 0$ æˆ–ç»å†è¿‡è¶³å¤Ÿå¤šçš„è¿­ä»£è½®æ¬¡
3. è¿”å›æœ€åçš„ $\mathbf{w}_{t+1}$ ä½œä¸ºæœ€ä½³ä¼°è®¡ *g* ï¼›

### ç»ƒä¹ ï¼šç†è§£ Logistic Regression çš„è¿­ä»£

![[A0-Logistic-Regression-quiz-iterate.png]]