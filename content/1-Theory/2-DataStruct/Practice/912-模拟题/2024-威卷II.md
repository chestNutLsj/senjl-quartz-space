## 数据结构

### 判断题

1. 由 2023个互异节点构成的度为 2 的 BST 的个数，与由 1012 对括号构成的合法表达式一样多。

> ❌
> 2023 个互异节点的 BST，其中叶子节点数比内部节点数多 1，则叶节点数为 1012，内部节点数为 1011，因此相当于 1011 个节点的 BST 数目——Catalan (1011)
> 
> 而 1012 对括号构成的合法表达式数目为 Catalan (12)——可以理解左子为左括号，右子为右括号的二叉树，并且其中必然有一个度数为 1 的节点。

2. 权值各异且单调增加的元素 $a_1,a_2,a_3,...,a_n$ 构成的哈夫曼树 T 中，$a_{i}$ 和 $a_{i+1}$ 所对应的节点要么在同层，要么在相邻层。

> ❌
> 题中“权值各异且单调增加”的元素构成的哈夫曼树中所有叶节点是单调的，其高度与元素中的顺序没有必然关系。
> ![[2024-威卷II-huffman.png]]

3. 跳转表在某层横向跳转的次数的期望是 logn

> ❌
> 单层横向跳转的次数的期望是 `O(1)`
> 而全部的层横向跳转次数的期望才是 `O(logn)`

4. Karp-Rabin 算法指纹相同的概率为 1/M，其中 M 为字符集的大小。

> ❌
> ![[2024-威卷II-karp-rabin.png]]


5. 判断以下 big-Θ-notation 的正误：

> - $(\log n)^{\log n}=\Theta(n^{\log\log n})$ 
> 	- ✅ 两边取 log 后完全等价
> - $[\log n]!=\Theta(n^{\log\log n})$
> 	- ❌ 预测卷 I 中有详细解析，如果说 $[\log n]!=O(n^{\log\log n})$ 则是正确的
> - $n\log n=\Theta[\log(n!)]$ 
> 	- ✅ 由斯特林渐进，$\log (n!)= n\log n-n\log e+O(\log n)$ ，这里 $n\log n$ 占据了主导，因此其满足 Θ 的关系
> - $\log^{*}(\log n)\Theta[\log(\log^{*}n)]$ 
> 	- ❌ 构造 `N=n^n^n^n...` 一共 n 个幂的幂的幂...，此时 $\log^{*}N$ 将不再是一个接近常数的值，转而变成一个变量，数量级大概是 n；而 $\log^{*}(\log N)=\log^{*}N -1=O(n)$ ，而 $\log(\log^{*}N)=\log n$ ，这二者自然不是可以用 $Θ$ 可以表示的。 
> - $2^{2^{n+1}}=\Theta(2^{2^{n}})$
> 	- ❌ $\lim\limits_{n\rightarrow\infty} \frac{2^{2^{n}}}{2^{2^{n+1}}}=\lim\limits_{n\rightarrow\infty} \frac{2^{2^{n}}}{2^{2^{n}}\cdot 2^{2^{n}}}=0$

6. 在任意一棵非空 AVL 树 T1 中，删除某节点 v 后（若失衡，则复衡之）得到树 T2，在 T2 中又添入 v 节点，判断以下叙述的正误，并举出例子：
A. 若 v 是 T1 的叶节点，则 T1 和 T3 可能不相同
B. 若 v 不是 T1 的叶节点，则 T1 和 T3 一定不相同

> A ✅ 如果删除叶节点导致失衡，则旋转后未必保持原位：
> ![[2024-威卷II-AVL-del-insert.png]]
> 
> B ❌ 如果删除内部节点后未失衡，而再加入该节后失衡，则会在原来的局部发生一次复衡调整，于是有可能恢复原位：
> ![[2024-威卷II-avl-del-internal-insert.png]]

### 填空题

1. 对非法表达式 `12 3 + ! 4 * + 3` 进行求值的结果是（    ）

> 87
> ![[2024-威卷II-error-evaluate.png]]

2. 排序算法 bubble, selection, insersion, binary insersion, merge, heap, tournament, quick 的最好、平均、最坏复杂度分别是：

| algo    | bubble   | selection | insersion | binary insersion | merge        | heap         | tournament   | quick        |
| ------- | -------- | --------- | --------- | ---------------- | ------------ | ------------ | ------------ | ------------ |
| Best    | $O(n)$   | $O(n^2)$  | $O(n)$    | $O(n\log n)$     | $O(n\log n)$ | $O(n\log n)$ | $O(n\log n)$ | $O(n\log n)$ |
| Average | $O(n^2)$ | $O(n^2)$  | $O(n^2)$  | $O(n^2)$         | $O(n\log n)$ | $O(n\log n)$ | $O(n\log n)$ | $O(n\log n)$ |
| Worst   | $O(n^2)$ | $O(n^2)$  | $O(n^2)$  | $O(n^2)$         | $O(n\log n)$ | $O(n\log n)$ | $O(n\log n)$ | $O(n^2)$     |

> - 这里对于 bubble、binary insersion 来说，最好情况就是已经升序有序，则 bubble 只需要扫描一遍，而 binary insersion 虽然不用移动，但仍是需要每轮比较 $\log k$ 次，k 是该轮中 sorted 区间的长度；
> - selection 由于每轮都要从 unsorted 区间中找出最大值，因此总是要比较 n 次（虽然 n 会递减，但数列和仍然是 $O(n^2)$ 量级）；

3. 默写一遍从 `ss[]` 构造 `gs[]` 的策略

> 1) `ss[j]=j+1` 时，对所有 `i<m-j-1`，`m-j-1` 都是 `gs[i]` 的一个可选项；
> 2) `ss[j]<=j` 时，`m-j-1` 是 `gs[m-ss[j]-1]` 的可选项；

4. 考查表达式求值算法。算法执行过程中的某时刻，若操作符栈中的括号多达 2010 个，则此时栈的规模（含栈底的'\\0'）至多可能多大？试说明理由，并示意性地画出当时栈中的内容。

> `(2010+1)*4+1 = 8045` 个字符，栈中内容是：`\0+*^(+*^(+*^(...(+*^(+*^!`，其中 $+$ 和 $\times$ 可以用相同优先级的运算符 $-$ 或 $\div$ 代替。


### 算法题

1. 对于一个无重复元素的有序循环数组 num，返回其中的最小值。有序循环数组的定义是，有序数组左边任意长度的部分依次序放到数组的末尾，右边的部分依次前移。比如有序数组 `[12345]`，其可生成的有序循环数组有 `[23451]`, `[45123]`...。设计尽可能高效的算法，并给出复杂度。

> 二分搜索即可，只不过每轮只需要与首元素比较大小即可——如果大于，则搜索右侧区间，否则搜索左侧区间。
> ![[2024-威卷II-circular-arr-1.png]]
> 
> ![[2024-威卷II-circular-arr-2.png]]
> 
> ![[2024-威卷II-circular-arr-3.png]]
> 
> 如果想到了习题解析中提到的指数查找（或者期末题中的手指搜索），也是可行的思路。现在分析这种策略和二分策略的复杂度比较：
> - 二分策略的最好情况就是移动的左半部分的数量**恰好** $\frac{n}{2}$ 时，或者移动数量**恰好**是 $\frac{n}{2^{k}}$ ，此时 $k$ 是一个比较小的常数，此时可以很快查找到（即 k 次查找），但最坏情况就是移动数量在上述的 $\frac{n}{2^{k}}$ 这样的轴点附近（比如移动数量是 $0$、$n$、$\frac{n}{2}-1$ 等），这时将会需要 $O(\log n)$ 次查找；
> - 而指数查找的最好情况是移动数量接近 $n$ 时，可以在很短时间内查找到，而最坏情况则是移动数量接近 $0$ 时，则需要 $O(\log n)$ 。
> - 对二者的复杂度做个图，大致如下：
> - ![[2024-威卷II-exponential-vs-binary.png]]
> - 如果让我选择的话，指数查找策略在 n 较小时优势较大，而渐进地来看与二分法的平均复杂度相同，不过指数法更加炫技一些。


2. 一棵 BST 的节点中，有 data 域和 size 域，其中 data 域存放节点所存储的数据，size 域存放以该节点为根的子树中的节点数目。示意图如下：
![[2024-威卷II-bst-inorder-seq.png]]
设树高为 h，试写出时间复杂度为 $O(h)$ 的算法，其能够确定某个节点在中序遍历中的次序（从 1 开始）。如上图中 `x.data==21` 节点，其在中序遍历中的次序为 11。

```
func rank(T:Tree, x:node):
{
	if (x.lchild = nil)
		int r = 1;
	else int r = x.lchild.size + 1;
	
	node y=x;
	while (y!=T){
		// r表示以y为根的子树中，节点x的rank值，因此当y更新到根时，即为所求
		if (y==y.parent.rchild){
			// 当y是其父亲的右子时，y的父亲节点的左子树中所有节点及父节点本身
			// 在中序序列中都在y的前面
			if (y.parent.lchild = nil)
				r = r+1;
			else r = r+y.parent.lchild.size+1;
		}
		y = y.parent;
		
	} // end of while()
	return r;
} // end of func()
```

## 组成原理

### 判断题

1. 一次访存过程中，可能发生 TLB 命中，页表未命中，Cache 命中。

> ❌
> ![[2024-威卷II-TLB-cache-pageTable-miss.png]]
> 
> 更详细地，在 RISC-V 中，有指令 `SFENCE.VMA`[^1] 用于清空 TLB，它的使用有如下要求：
> ![[2024-威卷II-SFENCE-VMA-use.png]]
> 即 `SFENCE.VMA` 用于同步内存和内存管理数据结构（所指即是页表）的内容，也用于使与 hart 相关的地址转换 cache（即 TLB ）失效。只要页表发生了切换，就要使用这个命令保证 TLB 中的内容和主存中的内容不致歧义。
> <mark style="background: #FF5582A6;">注意，常说的 cache 其实指的是存放内存数据的那一类 cache，用于加快从物理地址寻址到内存并取出数据的过程，其中并没有涉及到地址转换，而是据地址访问。</mark>
> ![[2024-威卷II-visit-memory.png]]

2. RISC 只采用硬连线控制，而不采用微程序控制。

> ✅
> ![[2024-威卷II-RISC-vs-CISC.png]]
> 为什么 RISC 只使用硬布线控制器？
> RISC 中指令的数量有限且是定长，不会经常性地扩充，因此使用硬布线控制器既可以提高指令取指、运行的速度，又可以避免 CISC 那样常常需要修改数据通路的繁琐。

3. RISC 通用寄存器的数目比 CISC 中的数目要多。

> ✅
> 上图中可以看到 RISC 中需要的通用寄存器数更多。
> 为什么？
> 因为 RISC 指令定长、且种类较少，因此需要大量的短指令完成 CISC 中长指令同样的功能，运算中也需要更多的通用寄存器存储中间值。

4. MIPS 中 R 类型指令的数据通路设计中，一定具有读口和写口的通用寄存器组，一定有一个 ALU 用于对寄存器读出的数据进行运算，一定有一条路径使 ALU 的输出被送到某个寄存器。执行 R 类型指令时通用寄存器组的“WrEn”信号一定为 1。

> ✅
> 下面的数据通路虽然是 RISC-V 的，但是和 MIPS 的结构是一样的。
> ![[22-Datapath#支持 R 型指令的数据通路设计]]

5. 某主存按字节编址，Cache 共有 64 行，采用 4 路组相联的映射方式，主存块大小为 32B，所有编号都从 0 开始。主存第 2593 号单元所在的主存块对应的 cache 的组号是 （   ）

> $1$
> 
> - Cache 有 64 行，采用 4 路组相联，因此共有 64/4=16 组，组号= $\log_{2}16=4$ 位；
> - 主存第 2593 号单元，由于主存以字节编址（Byte），即指 2593 B 位置的单元（十进制），因此其属于第 $\lceil \frac{2593}{32}\rceil=82$ 个块，而由于所有编号都从 0 开始，因此第 82 块的编号为 81，其将会被分配到 $81\%16=1$ 编号的组。
> 
> 进一步明确 cache 的术语：
> - Cache 行：tag+data 
> - 四路组相联 Cache：每个组有 4 个 Cache 行，主存中对应的块可以存到同一个组的任意一个行，行内是全相联的，因此可以同时查看 tag 以确定是否满足要求；
> - 标记有 3 个部分，分别是 tag、组号、块内偏移，
> 	- 组号与组数有关，组数就是 Cache 容量（行数）除以路数，
> 	- 块内偏移与数据页的大小有关，通常会给出，或者与虚拟地址中页内偏移一致；
> ![[IMG_20231207_165837.jpg]]

### 选择/填空题

1. 增加同步总线带宽的手段有很多，但以下不能提高总线带宽的是（    ）
A. 采用信号线复用技术
B. 增加总线宽度
C. 采用突发传送方式
D. 提高总线时钟频率

> A 复用无论是时分复用还是频分复用，都是从原总线带宽 N 分配出去，给不同的事务使用，因此总线带宽是没有变化的。
> B 增加总线宽度，意味着单个周期内能够传送的数据量更多，因此带宽自然提升了。
> C 突发传送方式意味着不需要经过总线仲裁器仲裁，因此减少了常规的仲裁用时，故而单位数据量的用时缩短，因此带宽也提升了。
> D 增加总线时钟频率，意味着传送单位数据的用时更短，因此带宽也提升了。
> 
> 其它可以提高总线带宽的方式：
> - 分别设置数据总线和地址总线：同时传输数据和地址，而不必分成两个时钟周期；
> - 成组传送：只传一次地址，然后传多次数据；


2. 若一个 float 类型的变量 x 有机器数 0x45100000 ，则变量 x 的值是多少？

> $2304$
> 
> float 类型以 IEEE 754 存储，机器数写成二进制表示为：0100 0101 0001 0000 0000...
> 其中
> - 第 1 位是符号位，表示该值为整数；
> - 接下来 8 位存放阶码 S，其真值表示为 $T=S-Bias=00010100-(2^{8}-1)=138-127=11$ ，注意 T 是阶码表示的真值，S 是当前阶码的位模式，Bias 是阶码的偏移量；
> - 后面是小数部分，默认隐含了整数部分的 1，因此整体的表示的真值为 $1.001\times2^{11}$，二进制表示——若转成十进制，则为 $1.125\times2048=2304$


3. 对以下代码，若采用一位预测位的流水线处理器执行，预测位初始为 0，试分析当 N=10 时，该程序段中**各分支指令**的预测正确率（    ）
```
// C
int sum(int N){
	int i,j,sum=0;
	for(i=0;i<N;i++)
		for(j=0;j<N;j++)
			sum += 1;
	
	return sum;
}

// ASM
Loop-i: beq $t1,$a0,Exit-i
		add $t2,$zero,$zero
Loop-j: beq $t2,$a0,Exit-j
		addi $t2,$t2,1
		addi $t0,$t0,1
		j Loop-j
Exit-j: addi $t1,$t1,1
		j Loop-i
Exit-i:...
```

> <mark style="background: #FF5582A6;">预测位是对每个分支指令分别设置。</mark>
> - 上述代码中有两个分支指令，分别是 `for(i:10)` 和 `for(j:10)`，预测位为 0 时，表示不采纳分支——即继续执行下一条指令，而预测位为 1 时，表示采纳分支——跳出当前循环。
> - 现在考虑 `for(i:10)` 这条指令，其在一开始预测是正确的——会从外循环连续地执行下一条指令，即内循环，因此它会一直正确，直到外循环需要跳出时，才会出错，因此总共预测 11 次，正确 10 次，正确率= $10/11=0.909091$；
> - 接下来考虑 `for(j:10)` 这条指令，其在一开始预测是正确的——从内循环连续地执行下一条指令，即 `sum+=1`，但是在出循环时会出错，预测位 `0->1`，而再次进入内循环时，由于预测位是 `1`，错误又发生了，因此每轮总共预测 11 次，正确 9 次 (除第一轮外），而一共进行 10 轮，因此正确率= $\frac{{9\times9+10}}{110}=0.827273$；
> 

4. 若上述题改成二位预测位，初始时为 00，重新计算预测正确率（    ）

> - 二位预测位中，00 和 01 都表示不采纳分支，前者为强不转移，后者为弱不转移；镜像地，11 和 10 都表示采纳分支，前者为强转移，后者为弱转移；
> - 现在考虑 `for(i:10)` 这条指令，与前文类似，该外循环只有最后一次跳出循环时才会预测错误，此时正确率仍为 $10/11=0.909091$；
> - 接着考虑 `for(j:10)` 这条指令，与前文不同，该循环在跳出循环时，预测错误，预测位由此 `00->01`，而再次回到内循环时，`01` 表示仍不转移，并强化为 “强不转移”状态 `00`，此时预测正确，因此，内循环每轮预测错误 1 次，总共 10 轮，正确率为 $1- \frac{10}{110}=1-0.0909091=0.9090909$；

5. 设计指令系统时，假设采用 16 位定长指令字格式，操作码使用拓展编码方式，地址码为 6 位，包括零地址、一地址、二地址 3 种指令格式的指令，其中二地址指令有 12条，一地址指令有 254条，则零地址指令的条数最多为（   ）

> 二地址指令的 opcode 从 0000 ~ 1011
> 一地址指令的 opcode 从 1100 000000 ~ 1111 111101
> 零地址指令的 opcode 从 1111 111110 000000 ~ 1111 111111 1111111 故最多 128条

### 流水线大题

某计算机指令流水线由 6 个功能段组成，依次为 A~F，每个功能段的组合逻辑延迟分别为 80ps、30ps、60ps、50ps、60ps、20ps，最后一个功能段需要写寄存器，寄存器延时为 20ps。在这些组合逻辑块之间插入必要的流水段寄存器就可实现相应的指令流水线。理想情况下，以下各种方式所得到的时钟周期、指令吞吐率和指令执行时间各是多少？应该在哪里插入流水段寄存器？（假定插入的流水段寄存器的延时也为 20ps）
根据对以下 4 种情况的分析，你能得到什么结论？
（1）插入一个流水段寄存器，得到一个两级流水线。
（2）插入两个流水段寄存器，得到一个三级流水线。
（3）插入三个流水段寄存器，得到一个四级流水线。
（4）吞吐量最大的流水线。

改成填空题
（1）中每条指令的执行时间是 （    ）
（2）中的时钟周期为 （    ）
（3）中的吞吐量为（    ）G 条指令/s（保留两位小数）
（4）吞吐量最大的流水线分为了（    ）个流水段

> 1) 380 ps ![[2024-威卷II-pipeline-1.png]]
> 流水线中逻辑功能段的延迟取各功能段中最大者，因此总延迟为 $190\times2=380ps$
> 
> 2) 130 ps![[2024-威卷II-pipeline-2.png]]
> 时钟周期取逻辑功能段中延迟最大者—— $130$ ps
> 
> 3) 9.09 GIPS ![[2024-威卷II-pipeline=3.png]]
> 流水线吞吐量=1/最长功能段延迟，因为流水线每过一个功能段读取一条指令。因此四段流水线的吞吐量为 $\frac{1}{110\times10^{-12}}\times10^{-9}=9.09$ GIPS
> 
> 4) 5段。要使吞吐量最大，则使得功能段延迟最小即可，而上述能够最小的功能段为 $80+20=100ps$，因此应当如下划分流水段：
> ![[2024-威卷II-pipeline-4.png]]

## 操作系统

### 判断题

1. 管程将资源抽象为条件变量，通过变量值的增减来控制进程的访问。

> ❌
> 管程将资源抽象为条件变量，条件变量存放于管程之中，只有获得管程使用权的进程才可以访问这些条件变量。

2. file1 引用计数为 2，建立一个软链接 file2，再建立一个硬链接 file3，file3 建立一个硬链接 file4，则此时 file2 的引用计数为 2，file4 的引用计数为 4。

> ❌
> 符号链接的引用计数永远是 1，硬链接的引用计数等于硬链接文件数+1（文件自身）
> ![[72-符号链接与硬链接#^aa4418]]

3. 改进版 CLOCK 算法的缺页率不一定比 CLOCK 的缺页率低，但至少不会比 CLOCK 的缺页率更高。

> ❌
> ![[2024-威卷II-improved-clock.png]]
> 该研究[^2]中讲述，LRU 在缺页率很高的情况下性能较差，例如扫描磁盘的过程中；而 Clock 有和 LRU 类似的堆栈性，因此也会性能降低。而改进版 Clock 的堆栈性比 Clock 更强，因此其性能下降会更严重。
> ![[616bb2ac6f87a87b5187790f5d3cd4e.jpg]]
> 上图是连神构造的例子，完美体现了上述思想，即如果访问页面的局部性很差，改进版 Clock 的缺页率会更高。

4. OS 采用 COW 机制时，fork ()函数不会复制进程的页目录表。

> ✅
> fork 函数调用之后，这个时候因为 Copy-On-Write（COW） 的存在父子进程实际上是共享物理内存的，并没有直接去拷贝一份，kernel 把会共享的所有的内存页的权限都设为 read-only。当父子进程都只读内存，然后执行 exec 函数时就可以省去大量的数据复制开销。
> 
> 当其中某个进程写内存时，内存管理单元 MMU 检测到内存页是 read-only 的，于是触发缺页异常（page-fault），处理器会从中断描述符表（IDT）中获取到对应的处理程序。在中断程序中，kernel 就会**把触发的异常的页复制一份**，于是父子进程各自持有独立的一份，之后进程再修改对应的数据。
> 
> COW的好处是显而易见的，同时也有相应的缺点，如果父子进程都需要进行大量的写操作，会产生大量的缺页异常（page-fault）。缺页异常不是没有代价的，它会处理器会停止执行当前程序或任务转而去执行专门用于处理中断或异常的程序。处理器会从中断描述符表（IDT）中获取到对应的处理程序，当异常或中断执行完毕之后，会继续回到被中断的程序或任务继续执行。

### 简答题

1. 文件分配的三种方式是：（   ）（   ）（   ）。分别有什么特征？（存储、文件读写、可靠性）

> 顺序、链式、索引、散列
> 
> 特征：
> - 顺序分配方式：需要占用连续的磁盘块；在交互式应用场景性能较差，每个记录的查找需要穷举；可靠性较高，一个块的损坏或缺失不会影响其它块；
> - 链式分配方式：不需占用连续的磁盘块，只需要在每个块的尾部添加指向下一个块的指针即可；不能随机读写，只能连续读写，速度较慢；可靠性较差，其中一个块的损坏将导致后续所有块都无法访问；
> - 索引分配方式：虽然可以不连续地占用磁盘块，但是必须有额外的索引开销；虽然不能随机读写，但极大地降低了搜索的开销，速度较快；可靠性较高，一个块的损坏或缺失不会影响其它块，但是如果索引块坏了其中所有指向的块都无法访问。
> ![[70-文件系统#文件组织和访问]]

### 虚存大题

在一个大端存储，采用虚拟页式存储的超微计算机系统中，内存为 256 字节。页面大小为 16 字节，采用二级页表结构，每个页表项占 1 字节，页表大小 4 字节。页表项组成如下：高 4 位为二级页表或访问页面的物理页号，低 4 位分别是存在位（Exist）、访问位（Read）、修改位（Write）和保留位（Reserved）。假定当前系统中正有两个并发执行的进程，进程 A 的第一级页表起始地址为 OxBO。
整个物理内存的存储内容如下。
![[BC06E42BF6F188CE95FBA8DC2959C5EB.png]]

请描述进程 A 访问逻辑地址 Ox20、0x40 和 0x80 的地址转换计算过程。要求写出计算过程，并给出对应的一级页表项、二级页表项和访存单元的物理地址和对应的存储内容

> - 内存 256 字节，每个页面 16 字节，故内存被划分为 256/16=16 个页面；由于每个页表项为 1 字节，页表大小为 4 字节，因此每个页表能存放 4 个页表项，分别指向 4 个页面，故总共二级页表需要 16/4=4 个；一级页表需要 4 条页表项，分别指向二级页表（不是页面，而是单独的页表），故需要 4/4=1 个一级页表；综上，虚拟地址中为了表示页内偏移，需要 4 个位，为了表示二级页表中的哪个页表项，需要 2 个位，为了表示一级页表中的哪个页表项，需要 2 个位。
> - 从上图可以看出，进程 A 的一级页表存放在 0xB0 起始处，四个页表项分别为 8C，9A，A9，00；
> - 故访问 0x20 (0010 0000)时，会访问的一级页表项是 0xB0 处的 00 号页表项——8C (1000 1100)，其含义是该表项所指页表存在、可读、不可写，并且起始地址是 0x80；从而二级页表项是 0x80 处 10 号页表项——BA (1011 1010)，其含义是所指页面存在、不可读、可写，起始地址是 0xB0；得到物理地址 B0（二级页表项的高 4 位+页内偏移），故访问物理地址 0xB0 得到内容 8C；
> - 而访问 0x40 (0100 0000)时，会访问的一级页表项是 0xB0 处的 01 号页表项——9A (1001 1010)，其含义是该表项所指页表存在、不可读、可写，并且起始地址是 0x90；从而二级页表项是 0x90 处 00 号页表项——AA (1010 1010)，其含义是所指页面存在、不可读、可写，起始地址是 0xA0；得到物理地址 A0（二级页表项的高 4 位+页内偏移），故访问物理地址 0xA0 得到内容 00；
> - 访问 0x80（1000 0000）时，会访问的一级页表项是 0xB0 处的 10 号页表项——A9 (1010 1001)，其含义是该表项所指页表存在、不可读、不可写，并且起始地址是 0xA0；从而二级页表项是 0xA0 处 00 号页表项——00 (0000 0000)，其含义是所指页面不存在，故会发生缺页中断；
> 
> 这个题里有个小 tips：高 4 位是会在低位补 0 后再作为地址使用的，这是因为高四位自身是无法承担起表示整个内存空间的任务，从而也导致了访问地址其实不是逐字节，而是逐 4 字节。

## 计算机网络

### 判断题

1. 一个 VLAN 端口可以被标记为多种 VLAN 颜色。

> ✅
> 考查的是 trunking 干线这个知识点。下面图来自 Cisco 的问答网页：
> ![[2024-威卷II-VLAN-trunk.png]]

2. CSMA/CD 和 p 坚持型 CSMA 都需要分时槽。

> ✅
> ![[2024-威卷II-p-persistent.png]]
> - 另外，1 坚持型是不停监测信道是否空闲，非坚持型则是检测到信道占用后等待随机一段时间，因此这二者不需要分时槽；
> 
> CSMA/CD 中也需要分时槽，不过该时槽与 jam 信号的传递有关，时槽最少需要 jam 信号传递完整个网络的时间的二倍：
> ![[2024-威卷II-csma-cs-slot.png]]

3. 一个 VLAN 组成一个广播域。

> ✅
> 姊妹问题：一个 VLAN 组成一个冲突域。 ❌
> 
> <mark style="background: #FF5582A6;">区分广播域和冲突域</mark>：
> - 广播域：广播分组能够直接到达的区域；
> - 冲突域：竞争广播信道的一组节点构成一个冲突域；而对于 VLAN 来说，可以有多个交换机的多个端口组成一个广播域，因此这个域中有多个物理链路，只有在同一条物理链路上的节点才是冲突的，对其他的广播域中节点来说，二者并不在同一冲突域中；
> - 进一步地，交换机的作用就是在有限的广播域中，划分出尽可能多的冲突域，于是每个冲突域的节点量少了，同一时间可以通信的冲突域多了，信道的利用率更高了。
> 

### TCP 拥塞控制

TCP 使用慢启动算法，初始阈值为 400KB，接收方窗口大小为 600KB，mss=100KB，填写以下列表：

![[IMG_1318(20231206-223823).png]]

A~H 依次填入阿拉伯数字：

> 300 500 600 600 100 400 300 18
> - 收到第一个包，则在原来拥塞窗口的基础上+1MSS，即 2MSS+1MSS=300KB，由于发送窗口取拥塞窗口和接收窗口中的较小者，因此发送方窗口的大小也为 300KB，同时阈值并没有变化，因此仍为 4MSS 即 400KB；
> - 收到 3 中所有包的确认，而 3 中所有包的序号为 3、4、5、6，即有 4 个包，而收到所有确认后本该翻倍得到 8 个 MSS，但是阈值为 400KB，因此进入拥塞避免状态，拥塞窗口仅增加 1 个 MSS，即 4MSS+1MSS=500KB，同样发送窗口取 $\text{min}\{500,600\}$ 即 500KB；
> - 同理，收到 4 中所有包的确认，仍处于拥塞避免状态，因此拥塞窗口增加 1 个 MSS，即 $\text{sender windows}=\text{min}\{500+100,600\}=600$ KB；
> - 当收到 5 中所有包的确认，拥塞避免状态仍持续，拥塞窗口增加 1 个 MSS，但是 $\text{sender windows}=\text{min}\{600+100,600\}=600$ KB，发送方窗口大小并没有变化，只是拥塞窗口增加了；
> - 一旦发生超时事件，拥塞窗口缩减为 1MSS，同样发送窗口随之缩减为 1MSS，即 100KB，此时阈值缩减为超时事件前拥塞窗口的一半，即 $\lfloor 7MSS/2\rfloor=3MSS$，即为 300KB；
> - 根据发送窗口即可很好地确定发送包的序号—— 第 1 个 RTT 发送 0 号包，第 2 个 RTT 发送 1、2 号包，第 3 个 RTT 发送 3、4、5、6 号包，第 4 个 RTT 发送 7、8、9、10、11 号包，第 5 个 RTT 发送 12、13、14、15、16、17 号包，第 6 个 RTT 发送 18、19、20、21、22、23 号包，于是 H 空中第 18 号包的 ACK 超时，于是重传之，且发送窗口仅允许重传这一个包；
> 
> 更详细的 TCP 拥塞控制策略：
> ![[30-Transport-layer#TCP 发送方如何限制向连接发送流量的速率？]]
> ![[30-Transport-layer#发送方如何探测到拥塞？]]
> ![[30-Transport-layer#TCP 拥塞控制：策略概述]]

[^1]: [The RISC-V Instruction Set Manual, Volume II: Privileged Architecture | Five EmbedDev](https://five-embeddev.com/riscv-isa-manual/latest/supervisor.html)
[^2]: [CLOCK-Pro: An Effective Improvement of the CLOCK Replacement](https://www.usenix.org/legacy/publications/library/proceedings/usenix05/tech/general/full_papers/jiang/jiang_html/html.html)