## 数据结构

### 判断

1. BST 中删除节点 *x* 的时间复杂度是 $O(d)$，其中 *d* 为 *x* 的深度。

> ❌
> 删除根节点的复杂度是 $O(1)+O(根节点直接后继的深度)$，而题目中的 $d$ 指的是被删除节点本身的深度。
> 
> 如果题目改成时间复杂度是 $O(h)$，其中 h 是树的高度，则会正确。

### 填空

1. 给出以下问题计算成本的 Big-O-notation ： （    ） $$
T(n)=\left\{
\begin{aligned}
    &O(1)   &(n\le 1)\\
    &2\times T(\sqrt{n})+O(\log n) &(n\ge2)
\end{aligned}
\right.
$$

> ![[IMG_20231215_203854.jpg]]

2. 若仅使用 9 个字符，且出现频率分别为 `{1,1,2,3,5,8,13,33,34}`，则平均带权编码长度为：（    ）

> ![[IMG_20231215_204508.jpg]]

3. 设文本串 T、模式串 P 的长度分别为 2024 和 17，且 T 中并不包含 P，则用 BM-GS 算法做模式匹配，最好情况下需要做字符比对的次数为：（    ）

> $\lfloor 2024/17\rfloor=119$ 次
> ![[2024-威卷-III-BM-GS-algo.png]]
> 注意代码中 while 循环的判断条件——当文本长度至少要大于模式串长度时才会进行匹配，否则就会直接退出。

## 组成原理

### 判断

1. 闪存每单元能编码的比特数越多，闪存的寿命越短

> ✅
> 闪存每单元能编码的比特数越多，即一个单元能够表示多个 bit，这将导致在更新其中一个 bit 时不得不反复擦除这个整体单元（FLASH 不支持随机的单个 bit 的擦写），因而会降低该单元的寿命。
> ![[2024-威卷-III-FLASH-erase.png]]
> 更多关于 FLASH 的说法：
> - 闪存的写入端粒度和擦除端粒度不同 ✅
> - FTL 提供逻辑块地址到物理块地址的映射 ✅
> - 闪存物理页在更新之前要进行擦除 ✅
> - ![[2024-威卷-III-FLASH.png]]
> - ![[2024-威卷-III-FLASH-opt.png]]
> - [[62-SSD]]

2. SRAM 和 DRAM 在断电后都会丢失信息

> ✅
> 前者物理结构是触发器，后者是电容，这两种存储结构都是 volatile 的。
> ![[2024-威卷III-volatile-SRAM.png]]
> 更多关于 SRAM 和 DRAM 的说法：
> - SRAM 不是破坏性读出，而 DRAM 是破坏性读出。✅
> - SRAM 不需要周期性地刷新，而 DRAM 需要。✅
> - SRAM 可以同时送行地址和列地址，DRAM 需要分两次送。✅
> 	- ![[2024-威卷III-DRAM-read.png]]
> - SRAM 访问速度更快、集成度更低、发热量更大、存储成本更高，而 DRAM 与之相反。✅
> - SRAM 为什么是 static 的？
> 	- ![[2024-威卷III-SRAM-static.png]]

3. 总线仲裁中，菊链仲裁的公平性较好

> ❌
> The daisy chain arbitration scheme got its name from the structure for ***the grant line which chains through each device from the highest priority to the lowest priority***.
> The higher priority device will pass the grant line to the lower priority device ONLY if it does not want it so priority is built into the scheme.
> ![[2024-威卷III-daisy-chain-arbitration.png]]
> 总的说来，菊链仲裁还是分布式的、基于优先级的仲裁方式，因此必然存在饥饿现象；而改进为集中式的仲裁器进行仲裁，则可以保证公平。

### Cache

对以下计算两个向量点积的程序段：
```c
float dotproduct(float x[8], float y[8]){
	float sum=0.0;
	int i;
	for(i=0;i<8;i++)
		sum += x[i] * y[i];
	return sum;
}
```

请回答下列问题：
1) 假定数据 cache 采用直接映射方式，数据区容量为 32B，每个主存块大小为 16B；编译器将变量 Sum 和 i 分配在寄存器中，数组 x 存放在 00000040H 开始的 32B 的连续存储区中，数组 y 则紧跟在 x 后进行存放。该程序数据访问的命中率是多少？要求说明每次访问时 cache 的命中情况。

> 0%
> 
> 如下图所示，由于直接映射，X 的 block0 和 Y 的 block2 会映射到 Cache 的同一区域，因此从第一次访问 `x[i]` 开始，就会 miss，之后调入 X 的 block0，但是在访问 `y[i]` 时又会 miss，将之前的 X 的 block0 清除并填入 Y 的 block2……如此循环，最终，没有一次 hit，故命中率=0%。
> ![[2024-威卷III-cache-hit-rate.png]]
> 

2) 将上述（1）中的数据 cache 改用 2 路组相联映射方式，块大小改为 8B，其他条件不变，则该程序数据访问的命中率是多少？

> 50%
> 
> 如下图所示，在二路组相联中，X 的 block0 可以选择 c0 或 c1 两个块存放，而 block1 可以选择 c2 和 c3 两个块存放，其余 block 分别按照奇偶序号依此规律存放。
> 因此，当最初访问 `x[0]` 不存在时，会将 block0 调入 c0 中，接着访问 `y[0]` 不存在，会将 block4 调入 c1 中，接下来访问 `x[1]` 和 `y[1]` 都会命中，后续规律和此相同。因此总共命中率是 50%。
> ![[2024-威卷III-2-way-cache.png]]

3) 在上述（1）中条件不变的情况下，将数组 x 定义为 `float[12]`，则数据访问的命中率是多少？

> 75%
> 
> 如下图所示，访问 `x[0]` 时，miss 并将 block0调入 c0中，访问 `y[0]` 时也 miss，于是将 block3调入 c1中，接下来访问 `x[1:3]` 和 `y[1:3]` 都会命中，因此命中率是 75%。
> ![[2024-威卷III-float-cache.png]]

### 流水线

设数据通路中各主要功能单元的操作时间如下。存储器: 400ps; ALU 和加法器: 200ps; 寄存器堆读口或写口: 100ps。假设 MUX、控制单元、PC、扩展器和传输线路等的延迟忽略不计, 程序中指令的组成比例为: 取数指令占 25%; 存数指令占 10%; ALU 运算类指令占 52%; 分支指令占 11%; 跳转指令占 2%。则下面的实现方式中, 哪个更快? 快多少?

1) 单周期方式, 每条指令在一个固定长度的时钟周期内完成，其平均每条指令执行时间为（   ）

> 单周期指令周期需要选择最长指令进行设置，因此考查 load 指令：
> 取指（读存储器 400）+译码（忽略延迟）+计算取数据的地址（读寄存器 100+运算 200）+取数（读存储器 400）+写回（写寄存器 100）=1200ps=1.2ns

2) 多周期方式, 并且每类指令时钟数为: 取数-5, 存数-4, ALU-4, 分支-3, 跳转-2，其平均每条指令执行时间为（    ）

> 多周期中，每个时钟周期的长度取自一个单独操作的最长延迟，本题中即为读存储器的 400ps，因此 load 需要2000ps，store 需要1600ps，ALU 计算指令需要 1600ps，branch 需要1200ps，jump 需要800ps，对其加权平均，得每条指令执行时间= $2000\times0.25+1600\times0.10+1600\times0.52+1200\times0.11+800\times0.02=1640ps=1.64ns$

3) 流水线方式, 具体来说, 每条指令分取指令、取数/译码、执行、存储器访问和写回 5 个阶段。假定没有结构冒险; 数据冒险采用“转发”技术处理, 取数指令引发 load-use 数据冒险的概率为 50%; 分支延迟损失时间片为 1, 预测准确率为 75%; 不考虑异常、中断和访问失效引起的流水线阻塞。则流水线下平均 CPI 为（    ）

> - 数据冒险采用“转发”处理，意味着连续的 ALU 计算指令不必推迟，ALU 计算指令的 CPI 是 1；
> - 存数指令的 CPI 也是1；
> - 取数指令引发 load-use 数据冒险的概率是 50%，意味着 load 指令之后有 50%的几率推迟 1 个周期，因此 load 指令的 CPI 是 $2*0.5+1*0.5=1.5$；
> - 分支延迟损失时间片为 1，预测争取率 75%，意味着 branch 有 25%的可能预测错误，引发处罚，于是 branch 的 CPI 为 $1*0.75+2*0.25=1.25$；
> - 跳转指令必然会跳跃，因此 CPI 是2；
> 
> 综上，对 CPI 进行加权计算，流水线中平均 $\text{CPI}=1.5*0.25+1*0.1+1*0.52+1.25*0.11+2*0.02=1.1725$

## 操作系统

### 判断

1. 使用自旋锁，不能保证进程按先来后到的顺序使用 CPU 资源。

> ✅
> 自旋锁会在获取锁的条件不满足时原地死循环，因此当有多个进程在自旋等待时，会形成竞争条件——宏观上并不能决定哪个进程会在锁空闲时获取，因为当前 OS 正在执行哪个进程的自旋是不确定的。

2. SSTF 算法具有最短的平均寻道长度。

> ❌
> Shortest Seek Time First 要求每一次选择寻道目标是距离当前位置最小者，但局部的寻道距离最小不能确保全局也是最小的。

3. Ctrl+C 和 Kill -9 -1 的区别是前者的信号可能被捕获，后者的信号无法被捕获。

> ✅
> 确切地说，这二者发出的信号是不同的，前者为 SIGINT，后者为 SIGKILL，而 SIGKILL 是不可以被捕获、忽略的。
> ![[2024-威卷III-SIGKILL.png]]
> - 更详细的手册：[Termination Signals (The GNU C Library)](https://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html)

### 选择

1. FCFS、RR、SPN、SRT、HRRN 中可能导致进程饥饿的调度算法有（    ）个，有（    ）是抢占式的。

> 能够出现饥饿的调度算法：SPN、SRT
> 抢占式的调度算法：RR、SRT
> 
> ![[50-CPU调度#调度策略]]

### 简答

1. 简述硬链接、软链接的特点。

> **硬链接**：
> - 硬链接通过直接链接到文件的索引节点来完成引用，因此硬链接可以移动原始文件到不同的目录位置仍正确运行
> - 硬链接不可以跨越文件系统、磁盘卷进行链接，因为不同文件系统的索引节点未必相同
> - 硬链接不可以链接到目录，这样是为了避免目录循环问题
> - 硬链接到某个文件或删除某个文件的硬链接，会使得索引节点中的引用计数值相应地增加或减少，硬链接的索引节点中引用计数值为 0 时索引节点会被释放
> 
> **软链接**：
> - 软链接通过链接到原始文件自身来完成引用，因此原始文件如果移动到不同目录位置，软链接就会失效
> - 软链接可以跨越文件系统和磁盘卷进行链接，因为它实际指向的是文件的位置
> - 软链接可以链接到目录
> - 软链接的引用计数值永远保持为 1，删除时才会归 0

2. 某进程的两个线程 T1 和 T2 并发执行 A~F 共 6 个操作，其中 T1 执行操作 A、E、F，T2 执行操作 B、C、D。下图表示上述 6 个操作必须满足的执行顺序的要求，即 C 必须在 A 和 B 都完成后再执行……请使用信号量的 PV 操作描述 T1 和 T2 之间的同步关系，并说明所用信号量的作用及其初值。
![[2024-威卷-III-PV.png]]

> 分析可得，T2 执行 C 必须在 T1 执行完 A 之后，T1 执行 E 必须在 T2 执行完 C 之后，因此有两对同步关系。

```
sem_t sem_AC=0; // 描述AC之间的同步关系
sem_t sem_CE=0; // 描述CE之间的同步关系

T1{
	A;
	signal(sem_AC);
	wait(sem_CE);
	E;
	F;
}

T2{
	B;
	wait(sem_AC);
	C;
	signal(sem_CE);
	D;
}
```

## 计算机网络

### 判断

1. 虚电路交换网络只在建立时需要进行路由选择。

> ✅
> 虚电路交换网络的思路就是只在建立时进行路由选择，避免后续每个分组都要进行路由选择的开销，并且如此可以稳定一条虚拟的连接，从而确保延迟、吞吐量等可控。
> ![[2024-威卷III-Virtual-circuit.png]]
> ![[2024-威卷III-virtual-circuit-route-table.png]]

2. SNMP 中的管理程序和代理程序按 Client-Server 模式工作，其中管理程序运行 SNMP 服务器程序，而代理程序运行 SNMP 客户端程序。

> ❌
> 和常规模式相反，SNMP 协议中管理程序运行客户端程序，代理程序运行服务端程序：
> ![[2024-威卷III-SNMP-agent.png]]
> 这种异常是因为在 SNMP 中，管理者向受管者建立连接、发送请求。

3. 网桥的自学习算法是一种最简单的生成树算法。

> ❌
> 网桥+自学习，网桥+生成树都是对的，但是这俩可不是一个东西：
> - 前者是用来构建网桥转发表的；
> 	- ![[60-Link-layer-and-LAN#交换机转发、过滤]]
> - 后者是用来消除转发环路的；
> 	- ![[80-Switching#生成树算法]]

### 选择/填空

1. 能够抑制广播风暴的网络设备有：（    ）
A. 中继器  B. 集线器  C. 网桥  D. 路由器

> 要能抑制广播风暴，就要能够隔离广播域，而这可以由支持 VLAN 的交换机和路由器实现。
> ![[80-Switching#网络设备总览]]

2. 将一个 IP 网络划分为 3 个子网，若其中一个子网是 192.168.9.128/26，则下列网络中不可能是另外两个子网之一的是：（    ）
A. 192.168.9.0/25
B. 192.168.9.0/26
C. 192.168.9.192/26
D. 192.168.9.192/27

> 首先，可以确定所给出的子网 IP 是：192.168.9.1000 0000/26，于是如果想要另外两个子网完成对 IP 网络的划分，有两种策略：
> 1. 如果认为 IP 空间是 256 个，那么就只能是 192.168.9.0000 0000/25 作为一个子网，192.168.9.1100 0000/26 作为一个子网
> 2. 如果认为 IP 空间是 128 个，那么就只能是 192.168.9.1110 0000/27 作为一个子网，192.168.9.1100 0000/27 作为一个子网
> 而无论如何，选择 192.168.9.0000 0000/26 都是无法完成 “3个子网”划分这一要求的，因此只能选 B。


3. 主机甲和主机乙之间建立一个 TCP 连接，双方持续有数据传输，且数据无差错和丢失。若甲收到一个来自乙的 TCP 段，该段的序号为 1913、确认序号为 2046、有效载荷为 100 Bytes，则甲立即发送给乙的 TCP 段的序号和确认序号分别是（    ）、（    ）

> 2046，2014
> 
> TCP 中，确认序号是期望从对方收到的下一个子节的序号，因此甲应当发送给乙的 TCP 段的序号为2046，而 ACK 序号为1913+100=2014（注意，1913~2013已经包含了 100个字节，因此期望收到的下一个字节就是 2014）