 

# Chapter 6. Building Our Own “Arc”

In [“Reference Counting”](ch01.xhtml#arc), we’ve seen the `std::sync::Arc<T>` type that allows for shared ownership through reference counting. The `Arc::new` function creates a new allocation, just like `Box::new`. However, unlike a `Box`, a cloned `Arc` will share the original allocation, without creating a new one. The shared allocation will only be dropped once the `Arc` and all its clones are dropped.

The memory ordering considerations involved in an implementation of this type can get quite interesting. In this chapter, we’ll put more of the theory to practice by implementing our own `Arc<T>`. We’ll start with a basic version, then extend it to support _weak pointers_ for cyclic structures, and finish the chapter with an optimized version that’s nearly identical to the implementation in the standard library.

# Basic Reference Counting

Our first version will use a single `AtomicUsize` to count the number of `Arc` objects that share an allocation. Let’s start with a struct that holds this counter and the `T` object:

```
struct
```

Note that this struct is not public. It’s an internal implementation detail of our `Arc` implementation.

Next is the `Arc<T>` struct itself, which is effectively just a pointer to a (shared) `ArcData<T>` object.

It might be tempting to make it a wrapper for a `Box<ArcData<T>>`, using a standard `Box` to handle the allocation of the `ArcData<T>`. However, a `Box` represents exclusive ownership, not shared ownership. We can’t use a reference either, because we’re not just borrowing the data owned by something else, and its lifetime (“until the last clone of this `Arc` is dropped”) is not directly representable with a Rust lifetime.

Instead, we’ll have to resort to using a pointer, and handle allocation and the concept of ownership manually. Instead of a `*mut T` or `*const T`, we’ll use a `std::ptr::NonNull<T>`, which represents a pointer to `T` that is never null. That way, an `Option<Arc<T>>` will be the same size as an `Arc<T>`, using the null pointer representation for `None`.

```
use
```

With a reference or a `Box`, the compiler automatically understands for which `T` it should make your struct `Send` or `Sync`. When using a raw pointer or `NonNull`, however, it’ll conservatively assume it’s never `Send` or `Sync` unless we explicitly tell it otherwise.

Sending an `Arc<T>` across threads results in a `T` object being shared, requiring `T` to be `Sync`. Similarly, sending an `Arc<T>` across threads could result in another thread dropping that `T`, effectively transferring it to the other thread, requiring `T` to be `Send`. In other words, `Arc<T>` should be `Send` if and only if `T` is both `Send` and `Sync`. The exact same holds for `Sync`, since a shared `&Arc<T>` can be cloned into a new `Arc<T>`.

```
unsafe
```

For `Arc<T>::new`, we’ll have to create a new allocation with an `ArcData<T>` with a reference count of one. We’ll use `Box::new` to create a new allocation, `Box::leak` to give up our exclusive ownership of this allocation, and `NonNull::from` to turn it into a pointer:

```
impl
```

We know the pointer will always point to a valid `ArcData<T>` as long as the `Arc` object exists. However, this is not something the compiler knows or checks for us, so accessing the `ArcData` through the pointer requires unsafe code. We’ll add a private helper function to get from the `Arc` to the `ArcData`, since this is something we’ll have to several times.

```
    
```

Using that, we can now implement the `Deref` trait to make our `Arc<T>` transparently behave like a reference to a `T`:

```
impl
```

Note that we don’t implement `DerefMut`. Since an `Arc<T>` represents shared ownership, we can’t unconditionally provide an `&mut T`.

Next: the `Clone` implementation. The cloned `Arc` will use the same pointer, after incrementing the reference counter:

```
impl
```

We can use `Relaxed` memory ordering to increment the reference counter, since there are no operations on other variables that need to strictly happen before or after this atomic operation. We already had access to the contained `T` before this operation (through the original `Arc`), and that remains unchanged afterwards (but now through at least two `Arc` objects).

An `Arc` would need to be cloned many times before the counter has any chance of overflowing, but running `std::mem::forget(arc.clone())` in a loop can make it happen. We can use any of the techniques discussed in [“Example: ID Allocation”](ch02.xhtml#example-id-allocation) and [“Example: ID Allocation Without Overflow”](ch02.xhtml#example-handle-overflow) to handle this issue.

To keep things as efficient as possible in the normal (non-overflowing) case, we’ll keep the original `fetch_add` and simply abort the whole process if we get uncomfortably close to overflowing.

```
        
```

###### Note

Aborting the process is not instant, leaving for some time during which another thread can also call `Arc::clone`, incrementing the reference counter further. Therefore, just checking for `usize::MAX - 1` would not suffice. However, using `usize::MAX / 2` as the limit works fine: assuming every thread takes at least a few bytes of space in memory, it’s impossible for `usize::MAX / 2` threads to exist concurrently.

Just like we increment the counter when cloning, we need to decrement it when dropping an `Arc`. The thread that sees the counter go from one to zero knows it dropped the last `Arc<T>`, and is responsible for dropping and deallocating the `ArcData<T>`.

We’ll use `Box::from_raw` to reclaim exclusive ownership of the allocation, and then drop it right away using `drop()`.

```
impl
```

For this operation, we can’t use `Relaxed` ordering, since we need to make sure that nothing is still accessing the data when we drop it. In other words, every single drop of one of the former `Arc` clones must have happened before the final drop. So, the final `fetch_sub` must establish a happens before relationship with every previous `fetch_sub` operation, which we can do using release and acquire ordering: decrementing it from, for example, two to one effectively “releases” the data, while decrementing it from one to zero “acquires” ownership of it.

We could use `AcqRel` memory ordering to cover both cases, but only the final decrement to zero needs `Acquire`, while the others only need `Release`. For efficiency, we’ll use only `Release` for the `fetch_sub` operation and a separate `Acquire` fence only when necessary:

```
        
```

## Testing It

To test that our `Arc` is behaving as intended, we can write a unit test that creates an `Arc` containing a special object that lets us know when it gets dropped:

```
#[test]
```

This compiles and runs fine, so it seems our `Arc` is behaving as intended! While this is encouraging, this doesn’t prove that its implementation is fully correct. It’s advisable to use a long stress test involving many threads to gain more confidence.

##### Miri

It can also be very useful to run tests using Miri. Miri is an experimental but very useful and powerful tool to check unsafe code for various forms of undefined behavior.

It is an interpreter for Rustc’s mid-level intermediate representation. This means that it will run your code not by compiling it to native processor instructions, but instead by interpreting your code at a point when information like types and lifetimes are still available. Because of this, it runs programs significantly slower than when compiled and run normally, but it is able to detect many mistakes that would result in undefined behavior.

It includes experimental support for detecting data races, which can detect memory ordering problems.

For more details and a guide on how to use it, see: [_https://github.com/rust-lang/miri_](https://github.com/rust-lang/miri)

## Mutation

As mentioned before, we can’t implement `DerefMut` for our `Arc`, since we can’t unconditionally promise exclusive access (`&mut T`) to the data, as the data might be accessed through other `Arc` objects.

However, what we can do, is to allow it conditionally. We can make a method that only gives out a `&mut T` if the reference counter is one, proving that there’s no other `Arc` object that could be used to access the same data.

This function, which we’ll call `get_mut`, will have to take a `&mut Self` to make sure nothing else can use this same `Arc` to access the `T`. Knowing that there’s only one `Arc` would be meaningless if that one `Arc` can still be shared.

We’ll need to use acquire memory ordering to make sure that threads that previously owned a clone of the `Arc` are no longer accessing the data. We need to establish a happens before relationship with every single `drop` that led to the reference counter being one.

This only matters when the reference counter is actually one; if it’s higher, we’ll not provide a `&mut T`, and the memory ordering is irrelevant. So, we can use a relaxed load, followed by a conditional acquire fence, as follows:

```
    
```

This function does not take a `self` argument, but takes a regular argument (named `arc`) instead. This means it can only be called as `Arc::get_mut(&mut a)`, and not as `a.get_mut()`. This is advisable for types that implement `Deref`, to avoid mixing it up with a similarly named method on the underlying `T`.

The returned mutable reference implicitly borrows the lifetime from the argument, meaning that nothing can use the original `Arc` as long as the returned `&mut T` is still around, allowing for safe mutation.

When the lifetime of the `&mut T` expires, the `Arc` can be used and shared with other threads again. One might wonder whether we need to worry about memory ordering for threads accessing the data afterwards. However, that’s the responsibility of whatever mechanism is used for sharing the `Arc` (or a new clone of it) with another thread. (For example, a mutex, a channel, or spawning a new thread.)

# Weak Pointers

Reference counting can be very useful when representing structures in memory consisting of multiple objects. For example, every node in a tree structure could contain an `Arc` to each of its child nodes. That way, when a node is dropped, its child nodes that are no longer in use are all (recursively) dropped as well.

It breaks down for _cyclic structures_, however. If a child node also contains an `Arc` to its parent node, neither will be dropped since there’s always at least one `Arc` that still refers to it.

The standard library’s `Arc` comes with a solution for that problem: `Weak<T>`. A `Weak<T>`, also called a _weak pointer_, behaves somewhat similar to an `Arc<T>`, but does not prevent an object from getting dropped. A `T` can be shared between several `Arc<T>` and `Weak<T>` objects, but when all `Arc<T>` objects are gone, the `T` is dropped, regardless of whether there are any `Weak<T>` objects left.

This means that a `Weak<T>` can exist without a `T`, and thus cannot provide a `&T` unconditionally, like a `Arc<T>` can. However, to access the `T` given a `Weak<T>`, it can be _upgraded_ to an `Arc<T>` through its `upgrade()` method. This method returns an `Option<Arc<T>>`, returning `None` if the `T` has already been dropped.

In an `Arc` based structure, `Weak` can be used to break cycles. For example, child nodes in a tree structure could use `Weak` rather than `Arc` for their parent node. Then, dropping of a parent node is not prevented through the existence of its child nodes.

Let’s implement this.

Just like before, when the number of `Arc` objects reaches zero, we can drop the contained `T` object. However, we can’t drop and deallocate the `ArcData` yet, since there might still be weak pointers referencing it. Only once the last `Weak` pointer is also gone can we drop and deallocate the `ArcData`.

So, we’ll use two counters: one for “the number of things that reference the `T`“, and another for “the number of things that reference the `ArcData<T>`“. In other words, the first counter is the same as before: it counts `Arc` objects, while the second counter counts both `Arc` and `Weak` objects.

We also need something that allows us to drop the contained object (`T`) while the `ArcData<T>` is still in use by the weak pointers. We’ll use an `Option<T>` so we can use `None` for when the data is dropped, and wrap that in an `UnsafeCell` for _interior mutability_ ([“Interior Mutability”](ch01.xhtml#interior-mutability)), to allow that to happen when the `ArcData<T>` isn’t exclusively owned.

```
struct
```

If we think of a `Weak<T>` as an object responsible for keeping an `ArcData<T>` alive, it can make sense to implement `Arc<T>` as a struct containing a `Weak<T>`, since an `Arc<T>` needs to do the same, and more.

```
pub
```

The `new` function is mostly the same as before, except it now has two counters to initialize at one:

```
impl
```

Just like before, we assume that the `ptr` field always points at a valid `ArcData<T>`. This time, we’ll encode that assumption as a private `data()` helper method on `Weak<T>`:

```
impl
```

In the `Deref` implementation for `Arc<T>`, we now have to use `UnsafeCell::get()` to get a pointer to the contents of the cell, and use unsafe code to promise it can safely be shared at this point. We also need `as_ref().unwrap()` to get a reference into the `Option<T>`. We don’t have to worry about this panicking, since the `Option` will only be `None` when there are no `Arc` objects left.

```
impl
```

The `Clone` implementation for `Weak<T>` is quite straight forward; it’s pretty much identical to our previous `Clone` implementation for `Arc<T>`.

```
impl
```

In the `Clone` implementation for our new `Arc<T>`, we need to increment both counters. We’ll simply use `self.weak.clone()` to re-use the code above for the first counter, so we only have to manually increment the second counter:

```
impl
```

Dropping a `Weak` should decrement its counter, and drop and deallocate the `ArcData` when the counter goes from one to zero. This is identical to what the `Drop` implementation of our previous `Arc` did.

```
impl
```

Dropping an `Arc` should decrement both counters. Note that one of these is already automatically taken care of, since every `Arc` contains a `Weak`, such that dropping an `Arc` will also result in dropping a `Weak`. We only have to take care of the other counter:

```
impl
```

###### Note

Dropping an object in Rust will first run its `Drop::drop` function (if it implements `Drop`), and then drop all of its fields, one by one, recursively.

The check in the `get_mut` method remains mostly unchanged, except it now needs to take weak pointers into account. It might seem like it could ignore weak pointers when checking for exclusivity, but a `Weak<T>` can be upgraded to an `Arc<T>` at any time. So, `get_mut` will have to check that there are no other `Arc<T>` nor `Weak<T>` before it can give out a `&mut T`:

```
impl
```

Next up: upgrading a weak pointer. Upgrading a `Weak` to an `Arc` is only possible when the data still exists. If there are only weak pointers left, there’s no data left that can be shared through an `Arc`. So, we’ll have to increase the `Arc` counter, but can only do so if it wasn’t already zero. We’ll use a compare-and-exchange loop ([“Compare-and-Exchange Operations”](ch02.xhtml#cas)) to do this.

Just like before, relaxed memory ordering is fine for incrementing a reference counter. There are no operations on other variables that need to strictly happen before or after this atomic operation.

```
impl
```

###### Tip

Note how this time we can check for `n < usize::MAX`, since that assertion would panic _before_ we modify `data_ref_count`.

The opposite, getting a `Weak<T>` from an `Arc<T>`, is much simpler:

```
impl
```

## Testing It

To quickly test our creation, we’ll modify our previous unit test to use weak pointers and verify they can be upgraded when expected:

```
#[test]
```

This also compiles and runs without problems, which leaves us with a very usable hand-made `Arc` implementation.

# Optimizing

While weak pointers can be useful, the `Arc` type is often used without any weak pointers at all. A downside of our last implementation is that cloning and dropping an `Arc` now both take two atomic operations each, as they have to increment or decrement both counters. This makes all `Arc` users “pay” for the cost of weak pointers, even when they are not using them.

It might seem like the solution is to count `Arc<T>` and `Weak<T>` pointers separately, but then we wouldn’t be able to atomically check that both counters are zero. To understand how that’s a problem, imagine we have a thread executing the following annoying function:

```
fn
```

This thread continuously downgrades and upgrades an `Arc`, such that it repeatedly cycles through moments where it holds no `Arc` ([![^1].

In our last implementation, we solved this by counting every `Arc` also as a `Weak`. A more subtle way of solving this, is to count all `Arc` pointers combined as one single `Weak` pointer. That way, the weak pointer counter (`alloc_ref_count`) never reaches zero as long as there are still any `Arc` objects around, just like in our last implementation, but cloning an `Arc` doesn’t need to touch that counter at all. Only dropping the very last `Arc` will need to decrement the weak pointer counter too.

Let’s try that.

This time, we can’t just implement `Arc<T>` as a wrapper around `Weak<T>`, so both will wrap a non-null pointer to the allocation:

```
pub
```

Since we’re optimizing our implementation, we might as well make `ArcData<T>` slightly smaller by using a `std::mem::ManuallyDrop<T>` instead of an `Option<T>`. We used an `Option<T>` to be able to replace a `Some(T)` by `None` when dropping the data, but we don’t actually need a separate `None` state to tell us the data is gone, since the existence or absence of `Arc<T>` already tells us that. A `ManuallyDrop<T>` takes the exact same amount of space as a `T`, but allows us to drop it at any point in time, manually, using an unsafe call to `ManuallyDrop::drop()`.

```
use
```

The `Arc::new` function remains almost unchanged, initializing both counters at one like before, but now using `ManuallyDrop::new()` instead of `Some()`:

```
impl
```

The `Deref` implementation can no longer make use of the private `data` method on the `Weak` type, so we’ll add the same private helper function on `Arc<T>`.

```
impl
```

The `Clone` and `Drop` implementations for `Weak<T>` are still the exact same as in our last implementation. Here they are, for completeness, including the private `Weak::data` helper function:

```
impl
```

And now we get to the part that this new optimized implementation was all about: cloning an `Arc<T>` now only needs to touch one counter:

```
impl
```

Similarly, dropping an `Arc<T>` now only needs to decrement one counter, except for the last drop that sees that counter go from one to zero. In that case, the weak pointer counter also needs to be decremented, such that it can reach zero once there are no weak pointers left. We do this by simply creating a `Weak<T>` out of thin air, and immediately dropping it.

```
impl
```

The `upgrade` method on `Weak<T>` remains mostly the same, except it no longer clones a weak pointer, since it doesn’t need to increment the weak pointer counter anymore. Upgrading only succeeds if there are already `Arc<T>` pointers to the allocation, which means that they’re already accounted for in the weak pointer counter.

```
impl
```

So far the differences to our previous implementation are very minimal. Where things get tricky, are with the last two methods we still need to implement: `downgrade` and `get_mut`.

Unlike before, the `get_mut` method now needs check if both counters are set to one to be able to determine whether there’s only one `Arc<T>` and no `Weak<T>` left, since a weak pointer counter of one can now represent multiple `Arc<T>` pointers. Reading the counters are two separate operations that happen at (slightly) different times, so we have to be very careful to not miss any concurrent downgrades, such as in the example case we saw at the start of [“Optimizing”](#optimizing-arc).

If we first check if `data_ref_count` is one, then we could miss a subsequent `upgrade()` before we check the other counter. But, if we first check if `alloc_ref_count` is one, then we could miss a subsequent `downgrade()` before we check the other counter.

A way out is to briefly block the `downgrade()` operation by “locking” the weak pointer counter. To do that, we don’t need anything like a mutex. We can use a special value, like `usize::MAX`, to represent a special “locked” state of the weak pointer counter. It’ll only be locked very very briefly, only to load the other counter, so the `downgrade` method could just spin until it’s unlocked, in the unlikely situation it runs at the exact same moment as `get_mut`.

So, `get_mut` will first have to check if `alloc_ref_count` is one and at the same time replace it by `usize::MAX`, if it was indeed one. That’s a job for `compare_exchange`.

Then it’ll have to check if the other counter is also one, after which it can immediately unlock the weak pointer counter. If the second counter was also one, we know we have exclusive access to the allocation and the data, such that we can return an `&mut T`.

```
    
```

As you might have expected, the locking operation (the `compare_exchange`) will have to use `Acquire` memory ordering, and the unlocking operation (the `store`) will have to use `Release` memory ordering.

If we had used `Relaxed` for the `compare_exchange` instead, it’d have been possible for the subsequent `load` from `data_ref_count` to not yet see the new value of a freshly upgraded `Weak` pointer, even though the `compare_exchange` already confirmed that every `Weak` pointer has been dropped.

If we had used `Relaxed` for the `store`, it’d be possible for the preceding `load` to observe the result of a future `Arc::drop` for an `Arc` that can still be downgraded.

The acquire-fence is the same as before: it synchronizes with the release-decrement operation in `Arc::Drop` to make sure every access through former `Arc` clones happened before the new exclusive access.

The last piece to the puzzle is the `downgrade` method, which will have to check for the special `usize::MAX` value to see if the weak pointer counter is locked, and spin until it is unlocked. Just like in the `upgrade` implementation, we’ll use a compare-and-exchange loop to check for the special value and overflow before incrementing the counter:

```
    
```

We use acquire memory ordering for `compare_exchange_weak`, which synchronizes with the release-store in the `get_mut` function. Otherwise, it’d be possible for the effect of a subsequent `Arc::drop` to be visible to a thread running `get_mut` before it unlocked the counter.

In other words, the acquire compare-and-exchange operation here effectively “locks” up `get_mut`, preventing it from succeeding. It can be “unlocked” again by a later `Weak::drop` that decrements the counter back to one, using release memory ordering.

###### Note

The optimized implementation of `Arc<T>` and `Weak<T>` that we just made is nearly identical to the one included in the Rust standard library.

If we run the exact same test as before ([“Testing It”](#arc-weak-test)), we see that this optimized implementation also compiles fine and passes our tests.

###### Tip

If you feel that getting the memory ordering decisions right for this optimized implementation was tricky, don’t worry. Many concurrent data structures are much simpler to implement correctly than this one. This `Arc` implementation is included in this chapter specifically because of its tricky subtleties around memory ordering.

# Summary

- `Arc<T>` provides shared ownership of a reference counted allocation.
    
- By checking if the reference counter is exactly one, an `Arc<T>` can conditionally provide exclusive access (`&mut T`).
    
- Incrementing the atomic reference counter can be done using a relaxed operation, but the final decrement must synchronize with all previous decrements.
    
- A _weak pointer_ (`Weak<T>`) can be used to avoid cycles.
    
- The `NonNull<T>` type represents a pointer to `T` that is never null.
    
- The `ManuallyDrop<T>` type can be used to manually decide, using `unsafe` code, when to drop a `T`.
    
- As soon as more than one atomic variable is involved, things get more complicated.
    
- Implementing an ad-hoc (spin) lock can sometimes be a valid strategy for operating on multiple atomic variables at once.