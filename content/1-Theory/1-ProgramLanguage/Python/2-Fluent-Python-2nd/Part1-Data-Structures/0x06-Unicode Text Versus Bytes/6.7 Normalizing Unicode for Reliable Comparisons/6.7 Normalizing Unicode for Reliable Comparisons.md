# Normalizing Unicode for Reliable Comparisons

String comparisons are complicated by the fact that Unicode has combining characters: diacritics and other marks that attach to the preceding character, appearing as one when printed.

For example, the word “café” may be composed in two ways, using four or five code points, but the result looks exactly the same:

```
>>> 
```

Placing `COMBINING ACUTE ACCENT` (U+0301) after “e” renders “é”. In the Unicode standard, sequences like `'é'` and `'e\u0301'` are called “canonical equivalents,” and applications are supposed to treat them as the same. But Python sees two different sequences of code points, and considers them not equal.

The solution is `unicodedata.normalize()`. The first argument to that function is one of four strings: `'NFC'`, `'NFD'`, `'NFKC'`, and `'NFKD'`. Let’s start with the first two.

Normalization Form C (NFC) composes the code points to produce the shortest equivalent string, while NFD decomposes, expanding composed characters into base characters and separate combining characters. Both of these normalizations make comparisons work as expected, as the next example shows:

```
>>> 
```

Keyboard drivers usually generate composed characters, so text typed by users will be in NFC by default. However, to be safe, it may be good to normalize strings with `normalize('NFC', user_text)` before saving. NFC is also the normalization form recommended by the W3C in [“Character Model for the World Wide Web: String Matching and Searching”](https://fpy.li/4-15).

Some single characters are normalized by NFC into another single character. The symbol for the ohm (Ω) unit of electrical resistance is normalized to the Greek uppercase omega. They are visually identical, but they compare as unequal, so it is essential to normalize to avoid surprises:

```
>>> 
```

The other two normalization forms are NFKC and NFKD, where the letter K stands for “compatibility.” These are stronger forms of normalization, affecting the so-called “compatibility characters.” Although one goal of Unicode is to have a single “canonical” code point for each character, some characters appear more than once for compatibility with preexisting standards. For example, the `MICRO SIGN`, `µ` (`U+00B5`), was added to Unicode to support round-trip conversion to `latin1`, which includes it, even though the same character is part of the Greek alphabet with code point `U+03BC` (`GREEK SMALL LETTER MU`). So, the micro sign is considered a “compatibility character.”

In the NFKC and NFKD forms, each compatibility character is replaced by a “compatibility decomposition” of one or more characters that are considered a “preferred” representation, even if there is some formatting loss—ideally, the formatting should be the responsibility of external markup, not part of Unicode. To exemplify, the compatibility decomposition of the one-half fraction `'½'` (`U+00BD`) is the sequence of three characters `'1/2'`, and the compatibility decomposition of the micro sign `'µ'` (`U+00B5`) is the lowercase mu `'μ'` (`U+03BC`).[^7]

Here is how the NFKC works in practice:

```
>>> 
```

Although `'1⁄2'` is a reasonable substitute for `'½'`, and the micro sign is really a lowercase Greek mu, converting `'4²'` to `'42'` changes the meaning. An application could store `'4²'` as `'4<sup>2</sup>'`, but the `normalize` function knows nothing about formatting. Therefore, NFKC or NFKD may lose or distort information, but they can produce convenient intermediate representations for searching and indexing.

Unfortunately, with Unicode everything is always more complicated than it first seems. For the `VULGAR FRACTION ONE HALF`, the NFKC normalization produced 1 and 2 joined by `FRACTION SLASH`, instead of `SOLIDUS`, a.k.a. “slash”—the familiar character with ASCII code decimal 47. Therefore, searching for the three-character ASCII sequence `'1/2'` would not find the normalized Unicode sequence.

###### Warning

NFKC and NFKD normalization cause data loss and should be applied only in special cases like search and indexing, and not for permanent storage of text.

When preparing text for searching or indexing, another operation is useful: case folding, our next subject.