# Further Reading

Ned Batchelder’s 2012 PyCon US talk [“Pragmatic Unicode, or, How Do I Stop the Pain?”](https://fpy.li/4-28) was outstanding. Ned is so professional that he provides a full transcript of the talk along with the slides and video.

“Character encoding and Unicode in Python: How to (╯°□°)╯︵ ┻━┻ with dignity” ([slides](https://fpy.li/4-1), [video](https://fpy.li/4-2)) was the excellent PyCon 2014 talk by Esther Nam and Travis Fischer, where I found this chapter’s pithy epigraph: “Humans use text. Computers speak bytes.”

Lennart Regebro—one of the technical reviewers for the first edition of this book—shares his “Useful Mental Model of Unicode (UMMU)” in the short post [“Unconfusing Unicode: What Is Unicode?”](https://fpy.li/4-31). Unicode is a complex standard, so Lennart’s UMMU is a really useful starting point.

The official [“Unicode HOWTO”](https://fpy.li/4-32) in the Python docs approaches the subject from several different angles, from a good historic intro, to syntax details, codecs, regular expressions, filenames, and best practices for Unicode-aware I/O (i.e., the Unicode sandwich), with plenty of additional reference links from each section. [Chapter 4, “Strings”](https://fpy.li/4-33), of Mark Pilgrim’s awesome book [_Dive into Python 3_](https://fpy.li/4-34) (Apress) also provides a very good intro to Unicode support in Python 3. In the same book, [Chapter 15](https://fpy.li/4-35) describes how the Chardet library was ported from Python 2 to Python 3, a valuable case study given that the switch from the old `str` to the new `bytes` is the cause of most migration pains, and that is a central concern in a library designed to detect encodings.

If you know Python 2 but are new to Python 3, Guido van Rossum’s [“What’s New in Python 3.0”](https://fpy.li/4-36) has 15 bullet points that summarize what changed, with lots of links. Guido starts with the blunt statement: “Everything you thought you knew about binary data and Unicode has changed.” Armin Ronacher’s blog post [“The Updated Guide to Unicode on Python”](https://fpy.li/4-37) is deep and highlights some of the pitfalls of Unicode in Python 3 (Armin is not a big fan of Python 3).

Chapter 2, “Strings and Text,” of the [_Python Cookbook_, 3rd ed.](https://fpy.li/pycook3) (O’Reilly), by David Beazley and Brian K. Jones, has several recipes dealing with Unicode normalization, sanitizing text, and performing text-oriented operations on byte sequences. Chapter 5 covers files and I/O, and it includes “Recipe 5.17. Writing Bytes to a Text File,” showing that underlying any text file there is always a binary stream that may be accessed directly when needed. Later in the cookbook, the `struct` module is put to use in “Recipe 6.11. Reading and Writing Binary Arrays of Structures.”

Nick Coghlan’s “Python Notes” blog has two posts very relevant to this chapter: [“Python 3 and ASCII Compatible Binary Protocols”](https://fpy.li/4-38) and [“Processing Text Files in Python 3”](https://fpy.li/4-39). Highly recommended.

A list of encodings supported by Python is available at [“Standard Encodings”](https://fpy.li/4-40) in the `codecs` module documentation. If you need to get that list programmatically, see how it’s done in the [_/Tools/unicode/listcodecs.py_](https://fpy.li/4-41) script that comes with the CPython source code.

The books _[Unicode Explained](https://fpy.li/4-42)_ by Jukka K. Korpela (O’Reilly) and [_Unicode Demystified_](https://fpy.li/4-43) by Richard Gillam (Addison-Wesley) are not Python-specific but were very helpful as I studied Unicode concepts. [_Programming with Unicode_](https://fpy.li/4-44) by Victor Stinner is a free, self-published book (Creative Commons BY-SA) covering Unicode in general, as well as tools and APIs in the context of the main operating systems and a few programming languages, including Python.

The W3C pages [“Case Folding: An Introduction”](https://fpy.li/4-45) and [“Character Model for the World Wide Web: String Matching”](https://fpy.li/4-15) cover normalization concepts, with the former being a gentle introduction and the latter a working group note written in dry standard-speak—the same tone of the [“Unicode Standard Annex #15—Unicode Normalization Forms”](https://fpy.li/4-47). The [“Frequently Asked Questions, Normalization”](https://fpy.li/4-48) section from [_Unicode.org_](https://fpy.li/4-49) is more readable, as is the [“NFC FAQ”](https://fpy.li/4-50) by Mark Davis—author of several Unicode algorithms and president of the Unicode Consortium at the time of this writing.

In 2016, the Museum of Modern Art (MoMA) in New York added to its collection [the original emoji](https://fpy.li/4-51), the 176 emojis designed by Shigetaka Kurita in 1999 for NTT DOCOMO—the Japanese mobile carrier. Going further back in history, [_Emojipedia_](https://fpy.li/4-52) published [“Correcting the Record on the First Emoji Set”](https://fpy.li/4-53), crediting Japan’s SoftBank for the earliest known emoji set, deployed in cell phones in 1997. SoftBank’s set is the source of 90 emojis now in Unicode, including U+1F4A9 (`PILE OF POO`). Matthew Rothenberg’s [_emojitracker.com_](https://fpy.li/4-54) is a live dashboard showing counts of emoji usage on Twitter, updated in real time. As I write this, `FACE WITH TEARS OF JOY` (U+1F602) is the most popular emoji on Twitter, with more than 3,313,667,315 recorded occurrences.

##### Soapbox

Non-ASCII Names in Source Code: Should You Use Them?

Python 3 allows non-ASCII identifiers in source code:

```
>>>
```

Some people dislike the idea. The most common argument to stick with ASCII identifiers is to make it easy for everyone to read and edit code. That argument misses the point: you want your source code to be readable and editable by its intended audience, and that may not be “everyone.” If the code belongs to a multinational corporation or is open source and you want contributors from around the world, the identifiers should be in English, and then all you need is ASCII.

But if you are a teacher in Brazil, your students will find it easier to read code that uses Portuguese variable and function names, correctly spelled. And they will have no difficulty typing the cedillas and accented vowels on their localized keyboards.

Now that Python can parse Unicode names and UTF-8 is the default source encoding, I see no point in coding identifiers in Portuguese without accents, as we used to do in Python 2 out of necessity—unless you need the code to run on Python 2 also. If the names are in Portuguese, leaving out the accents won’t make the code more readable to anyone.

This is my point of view as a Portuguese-speaking Brazilian, but I believe it applies across borders and cultures: choose the human language that makes the code easier to read by the team, then use the characters needed for correct spelling.

What Is “Plain Text”?

For anyone who deals with non-English text on a daily basis, “plain text” does not imply “ASCII.” The [Unicode Glossary](https://fpy.li/4-55) defines _plain text_ like this:

> Computer-encoded text that consists only of a sequence of code points from a given standard, with no other formatting or structural information.

That definition starts very well, but I don’t agree with the part after the comma. HTML is a great example of a plain-text format that carries formatting and structural information. But it’s still plain text because every byte in such a file is there to represent a text character, usually using UTF-8. There are no bytes with nontext meaning, as you can find in a _.png_ or _.xls_ document where most bytes represent packed binary values like RGB values and floating-point numbers. In plain text, numbers are represented as sequences of digit characters.

I am writing this book in a plain-text format called—ironically—[AsciiDoc](https://fpy.li/4-56), which is part of the toolchain of O’Reilly’s excellent [Atlas book publishing platform](https://fpy.li/4-57). AsciiDoc source files are plain text, but they are UTF-8, not ASCII. Otherwise, writing this chapter would have been really painful. Despite the name, AsciiDoc is just great.

The world of Unicode is constantly expanding and, at the edges, tool support is not always there. Not all characters I wanted to show were available in the fonts used to render the book. That’s why I had to use 0-Assets/Epubook/Fluent%20Python%20Clear,%20Concise,%20and%20Effective%20Programming/images instead of listings in several examples in this chapter. On the other hand, the Ubuntu and macOS terminals display most Unicode text very well—including the Japanese characters for the word “mojibake”: 文字化け.

How Are str Code Points Represented in RAM?

The official Python docs avoid the issue of how the code points of a `str` are stored in memory. It is really an implementation detail. In theory, it doesn’t matter: whatever the internal representation, every `str` must be encoded to `bytes` on output.

In memory, Python 3 stores each `str` as a sequence of code points using a fixed number of bytes per code point, to allow efficient direct access to any character or slice.

Since Python 3.3, when creating a new `str` object, the interpreter checks the characters in it and chooses the most economic memory layout that is suitable for that particular `str`: if there are only characters in the `latin1` range, that `str` will use just one byte per code point. Otherwise, two or four bytes per code point may be used, depending on the `str`. This is a simplification; for the full details, look up [PEP 393—Flexible String Representation](https://fpy.li/pep393).

The flexible string representation is similar to the way the `int` type works in Python 3: if the integer fits in a machine word, it is stored in one machine word. Otherwise, the interpreter switches to a variable-length representation like that of the Python 2 `long` type. It is nice to see the spread of good ideas.

However, we can always count on Armin Ronacher to find problems in Python 3. He explained to me why that was not such as great idea in practice: it takes a single `RAT` (U+1F400) to inflate an otherwise all-ASCII text into a memory-hogging array using four bytes per character, when one byte would suffice for each character except the `RAT`. In addition, because of all the ways Unicode characters combine, the ability to quickly retrieve an arbitrary character by position is overrated—and extracting arbitrary slices from Unicode text is naïve at best, and often wrong, producing mojibake. As emojis become more popular, these problems will only get worse.

[^1]: .

[^2]:  Python 2.6 and 2.7 also had `bytes`, but it was just an alias to the `str` type.

[^3]:  Trivia: the ASCII “single quote” character that Python uses by default as the string delimiter is actually named APOSTROPHE in the Unicode standard. The real single quotes are asymmetric: left is U+2018 and right is U+2019.

[^4]: .

[^5]:  at US PyCon 2012.

[^6]: .

[^7]:  Curiously, the micro sign is considered a “compatibility character,” but the ohm symbol is not. The end result is that NFC doesn’t touch the micro sign but changes the ohm symbol to capital omega, while NFKC and NFKD change both the ohm and the micro into Greek characters.

[^8]:  Diacritics affect sorting only in the rare case when they are the only difference between two words—in that case, the word with a diacritic is sorted after the plain word.

[^9]:  Again, I could not find a solution, but did find other people reporting the same problem. Alex Martelli, one of the tech reviewers, had no problem using `setlocale` and `locale.strxfrm` on his Macintosh with macOS 10.9. In summary: your mileage may vary.

[^10]:  That’s an image—not a code listing—because emojis are not well supported by O’Reilly’s digital publishing toolchain as I write this.

[^11]:  Although it was not better than `re` at identifying digits in this particular sample.