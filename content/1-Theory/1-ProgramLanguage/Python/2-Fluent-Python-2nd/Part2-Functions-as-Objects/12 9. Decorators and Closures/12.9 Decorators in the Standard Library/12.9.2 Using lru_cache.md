## Using lru_cache

The `functools.cache` decorator is actually a simple wrapper around the older `functools.lru_cache` function, which is more flexible and compatible with Python 3.8 and earlier versions.

The main advantage of `@lru_cache` is that its memory usage is bounded by the `maxsize` parameter, which has a rather conservative default value of 128—which means the cache will hold at most 128 entries at any time.

The acronym LRU stands for Least Recently Used, meaning that older entries that have not been read for a while are discarded to make room for new ones.

Since Python 3.8, `lru_cache` can be applied in two ways. This is how to use it in the simplest way:

```
@lru_cache
```

The other way—available since Python 3.2—is to invoke it as a function, with `()`:

```
@lru_cache
```

In both cases, the default parameters would be used. These are:

`maxsize=128`

Sets the maximum number of entries to be stored. After the cache is full, the least recently used entry is discarded to make room for each new entry. For optimal performance, `maxsize` should be a power of 2. If you pass `maxsize=None`, the LRU logic is disabled, so the cache works faster but entries are never discarded, which may consume too much memory. That’s what `@functools.cache` does.

`typed=False`

Determines whether the results of different argument types are stored separately. For example, in the default setting, float and integer arguments that are considered equal are stored only once, so there would be a single entry for the calls `f(1)` and `f(1.0)`. If `typed=True`, those arguments would produce different entries, possibly storing distinct results.

Here is an example invoking `@lru_cache` with nondefault parameters:

```
@lru_cache
```

Now let’s study another powerful decorator: `functools.singledispatch`.