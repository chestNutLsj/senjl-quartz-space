# Further Reading

A detailed technical explanation of generators appears in _The Python Language Reference_ in [“6.2.9. Yield expressions”](https://fpy.li/17-27). The PEP where generator functions were defined is [PEP 255—Simple Generators](https://fpy.li/pep255).

The [`itertools` module documentation](https://fpy.li/17-28) is excellent because of all the examples included. Although the functions in that module are implemented in C, the documentation shows how some of them would be written in Python, often by leveraging other functions in the module. The usage examples are also great; for instance, there is a snippet showing how to use the `accumulate` function to amortize a loan with interest, given a list of payments over time. There is also an [“Itertools Recipes”](https://fpy.li/17-29) section with additional high-performance functions that use the `itertools` functions as building blocks.

Beyond Python’s standard library, I recommend the [More Itertools](https://fpy.li/17-30) package, which follows the fine `itertools` tradition in providing powerful generators with plenty of examples and some useful recipes.

Chapter 4, “Iterators and Generators,” of _Python Cookbook_, 3rd ed., by David Beazley and Brian K. Jones (O’Reilly), has 16 recipes covering this subject from many different angles, focusing on practical applications. It includes some illuminating recipes with `yield from`.

Sebastian Rittau—currently a top contributor of _typeshed_—explains why iterators should be iterable, as he noted in 2006 that, [“Java: Iterators are not Iterable”](https://fpy.li/17-31).

The `yield from` syntax is explained with examples in the “What’s New in Python 3.3” section of [PEP 380—Syntax for Delegating to a Subgenerator](https://fpy.li/17-32). My post [“Classic Coroutines”](https://fpy.li/oldcoro) at [_fluentpython.com_](http://fluentpython.com) explains `yield from` in depth, including Python pseudocode of its implementation in C.

David Beazley is the ultimate authority on Python generators and coroutines. The _[Python Cookbook](https://fpy.li/pycook3)_, 3rd ed., (O’Reilly) he coauthored with Brian Jones has numerous recipes with coroutines. Beazley’s PyCon tutorials on the subject are famous for their depth and breadth. The first was at PyCon US 2008: [“Generator Tricks for Systems Programmers”](https://fpy.li/17-33). PyCon US 2009 saw the legendary [“A Curious Course on Coroutines and Concurrency”](https://fpy.li/17-34) (hard-to-find video links for all three parts: [part 1](https://fpy.li/17-35), [part 2](https://fpy.li/17-36), and [part 3](https://fpy.li/17-37)). His tutorial from PyCon 2014 in Montréal was [“Generators: The Final Frontier”](https://fpy.li/17-38), in which he tackles more concurrency examples—so it’s really more about topics in [Chapter 21](ch21.html#async_ch). Dave can’t resist making brains explode in his classes, so in the last part of “The Final Frontier,” coroutines replace the classic Visitor pattern in an arithmetic expression evaluator.

Coroutines allow new ways of organizing code, and just as recursion or polymorphism (dynamic dispatch), it takes some time getting used to their possibilities. An interesting example of classic algorithm rewritten with coroutines is in the post [“Greedy algorithm with coroutines”](https://fpy.li/17-39), by James Powell.

Brett Slatkin’s [_Effective Python_, 1st ed.](https://fpy.li/17-40) (Addison-Wesley) has an excellent short chapter titled “Consider Coroutines to Run Many Functions Concurrently.” That chapter is not in the second edition of _Effective Python_, but it is still [available online as a sample chapter](https://fpy.li/17-41). Slatkin presents the best example of driving coroutines with `yield from` that I’ve seen: an implementation of John Conway’s [Game of Life](https://fpy.li/17-42) in which coroutines manage the state of each cell as the game runs. I refactored the code for the Game of Life example—separating the functions and classes that implement the game from the testing snippets used in Slatkin’s original code. I also rewrote the tests as doctests, so you can see the output of the various coroutines and classes without running the script. The [refactored example](https://fpy.li/17-43) is posted as a [GitHub gist](https://fpy.li/17-44).

##### Soapbox

The Minimalistic Iterator Interface in Python

In the “Implementation” section of the Iterator pattern,[^18] the Gang of Four wrote:

> The minimal interface to Iterator consists of the operations First, Next, IsDone, and CurrentItem.

However, that very sentence has a footnote that reads:

> We can make this interface even smaller by merging Next, IsDone, and CurrentItem into a single operation that advances to the next object and returns it. If the traversal is finished, then this operation returns a special value (0, for instance) that marks the end of the iteration.

This is close to what we have in Python: the single method `__next__` does the job. But instead of using a sentinel, which could be overlooked by mistake, the `StopIteration` exception signals the end of the iteration. Simple and correct: that’s the Python way.

Pluggable Generators

Anyone who manages large datasets finds many uses for generators. This is the story of the first time I built a practical solution around generators.

Years ago I worked at BIREME, a digital library run by PAHO/WHO (Pan-American Health Organization/World Health Organization) in São Paulo, Brazil. Among the bibliographic datasets created by BIREME are LILACS (Latin American and Caribbean Health Sciences index) and SciELO (Scientific Electronic Library Online), two comprehensive databases indexing the research literature about health sciences produced in the region.

Since the late 1980s, the database system used to manage LILACS is CDS/ISIS, a non-relational document database created by UNESCO. One of my jobs was to research alternatives for a possible migration of LILACS—and eventually the much larger SciELO—to a modern, open source, document database such as CouchDB or MongoDB. At the time, I wrote a paper explaining the semistructured data model and different ways to represent CDS/ISIS data with JSON-like records: [“From ISIS to CouchDB: Databases and Data Models for Bibliographic Records”](https://fpy.li/17-45).

As part of that research, I wrote a Python script to read a CDS/ISIS file and write a JSON file suitable for importing to CouchDB or MongoDB. Initially, the script read files in the ISO-2709 format exported by CDS/ISIS. The reading and writing had to be done incrementally because the full datasets were much bigger than main memory. That was easy enough: each iteration of the main `for` loop read one record from the _.iso_ file, massaged it, and wrote it to the _.json_ output.

However, for operational reasons, it was deemed necessary that _isis2json.py_ supported another CDS/ISIS data format: the binary _.mst_ files used in production at BIREME—to avoid the costly export to ISO-2709. Now I had a problem: the libraries used to read ISO-2709 and _.mst_ files had very different APIs. And the JSON writing loop was already complicated because the script accepted a variety of command-line options to restructure each output record. Reading data using two different APIs in the same `for` loop where the JSON was produced would be unwieldy.

The solution was to isolate the reading logic into a pair of generator functions: one for each supported input format. In the end, I split the _isis2json.py_ script into four functions. You can see the Python 2 source code with dependencies in the [_fluentpython/isis2json_](https://fpy.li/17-46) repository on GitHub.[^19]

Here is a high-level overview of how the script is structured:

`main`

The `main` function uses `argparse` to read command-line options that configure the structure of the output records. Based on the input filename extension, a suitable generator function is selected to read the data and yield the records, one by one.

`iter_iso_records`

This generator function reads _.iso_ files (assumed to be in the ISO-2709 format). It takes two arguments: the filename and `isis_json_type`, one of the options related to the record structure. Each iteration of its `for` loop reads one record, creates an empty `dict`, populates it with field data, and yields the `dict`.

`iter_mst_records`

This other generator functions reads _.mst_ files.[^20] If you look at the source code for _isis2json.py_, you’ll see that it’s not as simple as `iter_iso_records`, but its interface and overall structure is the same: it takes a filename and an `isis_json_type` argument and enters a `for` loop, which builds and yields one `dict` per iteration, representing a single record.

`write_json`

This function performs the actual writing of the JSON records, one at a time. It takes numerous arguments, but the first one—`input_gen`—is a reference to a generator function: either `iter_iso_records` or `iter_mst_records`. The main `for` loop in `write_json` iterates over the dictionaries yielded by the selected `input_gen` generator, restructures it in different ways as determined by the command-line options, and appends the JSON record to the output file.

By leveraging generator functions, I was able to decouple the reading from the writing. Of course, the simplest way to decouple them would be to read all records to memory, then write them to disk. But that was not a viable option because of the size of the datasets. Using generators, the reading and writing is interleaved, so the script can process files of any size. Also, the special logic for reading a record in the different input formats is separated from the logic of restructuring each record for writing.

Now, if we need _isis2json.py_ to support an additional input format—say, MARCXML, a DTD used by the US Library of Congress to represent ISO-2709 data—it will be easy to add a third generator function to implement the reading logic, without changing anything in the complicated `write_json` function.

This is not rocket science, but it’s a real example where generators enabled an efficient and flexible solution to process databases as a stream of records, keeping memory usage low regardless of the size of the dataset.

[^1]: , a blog post.

[^2]: .

[^3]:  Thanks to tech reviewer Leonardo Rochael for this fine example.

[^4]: ` would also be an iterator, as it should be. However, I used a `for` loop with `yield` here to introduce the syntax of a generator function, which requires the `yield` keyword, as we’ll see in the next section. During review of the second edition of this book, Leonardo Rochael suggested yet another shortcut for the body of `__iter__`: `yield from self.words`. We’ll also cover `yield from` later in this chapter.

[^5]:  Sometimes I add a `gen` prefix or suffix when naming generator functions, but this is not a common practice. And you can’t do that if you’re implementing an iterable, of course: the necessary special method must be named `__iter__`.

[^6]:  Thanks to David Kwast for suggesting this example.

[^7]: .

[^8]:  includes doctests and a script, _aritprog_runner.py_, which runs the tests against all variations of the _aritprog*.py_ scripts.

[^9]:  Here, the term “mapping” is unrelated to dictionaries, but has to do with the `map` built-in.

[^10]:  `chain` and most `itertools` functions are written in C.

[^11]:  As of version 0.910, Mypy still uses the deprecated `typing` types.

[^12]: .

[^13]: .

[^14]:  In fact, it never returns unless some exception breaks the loop. Mypy 0.910 accepts both `None` and `typing​.NoReturn` as the generator return type parameter—but it also accepts `str` in that position, so apparently it can’t fully analyze the coroutine code at this time.

[^15]:  I considered renaming the field, but `count` is the best name for the local variable in the coroutine, and is the name I used for this variable in similar examples in the book, so it makes sense to use the same name in the `Result` field. I don’t hesitate to use `# type: ignore` to avoid the limitations and annoyances of static type checkers when submission to the tool would make the code worse or needlessly complicated.

[^16]:  Since Python 3.7, `typing.Generator` and other types that correspond to ABCs in `collections.abc` were refactored with a wrapper around the corresponding ABC, so their generic parameters aren’t visible in the _typing.py_ source file. That’s why I refer to Python 3.6 source code here.

[^17]: , to _grok_ is not merely to learn something, but to absorb it so “it becomes part of you, part of your identity.”

[^18]:  Gamma et. al., _Design Patterns: Elements of Reusable Object-Oriented Software_, p. 261.

[^19]:  The code is in Python 2 because one of its optional dependencies is a Java library named _Bruma_, which we can import when we run the script with Jython—which does not yet support Python 3.

[^20]:  file in the repository. The dependencies are imported inside the generator functions that need them, so the script can run even if only one of the external libraries is available.