## Code for the Multicore Prime Checker

When we delegate computing to threads or processes, our code does not call the worker function directly, so we can’t simply get a return value. Instead, the worker is driven by the thread or process library, and it eventually produces a result that needs to be stored somewhere. Coordinating workers and collecting results are common uses of queues in concurrent programming—and also in distributed systems.

Much of the new code in _procs.py_ has to do with setting up and using queues. The top of the file is in [Example 19-13](#primes_procs_top_ex).

###### Warning

`SimpleQueue` was added to `multiprocessing` in Python 3.9. If you’re using an earlier version of Python, you can replace `SimpleQueue` with `Queue` in [Example 19-13](#primes_procs_top_ex).

##### Example 19-13. procs.py: multiprocess primality check; imports, types, and functions

```
import
```

[![^1]

Trying to emulate `threading`, `multiprocessing` provides `multiprocessing.SimpleQueue`, but this is a method bound to a predefined instance of a lower-level `BaseContext` class. We must call this `SimpleQueue` to build a queue, we can’t use it in type hints.

[![^2]

`multiprocessing.queues` has the `SimpleQueue` class we need for type hints.

[![^3]

`PrimeResult` includes the number checked for primality. Keeping `n` together with the other result fields simplifies displaying results later.

[![^4]

This is a type alias for a `SimpleQueue` that the `main` function ([Example 19-14](#primes_procs_main_ex)) will use to send numbers to the processes that will do the work.

[![^5]

Type alias for a second `SimpleQueue` that will collect the results in `main`. The values in the queue will be tuples made of the number to be tested for primality, and a `Result` tuple.

[![^6]

This is similar to _sequential.py_.

[![^7]

`worker` gets a queue with the numbers to be checked, and another to put results.

[![^8]

In this code, I use the number `0` as a _poison pill_: a signal for the worker to finish. If `n` is not `0`, proceed with the loop.[^14]

[![^9]

Invoke the primality check and enqueue `PrimeResult`.

[![^10]

Send back a `PrimeResult(0, False, 0.0)` to let the main loop know that this worker is done.

[![^11]

`procs` is the number of processes that will compute the prime checks in parallel.

[![^12]

Enqueue the numbers to be checked in `jobs`.

[![^13]

Fork a child process for each worker. Each child will run the loop inside its own instance of the `worker` function, until it fetches a `0` from the `jobs` queue.

[![^14]

Start each child process.

[![^15]

Enqueue one `0` for each process, to terminate them.

##### Loops, Sentinels, and Poison Pills

The `worker` function in [Example 19-13](#primes_procs_top_ex) follows a common pattern in concurrent programming: looping indefinitely while taking items from a queue and processing each with a function that does the actual work. The loop ends when the queue produces a sentinel value. In this pattern, the sentinel that shuts down the worker is often called a “poison pill.”

`None` is often used as a sentinel value, but it may be unsuitable if it can occur in the data stream. Calling `object()` is a common way to get a unique value to use as sentinel. However, that does not work across processes because Python objects must be serialized for inter-process communication, and when you `pickle.dump` and `pickle.load` an instance of `object`, the unpickled instance is distinct from the original: it doesn’t compare equal. A good alternative to `None` is the `Ellipsis` built-in object (a.k.a. `...`), which survives serialization without losing its identity.[^15]

Python’s standard library uses [lots of different values](https://fpy.li/19-22) as sentinels. [PEP 661—Sentinel Values](https://fpy.li/pep661) proposes a standard sentinel type. As of September 2021, it’s only a draft.

Now let’s study the `main` function of _procs.py_ in [Example 19-14](#primes_procs_main_ex).

##### Example 19-14. procs.py: multiprocess primality check; `main` function

```
def
```

[![^1]

If no command-line argument is given, set the number of processes to the number of CPU cores; otherwise, create as many processes as given in the first argument.

[![^2]

`jobs` and `results` are the queues described in [Example 19-13](#primes_procs_top_ex).

[![^3]

Start `proc` processes to consume `jobs` and post `results`.

[![^4]

Retrieve the results and display them; `report` is defined in ![^6].

[![^5]

Display how many numbers were checked and the total elapsed time.

[![^6]

The arguments are the number of `procs` and the queue to post the results.

[![^7]

Loop until all processes are done.

[![^8]

Get one `PrimeResult`. Calling `.get()` on a queue block until there is an item in the queue. It’s also possible to make this nonblocking, or set a timeout. See the [`SimpleQueue.get`](https://fpy.li/19-23) documentation for details.

[![^9]

If `n` is zero, then one process exited; increment the `procs_done` count.

[![^10]

Otherwise, increment the `checked` count (to keep track of the numbers checked) and display the results.

The results will not come back in the same order the jobs were submitted. That’s why I had to put `n` in each `PrimeResult` tuple. Otherwise, I’d have no way to know which result belonged to each number.

If the main process exits before all subprocesses are done, you may see confusing tracebacks on `FileNotFoundError` exceptions caused by an internal lock in `multiprocessing`. Debugging concurrent code is always hard, and debugging `multiprocessing` is even harder because of all the complexity behind the thread-like façade. Fortunately, the `ProcessPoolExecutor` we’ll meet in [Chapter 20](ch20.html#futures_ch) is easier to use and more robust.

###### Note

Thanks to reader Michael Albert who noticed the code I published during the early release had a [_race condition_](https://fpy.li/19-24) in [Example 19-14](#primes_procs_main_ex). A race condition is a bug that may or may not occur depending on the order of actions performed by concurrent execution units. If “A” happens before “B,” all is fine; but it “B” happens first, something goes wrong. That’s the race.

If you are curious, this diff shows the bug and how I fixed it: [_example-code-2e/commit/2c123057_](https://fpy.li/19-25)—but note that I later refactored the example to delegate parts of `main` to the `start_jobs` and `report` functions. There’s a [_README.md_](https://fpy.li/19-26) file in the same directory explaining the problem and the solution.