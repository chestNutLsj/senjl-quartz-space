## Experimenting with More or Fewer Processes

You may want try running _procs.py_, passing arguments to set the number of worker processes. For example, this command…

$ python3 procs.py `2`

…will launch two worker processes, producing results almost twice as fast as _sequential.py_—if your machine has at least two cores and is not too busy running other programs.

I ran _procs.py_ 12 times with 1 to 20 processes, totaling 240 runs. Then I computed the median time for all runs with the same number of processes, and plotted [Figure 19-2](#procs_x_time_fig).

![Median run times for each number of processes from 1 to 20.](assets/flpy_1902.png)

###### Figure 19-2. Median run times for each number of processes from 1 to 20. Highest median time was 40.81s, with 1 process. Lowest median time was 10.39s, with 6 processes, indicated by the dotted line.

In this 6-core laptop, the lowest median time was with 6 processes: 10.39s—marked by the dotted line in [Figure 19-2](#procs_x_time_fig). I expected the run time to increase after 6 processes due to CPU contention, and it reached a local maximum of 12.51s at 10 processes. I did not expect and I can’t explain why the performance improved at 11 processes and remained almost flat from 13 to 20 processes, with median times only slightly higher than the lowest median time at 6 processes.