## Distributed Task Queues

When the application server delivers a request to one of the Python processes running your code, your app needs to respond quickly: you want the process to be available to handle the next request as soon as possible. However, some requests demand actions that may take longer—for example, sending email or generating a PDF. That’s the problem that distributed task queues are designed to solve.

[_Celery_](https://fpy.li/19-47) and [_RQ_](https://fpy.li/19-48) are the best known open source task queues with Python APIs. Cloud providers also offer their own proprietary task queues.

These products wrap a message queue and offer a high-level API for delegating tasks to workers, possibly running on different machines.

###### Note

In the context of task queues, the words _producer_ and _consumer_ are used instead of traditional client/server terminology. For example, a _Django_ view handler _produces_ job requests, which are put in the queue to be _consumed_ by one or more PDF rendering processes.

Quoting directly from _Celery_’s [FAQ](https://fpy.li/19-49), here are some typical use cases:

> - Running something in the background. For example, to finish the web request as soon as possible, then update the users page incrementally. This gives the user the impression of good performance and “snappiness,” even though the real work might actually take some time.
>     
> - Running something after the web request has finished.
>     
> - Making sure something is done, by executing it asynchronously and using retries.
>     
> - Scheduling periodic work.
>     

Besides solving these immediate problems, task queues support horizontal scalability. Producers and consumers are decoupled: a producer doesn’t call a consumer, it puts a request in a queue. Consumers don’t need to know anything about the producers (but the request may include information about the producer, if an acknowledgment is required). Crucially, you can easily add more workers to consume tasks as demand grows. That’s why _Celery_ and _RQ_ are called distributed task queues.

Recall that our simple _procs.py_ ([Example 19-13](#primes_procs_top_ex)) used two queues: one for job requests, the other for collecting results. The distributed architecture of _Celery_ and _RQ_ uses a similar pattern. Both support using the [_Redis_](https://fpy.li/19-50) NoSQL database as a message queue and result storage. _Celery_ also supports other message queues like _RabbitMQ_ or _Amazon SQS_, as well other databases for result storage.

This wraps up our introduction to concurrency in Python. The next two chapters will continue this theme, focusing on the `concurrent.futures` and `asyncio` packages of the standard library.