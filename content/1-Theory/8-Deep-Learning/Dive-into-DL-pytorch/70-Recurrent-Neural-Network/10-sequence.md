---
publish: "true"
tags:
  - D2L
  - Deep_Learning
  - PyTorch
date: 2024-02-21
---
# åºåˆ—æ¨¡å‹

æƒ³è±¡ä¸€ä¸‹æœ‰äººæ­£åœ¨çœ‹ç½‘é£ï¼ˆNetflixï¼Œä¸€ä¸ªå›½å¤–çš„è§†é¢‘ç½‘ç«™ï¼‰ä¸Šçš„ç”µå½±ã€‚ä¸€åå¿ å®çš„ç”¨æˆ·ä¼šå¯¹æ¯ä¸€éƒ¨ç”µå½±éƒ½ç»™å‡ºè¯„ä»·ï¼Œæ¯•ç«Ÿä¸€éƒ¨å¥½ç”µå½±éœ€è¦æ›´å¤šçš„æ”¯æŒå’Œè®¤å¯ã€‚ç„¶è€Œäº‹å®è¯æ˜ï¼Œäº‹æƒ…å¹¶ä¸é‚£ä¹ˆç®€å•ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œäººä»¬å¯¹ç”µå½±çš„çœ‹æ³•ä¼šå‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ã€‚äº‹å®ä¸Šï¼Œå¿ƒç†å­¦å®¶ç”šè‡³å¯¹è¿™äº›ç°è±¡èµ·äº†åå­—ï¼š

* *é”šå®š*ï¼ˆanchoringï¼‰æ•ˆåº”ï¼šåŸºäºå…¶ä»–äººçš„æ„è§åšå‡ºè¯„ä»·ã€‚ä¾‹å¦‚ï¼Œå¥¥æ–¯å¡é¢å¥–åï¼Œå—åˆ°å…³æ³¨çš„ç”µå½±çš„è¯„åˆ†ä¼šä¸Šå‡ï¼Œå°½ç®¡å®ƒè¿˜æ˜¯åŸæ¥é‚£éƒ¨ç”µå½±ã€‚è¿™ç§å½±å“å°†æŒç»­å‡ ä¸ªæœˆï¼Œç›´åˆ°äººä»¬å¿˜è®°äº†è¿™éƒ¨ç”µå½±æ›¾ç»è·å¾—çš„å¥–é¡¹ã€‚ç»“æœ[^1]è¡¨æ˜ï¼Œè¿™ç§æ•ˆåº”ä¼šä½¿è¯„åˆ†æé«˜åŠä¸ªç™¾åˆ†ç‚¹ä»¥ä¸Šã€‚
* *äº«ä¹é€‚åº”*ï¼ˆhedonic adaptionï¼‰ï¼šäººä»¬è¿…é€Ÿæ¥å—å¹¶ä¸”é€‚åº”ä¸€ç§æ›´å¥½æˆ–è€…æ›´åçš„æƒ…å†µä½œä¸ºæ–°çš„å¸¸æ€ã€‚ä¾‹å¦‚ï¼Œåœ¨çœ‹äº†å¾ˆå¤šå¥½ç”µå½±ä¹‹åï¼Œäººä»¬ä¼šå¼ºçƒˆæœŸæœ›ä¸‹éƒ¨ç”µå½±ä¼šæ›´å¥½ã€‚å› æ­¤ï¼Œåœ¨è®¸å¤šç²¾å½©çš„ç”µå½±è¢«çœ‹è¿‡ä¹‹åï¼Œå³ä½¿æ˜¯ä¸€éƒ¨æ™®é€šçš„ä¹Ÿå¯èƒ½è¢«è®¤ä¸ºæ˜¯ç³Ÿç³•çš„ã€‚
* *å­£èŠ‚æ€§*ï¼ˆseasonalityï¼‰ï¼šå°‘æœ‰è§‚ä¼—å–œæ¬¢åœ¨å…«æœˆçœ‹åœ£è¯è€äººçš„ç”µå½±ã€‚
* æœ‰æ—¶ï¼Œç”µå½±ä¼šç”±äºå¯¼æ¼”æˆ–æ¼”å‘˜åœ¨åˆ¶ä½œä¸­çš„ä¸å½“è¡Œä¸ºå˜å¾—ä¸å—æ¬¢è¿ã€‚
* æœ‰äº›ç”µå½±å› ä¸ºå…¶æåº¦ç³Ÿç³•åªèƒ½æˆä¸ºå°ä¼—ç”µå½±ã€‚*Plan9from Outer Space* å’Œ*Troll2* å°±å› ä¸ºè¿™ä¸ªåŸå› è€Œè‡­åæ˜­è‘—çš„ã€‚
* å¯ä»¥å†åŠ ä¸€ä¸ªï¼Œæ°´å†›ğŸ˜…

ç®€è€Œè¨€ä¹‹ï¼Œç”µå½±è¯„åˆ†å†³ä¸æ˜¯å›ºå®šä¸å˜çš„ã€‚å› æ­¤ï¼Œä½¿ç”¨æ—¶é—´åŠ¨åŠ›å­¦å¯ä»¥å¾—åˆ°æ›´å‡†ç¡®çš„ç”µå½±æ¨è[^2]ã€‚å½“ç„¶ï¼Œåºåˆ—æ•°æ®ä¸ä»…ä»…æ˜¯å…³äºç”µå½±è¯„åˆ†çš„ã€‚ä¸‹é¢ç»™å‡ºäº†æ›´å¤šçš„åœºæ™¯ã€‚

* åœ¨ä½¿ç”¨ç¨‹åºæ—¶ï¼Œè®¸å¤šç”¨æˆ·éƒ½æœ‰å¾ˆå¼ºçš„ç‰¹å®šä¹ æƒ¯ã€‚ä¾‹å¦‚ï¼Œåœ¨å­¦ç”Ÿæ”¾å­¦åç¤¾äº¤åª’ä½“åº”ç”¨æ›´å—æ¬¢è¿ã€‚åœ¨å¸‚åœºå¼€æ”¾æ—¶è‚¡å¸‚äº¤æ˜“è½¯ä»¶æ›´å¸¸ç”¨ã€‚
* é¢„æµ‹æ˜å¤©çš„è‚¡ä»·è¦æ¯”è¿‡å»çš„è‚¡ä»·æ›´å›°éš¾ï¼Œå°½ç®¡ä¸¤è€…éƒ½åªæ˜¯ä¼°è®¡ä¸€ä¸ªæ•°å­—ã€‚æ¯•ç«Ÿï¼Œå…ˆè§ä¹‹æ˜æ¯”äº‹åè¯¸è‘›äº®éš¾å¾—å¤šã€‚åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œå‰è€…ï¼ˆå¯¹è¶…å‡ºå·²çŸ¥è§‚æµ‹èŒƒå›´è¿›è¡Œé¢„æµ‹ï¼‰ç§°ä¸º*å¤–æ¨æ³•*ï¼ˆextrapolationï¼‰ï¼Œè€Œåè€…ï¼ˆåœ¨ç°æœ‰è§‚æµ‹å€¼ä¹‹é—´è¿›è¡Œä¼°è®¡ï¼‰ç§°ä¸º*å†…æ’æ³•*ï¼ˆinterpolationï¼‰ã€‚
* åœ¨æœ¬è´¨ä¸Šï¼ŒéŸ³ä¹ã€è¯­éŸ³ã€æ–‡æœ¬å’Œè§†é¢‘éƒ½æ˜¯è¿ç»­çš„ã€‚å¦‚æœå®ƒä»¬çš„åºåˆ—è¢«æˆ‘ä»¬é‡æ’ï¼Œé‚£ä¹ˆå°±ä¼šå¤±å»åŸæœ‰çš„æ„ä¹‰ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ªæ–‡æœ¬æ ‡é¢˜â€œç‹—å’¬äººâ€è¿œæ²¡æœ‰â€œäººå’¬ç‹—â€é‚£ä¹ˆä»¤äººæƒŠè®¶ï¼Œå°½ç®¡ç»„æˆä¸¤å¥è¯çš„å­—å®Œå…¨ç›¸åŒã€‚
* åœ°éœ‡å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œå³å¤§åœ°éœ‡å‘ç”Ÿåï¼Œå¾ˆå¯èƒ½ä¼šæœ‰å‡ æ¬¡å°ä½™éœ‡ï¼Œè¿™äº›ä½™éœ‡çš„å¼ºåº¦æ¯”éå¤§åœ°éœ‡åçš„ä½™éœ‡è¦å¤§å¾—å¤šã€‚äº‹å®ä¸Šï¼Œåœ°éœ‡æ˜¯æ—¶ç©ºç›¸å…³çš„ï¼Œå³ä½™éœ‡é€šå¸¸å‘ç”Ÿåœ¨å¾ˆçŸ­çš„æ—¶é—´è·¨åº¦å’Œå¾ˆè¿‘çš„è·ç¦»å†…ã€‚
* äººç±»ä¹‹é—´çš„äº’åŠ¨ä¹Ÿæ˜¯è¿ç»­çš„ï¼Œè¿™å¯ä»¥ä»å¾®åšä¸Šçš„äº‰åµå’Œè¾©è®ºä¸­çœ‹å‡ºã€‚

## ç»Ÿè®¡å·¥å…·

å¤„ç†åºåˆ—æ•°æ®éœ€è¦ç»Ÿè®¡å·¥å…·å’Œæ–°çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä»¥ä¸‹å›¾æ‰€ç¤ºçš„è‚¡ç¥¨ä»·æ ¼ï¼ˆå¯Œæ—¶100æŒ‡æ•°ï¼‰ä¸ºä¾‹ï¼š
- è¿‘ 30 å¹´çš„å¯Œæ—¶ 100 æŒ‡æ•°ï¼š ![[ftse100.png]]
- å…¶ä¸­ï¼Œç”¨ $x_t$ è¡¨ç¤ºä»·æ ¼ï¼Œå³åœ¨*æ—¶é—´æ­¥*ï¼ˆtime stepï¼‰$t \in \mathbb{Z}^+$ æ—¶ï¼Œè§‚å¯Ÿåˆ°çš„ä»·æ ¼ $x_t$ ã€‚è¯·æ³¨æ„ï¼Œ$t$ **å¯¹äºæœ¬æ–‡ä¸­çš„åºåˆ—é€šå¸¸æ˜¯ç¦»æ•£çš„ï¼Œå¹¶åœ¨æ•´æ•°æˆ–å…¶å­é›†ä¸Šå˜åŒ–**ã€‚

å‡è®¾ä¸€ä¸ªäº¤æ˜“å‘˜æƒ³åœ¨ $t$ æ—¥çš„è‚¡å¸‚ä¸­è¡¨ç°è‰¯å¥½ï¼Œäºæ˜¯é€šè¿‡ä»¥ä¸‹é€”å¾„é¢„æµ‹ $x_t$ ï¼š
$$
x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).\tag{7.1.1}
$$

### è‡ªå›å½’æ¨¡å‹

ä¸ºäº†å®ç°è¿™ä¸ªé¢„æµ‹ï¼Œäº¤æ˜“å‘˜å¯ä»¥ä½¿ç”¨å›å½’æ¨¡å‹ï¼Œä¾‹å¦‚åœ¨ [[30-linear-regression-concise-implementation|2.3 èŠ‚]] ä¸­è®­ç»ƒçš„æ¨¡å‹ã€‚ä»…æœ‰ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼šè¾“å…¥æ•°æ®çš„æ•°é‡ï¼Œè¾“å…¥ $x_{t-1}, \ldots, x_1$ æœ¬èº«å›  $t$ è€Œå¼‚ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥æ•°æ®çš„æ•°é‡è¿™ä¸ªæ•°å­—å°†ä¼šéšç€æˆ‘ä»¬é‡åˆ°çš„æ•°æ®é‡çš„å¢åŠ è€Œå¢åŠ ï¼Œå› æ­¤éœ€è¦ä¸€ä¸ªè¿‘ä¼¼æ–¹æ³•æ¥ä½¿è¿™ä¸ªè®¡ç®—å˜å¾—å®¹æ˜“å¤„ç†ã€‚æœ¬ç« åé¢çš„å¤§éƒ¨åˆ†å†…å®¹å°†å›´ç»•ç€å¦‚ä½•æœ‰æ•ˆä¼°è®¡ $P(x_t \mid x_{t-1}, \ldots, x_1)$ å±•å¼€ã€‚ç®€å•åœ°è¯´ï¼Œå®ƒå½’ç»“ä¸ºä»¥ä¸‹ä¸¤ç§ç­–ç•¥ã€‚

- ç¬¬ä¸€ç§ç­–ç•¥ï¼Œå‡è®¾åœ¨ç°å®æƒ…å†µä¸‹ç›¸å½“é•¿çš„åºåˆ— $x_{t-1}, \ldots, x_1$ å¯èƒ½æ˜¯ä¸å¿…è¦çš„ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦æ»¡è¶³æŸä¸ªé•¿åº¦ä¸º $\tau$ çš„æ—¶é—´è·¨åº¦ï¼Œå³ä½¿ç”¨è§‚æµ‹åºåˆ— $x_{t-1}, \ldots, x_{t-\tau}$ ã€‚å½“ä¸‹è·å¾—çš„**æœ€ç›´æ¥çš„å¥½å¤„å°±æ˜¯å‚æ•°çš„æ•°é‡æ€»æ˜¯ä¸å˜çš„**ï¼Œè‡³å°‘åœ¨ $t > \tau$ æ—¶å¦‚æ­¤ï¼Œè¿™å°±ä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªä¸Šé¢æåŠçš„æ·±åº¦ç½‘ç»œã€‚è¿™ç§æ¨¡å‹è¢«ç§°ä¸º***è‡ªå›å½’æ¨¡å‹***ï¼ˆautoregressive modelsï¼‰ï¼Œå› ä¸ºå®ƒä»¬æ˜¯å¯¹è‡ªå·±æ‰§è¡Œå›å½’ã€‚

- ç¬¬äºŒç§ç­–ç•¥ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ˜¯**ä¿ç•™ä¸€äº›å¯¹è¿‡å»è§‚æµ‹çš„æ€»ç»“ $h_t$ ï¼Œå¹¶ä¸”åŒæ—¶æ›´æ–°é¢„æµ‹ $\hat{x}_t$ å’Œæ€»ç»“ $h_t$** ã€‚è¿™å°±äº§ç”Ÿäº†åŸºäº $\hat{x}_t = P(x_t \mid h_{t})$ ä¼°è®¡ $x_t$ï¼Œä»¥åŠå…¬å¼ $h_t = g(h_{t-1}, x_{t-1})$ æ›´æ–°çš„æ¨¡å‹ã€‚ç”±äº $h_t$ ä»æœªè¢«è§‚æµ‹åˆ°ï¼Œè¿™ç±»æ¨¡å‹ä¹Ÿè¢«ç§°ä¸º***éšå˜é‡è‡ªå›å½’æ¨¡å‹***ï¼ˆlatent autoregressive modelsï¼‰ã€‚
	- ![[sequence-model.svg]]

è¿™ä¸¤ç§æƒ…å†µéƒ½æœ‰ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„é—®é¢˜ï¼šå¦‚ä½•ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Ÿä¸€ä¸ªç»å…¸æ–¹æ³•æ˜¯ä½¿ç”¨å†å²è§‚æµ‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæœªæ¥è§‚æµ‹ã€‚æ˜¾ç„¶ï¼Œæˆ‘ä»¬å¹¶ä¸æŒ‡æœ›æ—¶é—´ä¼šåœæ»ä¸å‰ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå¸¸è§çš„å‡è®¾æ˜¯è™½ç„¶ç‰¹å®šå€¼ $x_t$ å¯èƒ½ä¼šæ”¹å˜ï¼Œä½†æ˜¯åºåˆ—æœ¬èº«çš„åŠ¨æ€ç‰¹å¾ä¸ä¼šæ”¹å˜ã€‚è¿™æ ·çš„å‡è®¾æ˜¯åˆç†çš„ï¼Œå› ä¸ºæ–°çš„åŠ¨æ€ç‰¹å¾ä¸€å®šå—æ–°çš„æ•°æ®å½±å“ï¼Œè€Œæˆ‘ä»¬ä¸å¯èƒ½ç”¨ç›®å‰æ‰€æŒæ¡çš„æ•°æ®æ¥é¢„æµ‹æ–°çš„åŠ¨åŠ›å­¦ã€‚ç»Ÿè®¡å­¦å®¶ç§°ä¸å˜çš„åŠ¨æ€ç‰¹å¾ä¸º*é™æ­¢çš„*ï¼ˆstationaryï¼‰ã€‚å› æ­¤ï¼Œæ•´ä¸ªåºåˆ—çš„ä¼°è®¡å€¼éƒ½å°†é€šè¿‡ä»¥ä¸‹çš„æ–¹å¼è·å¾—ï¼š
$$
P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).\tag{7.1.2}
$$

æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬å¤„ç†çš„æ˜¯ç¦»æ•£çš„å¯¹è±¡ï¼ˆå¦‚å•è¯ï¼‰ï¼Œè€Œä¸æ˜¯è¿ç»­çš„æ•°å­—ï¼Œåˆ™ä¸Šè¿°çš„è€ƒè™‘ä»ç„¶æœ‰æ•ˆã€‚å”¯ä¸€çš„å·®åˆ«æ˜¯ï¼Œå¯¹äºç¦»æ•£çš„å¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åˆ†ç±»å™¨è€Œä¸æ˜¯å›å½’æ¨¡å‹æ¥ä¼°è®¡ $P(x_t \mid  x_{t-1}, \ldots, x_1)$ ã€‚

### é©¬å°”å¯å¤«æ¨¡å‹

å›æƒ³ä¸€ä¸‹ï¼Œåœ¨è‡ªå›å½’æ¨¡å‹çš„è¿‘ä¼¼æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ $x_{t-1}, \ldots, x_{t-\tau}$ è€Œä¸æ˜¯ $x_{t-1}, \ldots, x_1$ æ¥ä¼°è®¡ $x_t$ ã€‚åªè¦è¿™ç§æ˜¯è¿‘ä¼¼ç²¾ç¡®çš„ï¼Œæˆ‘ä»¬å°±è¯´åºåˆ—æ»¡è¶³*é©¬å°”å¯å¤«æ¡ä»¶*ï¼ˆMarkov conditionï¼‰ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚æœ $\tau = 1$ ï¼Œå¾—åˆ°ä¸€ä¸ª*ä¸€é˜¶é©¬å°”å¯å¤«æ¨¡å‹*ï¼ˆfirst-order Markov modelï¼‰ï¼Œ$P(x)$ ç”±ä¸‹å¼ç»™å‡ºï¼š
$$
P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ å½“ } P(x_1 \mid x_0) = P(x_1).\tag{7.1.3}
$$

>[!note] [Causal Markov condition](https://en.wikipedia.org/wiki/Causal_Markov_condition?useskin=vector)
>The **Markov condition**, sometimes called the **Markov assumption**, is an assumption made in Bayesian probability theory, that every node in a Bayesian network is conditionally independent of its nondescendants, given its parents. Stated loosely, it is assumed that a node has no bearing on nodes which do not descend from it. In a DAG, this local Markov condition is equivalent to the global Markov condition, which states that d-separations in the graph also correspond to conditional independence relations. This also means that a node is conditionally independent of the entire network, given its Markov blanket.
>The related Causal Markov (CM) condition states that, conditional on the set of all its direct causes, a node is independent of all variables which are not effects or direct causes of that node. In the event that the structure of a Bayesian network accurately depicts causality, the two conditions are equivalent. However, a network may accurately embody the Markov condition without depicting causality, in which case it should not be assumed to embody the causal Markov condition.

å½“å‡è®¾ $x_t$ ä»…æ˜¯ç¦»æ•£å€¼æ—¶ï¼Œè¿™æ ·çš„æ¨¡å‹ç‰¹åˆ«æ£’ï¼Œå› ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŠ¨æ€è§„åˆ’å¯ä»¥æ²¿ç€é©¬å°”å¯å¤«é“¾ç²¾ç¡®åœ°è®¡ç®—ç»“æœã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°è®¡ç®— $P(x_{t+1} \mid x_{t-1})$ ï¼š

$$
\begin{aligned}
P(x_{t+1} \mid x_{t-1})
&= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\
&= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\
&= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})
\end{aligned}\tag{7.1.4}
$$

åˆ©ç”¨è¿™ä¸€äº‹å®ï¼Œæˆ‘ä»¬åªéœ€è¦è€ƒè™‘è¿‡å»è§‚å¯Ÿä¸­çš„ä¸€ä¸ªéå¸¸çŸ­çš„å†å²ï¼š$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$ ã€‚éšé©¬å°”å¯å¤«æ¨¡å‹ä¸­çš„åŠ¨æ€è§„åˆ’è¶…å‡ºäº†æœ¬èŠ‚çš„èŒƒå›´ï¼Œè€ŒåŠ¨æ€è§„åˆ’è¿™äº›è®¡ç®—å·¥å…·å·²ç»åœ¨æ§åˆ¶ç®—æ³•å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•å¹¿æ³›ä½¿ç”¨ã€‚

### å› æœå…³ç³»

åŸåˆ™ä¸Šï¼Œå°† $P(x_1, \ldots, x_T)$ å€’åºå±•å¼€ä¹Ÿæ²¡ä»€ä¹ˆé—®é¢˜ã€‚æ¯•ç«Ÿï¼ŒåŸºäºæ¡ä»¶æ¦‚ç‡å…¬å¼ï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥å†™å‡ºï¼š
$$
P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).\tag{7.1.5}
$$

äº‹å®ä¸Šï¼Œå¦‚æœåŸºäºä¸€ä¸ªé©¬å°”å¯å¤«æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥å¾—åˆ°ä¸€ä¸ªåå‘çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œ**æ•°æ®å­˜åœ¨ä¸€ä¸ªè‡ªç„¶çš„æ–¹å‘ï¼Œå³åœ¨æ—¶é—´ä¸Šæ˜¯å‰è¿›çš„**ã€‚å¾ˆæ˜æ˜¾ï¼Œæœªæ¥çš„äº‹ä»¶ä¸èƒ½å½±å“è¿‡å»ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æ”¹å˜ $x_t$ï¼Œå¯èƒ½ä¼šå½±å“æœªæ¥å‘ç”Ÿçš„äº‹æƒ… $x_{t+1}$ï¼Œä½†ä¸èƒ½åè¿‡æ¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬æ”¹å˜ $x_t$ï¼ŒåŸºäºè¿‡å»äº‹ä»¶å¾—åˆ°çš„åˆ†å¸ƒä¸ä¼šæ”¹å˜ã€‚å› æ­¤ï¼Œè§£é‡Š $P(x_{t+1} \mid x_t)$ åº”è¯¥æ¯”è§£é‡Š $P(x_t \mid x_{t+1})$ æ›´å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯¹äºæŸäº›å¯åŠ æ€§å™ªå£° $\epsilon$ï¼Œæ˜¾ç„¶æˆ‘ä»¬å¯ä»¥æ‰¾åˆ° $x_{t+1} = f(x_t) + \epsilon$ ï¼Œè€Œåä¹‹åˆ™ä¸è¡Œ[^3]ã€‚è€Œè¿™ä¸ªå‘å‰æ¨è¿›çš„æ–¹å‘æ°å¥½ä¹Ÿæ˜¯æˆ‘ä»¬é€šå¸¸æ„Ÿå…´è¶£çš„æ–¹å‘ã€‚å½¼å¾—æ–¯ç­‰äºº[^4]å¯¹è¯¥ä¸»é¢˜çš„æ›´å¤šå†…å®¹åšäº†è¯¦å°½çš„è§£é‡Šï¼Œè€Œæˆ‘ä»¬çš„ä¸Šè¿°è®¨è®ºåªæ˜¯å…¶ä¸­çš„å†°å±±ä¸€è§’ã€‚

## è®­ç»ƒ

åœ¨äº†è§£äº†ä¸Šè¿°ç»Ÿè®¡å·¥å…·åï¼Œè®©æˆ‘ä»¬åœ¨å®è·µä¸­å°è¯•ä¸€ä¸‹ï¼é¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€äº›æ•°æ®ï¼šä½¿ç”¨æ­£å¼¦å‡½æ•°å’Œä¸€äº›å¯åŠ æ€§å™ªå£°æ¥ç”Ÿæˆåºåˆ—æ•°æ®ï¼Œæ—¶é—´æ­¥ä¸º $1, 2, \ldots, 1000$ ã€‚

```python
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import torch
from torch import nn

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
time = d2l.arange(1, T + 1, dtype=d2l.float32)
x = d2l.sin(0.01 * time) + d2l.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```

![[10-sequence-sin.png]]

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªåºåˆ—è½¬æ¢ä¸ºæ¨¡å‹çš„*ç‰¹å¾ï¼æ ‡ç­¾*ï¼ˆfeature-labelï¼‰å¯¹ã€‚åŸºäºåµŒå…¥ç»´åº¦ $\tau$ï¼Œæˆ‘ä»¬å°†æ•°æ®æ˜ å°„ä¸ºæ•°æ®å¯¹ $y_t = x_t$ å’Œ $\mathbf{x}_t = [x_{t-\tau}, \ldots, x_{t-1}]$ ã€‚è¿™æ¯”æˆ‘ä»¬æä¾›çš„æ•°æ®æ ·æœ¬å°‘äº† $\tau$ ä¸ªï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è¶³å¤Ÿçš„å†å²è®°å½•æ¥æè¿°å‰ $\tau$ ä¸ªæ•°æ®æ ·æœ¬ã€‚ä¸€ä¸ªç®€å•çš„è§£å†³åŠæ³•æ˜¯ï¼šå¦‚æœæ‹¥æœ‰è¶³å¤Ÿé•¿çš„åºåˆ—å°±ä¸¢å¼ƒè¿™å‡ é¡¹ï¼›å¦ä¸€ä¸ªæ–¹æ³•æ˜¯ç”¨é›¶å¡«å……åºåˆ—ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å‰600ä¸ªâ€œç‰¹å¾ï¼æ ‡ç­¾â€å¯¹è¿›è¡Œè®­ç»ƒã€‚

```python
tau = 4
features = d2l.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = d2l.reshape(x[tau:], (-1, 1))

batch_size, n_train = 16, 600
# åªæœ‰å‰n_trainä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç›¸å½“ç®€å•çš„æ¶æ„è®­ç»ƒæ¨¡å‹ï¼šä¸€ä¸ªæ‹¥æœ‰ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ŒReLUæ¿€æ´»å‡½æ•°å’Œå¹³æ–¹æŸå¤±ã€‚

```python
# åˆå§‹åŒ–ç½‘ç»œæƒé‡çš„å‡½æ•°
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

# ä¸€ä¸ªç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº
def get_net():
    net = nn.Sequential(nn.Linear(4, 10),
                        nn.ReLU(),
                        nn.Linear(10, 1))
    net.apply(init_weights)
    return net

# å¹³æ–¹æŸå¤±ã€‚æ³¨æ„ï¼šMSELossè®¡ç®—å¹³æ–¹è¯¯å·®æ—¶ä¸å¸¦ç³»æ•°1/2
loss = nn.MSELoss(reduction='none')
```

ç°åœ¨ï¼Œå‡†å¤‡è®­ç»ƒæ¨¡å‹äº†ã€‚å®ç°ä¸‹é¢çš„è®­ç»ƒä»£ç çš„æ–¹å¼ä¸å‰é¢å‡ èŠ‚ï¼ˆå¦‚ [[30-linear-regression-concise-implementation|2.3 èŠ‚]]ï¼‰ä¸­çš„å¾ªç¯è®­ç»ƒåŸºæœ¬ç›¸åŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šæ·±å…¥æ¢è®¨å¤ªå¤šç»†èŠ‚ã€‚

```python
def train(net, train_iter, loss, epochs, lr):
    trainer = torch.optim.Adam(net.parameters(), lr)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.sum().backward()
            trainer.step()
        print(f'epoch {epoch + 1}, '
              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')

net = get_net()
train(net, train_iter, loss, 5, 0.01)
```

```output
epoch 1, loss: 0.066243
epoch 2, loss: 0.057641
epoch 3, loss: 0.056657
epoch 4, loss: 0.057712
epoch 5, loss: 0.056605

```

## é¢„æµ‹

ç”±äºè®­ç»ƒæŸå¤±å¾ˆå°ï¼Œå› æ­¤æˆ‘ä»¬æœŸæœ›æ¨¡å‹èƒ½æœ‰å¾ˆå¥½çš„å·¥ä½œæ•ˆæœã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ„å‘³ç€ä»€ä¹ˆã€‚

é¦–å…ˆæ˜¯æ£€æŸ¥æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯*å•æ­¥é¢„æµ‹*ï¼ˆone-step-ahead predictionï¼‰ã€‚

```python
onestep_preds = net(features)
d2l.plot([time, time[tau:]], 
         [d2l.numpy(x), d2l.numpy(onestep_preds)], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000], 
         figsize=(6, 3))
```

![[1step.svg]]

æ­£å¦‚æˆ‘ä»¬æ‰€æ–™ï¼Œå•æ­¥é¢„æµ‹æ•ˆæœä¸é”™ã€‚å³ä½¿è¿™äº›é¢„æµ‹çš„æ—¶é—´æ­¥è¶…è¿‡äº† $600+4$ï¼ˆ`n_train + tau`ï¼‰ï¼Œå…¶ç»“æœçœ‹èµ·æ¥ä»ç„¶æ˜¯å¯ä¿¡çš„ã€‚ç„¶è€Œæœ‰ä¸€ä¸ªå°é—®é¢˜ï¼šå¦‚æœæ•°æ®è§‚å¯Ÿåºåˆ—çš„æ—¶é—´æ­¥åªåˆ° $604$ ï¼Œæˆ‘ä»¬éœ€è¦ä¸€æ­¥ä¸€æ­¥åœ°å‘å‰è¿ˆè¿›ï¼š
$$
\begin{aligned}
\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\
\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \hat{x}_{605}), \\
\hat{x}_{607} = f(x_{603}, x_{604}, \hat{x}_{605}, \hat{x}_{606}),\\
\hat{x}_{608} = f(x_{604}, \hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}),\\
\hat{x}_{609} = f(\hat{x}_{605}, \hat{x}_{606}, \hat{x}_{607}, \hat{x}_{608}),\\
\ldots
\end{aligned}\tag{7.1.6}
$$

é€šå¸¸ï¼Œå¯¹äºç›´åˆ° $x_t$ çš„è§‚æµ‹åºåˆ—ï¼Œå…¶åœ¨æ—¶é—´æ­¥ $t+k$ å¤„çš„é¢„æµ‹è¾“å‡º $\hat{x}_{t+k}$ ç§°ä¸º $k$*æ­¥é¢„æµ‹*ï¼ˆ$k$ -step-ahead-predictionï¼‰ã€‚ç”±äºæˆ‘ä»¬çš„è§‚å¯Ÿå·²ç»åˆ°äº† $x_{604}$ï¼Œå®ƒçš„ $k$ æ­¥é¢„æµ‹æ˜¯ $\hat{x}_{604+k}$ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„é¢„æµ‹ï¼ˆè€Œä¸æ˜¯åŸå§‹æ•°æ®ï¼‰æ¥è¿›è¡Œå¤šæ­¥é¢„æµ‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚

```python
multistep_preds = d2l.zeros(T)
multistep_preds[: n_train + tau] = x[: n_train + tau]
for i in range(n_train + tau, T):
    multistep_preds[i] = net(
        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))

d2l.plot([time, time[tau:], time[n_train + tau:]],
         [d2l.numpy(x), d2l.numpy(onestep_preds),
          d2l.numpy(multistep_preds[n_train + tau:])], 'time',
         'x', legend=['data', '1-step preds', 'multistep preds'],
         xlim=[1, 1000], figsize=(6, 3))
```

![[multistep.svg]]

å¦‚ä¸Šé¢çš„ä¾‹å­æ‰€ç¤ºï¼Œç»¿çº¿çš„é¢„æµ‹æ˜¾ç„¶å¹¶ä¸ç†æƒ³ã€‚ç»è¿‡å‡ ä¸ªé¢„æµ‹æ­¥éª¤ä¹‹åï¼Œé¢„æµ‹çš„ç»“æœå¾ˆå¿«å°±ä¼šè¡°å‡åˆ°ä¸€ä¸ªå¸¸æ•°ã€‚ä¸ºä»€ä¹ˆè¿™ä¸ªç®—æ³•æ•ˆæœè¿™ä¹ˆå·®å‘¢ï¼Ÿäº‹å®æ˜¯ç”±äºé”™è¯¯çš„ç´¯ç§¯ï¼šå‡è®¾åœ¨æ­¥éª¤ $1$ ä¹‹åï¼Œæˆ‘ä»¬ç§¯ç´¯äº†ä¸€äº›é”™è¯¯ $\epsilon_1 = \bar\epsilon$ ã€‚äºæ˜¯ï¼Œæ­¥éª¤ $2$ çš„è¾“å…¥è¢«æ‰°åŠ¨äº† $\epsilon_1$ï¼Œç»“æœç§¯ç´¯çš„è¯¯å·®æ˜¯ä¾ç…§æ¬¡åºçš„ $\epsilon_2 = \bar\epsilon + c \epsilon_1$ ï¼Œå…¶ä¸­ $c$ ä¸ºæŸä¸ªå¸¸æ•°ï¼Œåé¢çš„é¢„æµ‹è¯¯å·®ä¾æ­¤ç±»æ¨ã€‚å› æ­¤è¯¯å·®å¯èƒ½ä¼šç›¸å½“å¿«åœ°åç¦»çœŸå®çš„è§‚æµ‹ç»“æœã€‚ä¾‹å¦‚ï¼Œæœªæ¥ $24$ å°æ—¶çš„å¤©æ°”é¢„æŠ¥å¾€å¾€ç›¸å½“å‡†ç¡®ï¼Œä½†è¶…è¿‡è¿™ä¸€ç‚¹ï¼Œç²¾åº¦å°±ä¼šè¿…é€Ÿä¸‹é™ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åŠåç»­ç« èŠ‚ä¸­è®¨è®ºå¦‚ä½•æ”¹è¿›è¿™ä¸€ç‚¹ã€‚

åŸºäº $k = 1, 4, 16, 64$ï¼Œé€šè¿‡å¯¹æ•´ä¸ªåºåˆ—é¢„æµ‹çš„è®¡ç®—ï¼Œè®©æˆ‘ä»¬æ›´ä»”ç»†åœ°çœ‹ä¸€ä¸‹ $k$ æ­¥é¢„æµ‹çš„å›°éš¾ã€‚

```python
max_steps = 64
```

```python
features = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))
# åˆ—iï¼ˆi<tauï¼‰æ˜¯æ¥è‡ªxçš„è§‚æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
for i in range(tau):
    features[:, i] = x[i: i + T - tau - max_steps + 1]

# åˆ—iï¼ˆi>=tauï¼‰æ˜¯æ¥è‡ªï¼ˆi-tau+1ï¼‰æ­¥çš„é¢„æµ‹ï¼Œå…¶æ—¶é—´æ­¥ä»ï¼ˆiï¼‰åˆ°ï¼ˆi+T-tau-max_steps+1ï¼‰
for i in range(tau, tau + max_steps):
    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), -1)

steps = (1, 4, 16, 64)
d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],
         [d2l.numpy(features[:, tau + i - 1]) for i in steps], 'time', 'x',
         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],
         figsize=(6, 3))
```

![[64steps.svg]]

ä»¥ä¸Šä¾‹å­æ¸…æ¥šåœ°è¯´æ˜äº†å½“æˆ‘ä»¬è¯•å›¾é¢„æµ‹æ›´è¿œçš„æœªæ¥æ—¶ï¼Œé¢„æµ‹çš„è´¨é‡æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚è™½ç„¶â€œ$4$ æ­¥é¢„æµ‹â€çœ‹èµ·æ¥ä»ç„¶ä¸é”™ï¼Œä½†è¶…è¿‡è¿™ä¸ªè·¨åº¦çš„ä»»ä½•é¢„æµ‹å‡ ä¹éƒ½æ˜¯æ— ç”¨çš„ã€‚

## å°ç»“

* å†…æ’æ³•ï¼ˆåœ¨ç°æœ‰è§‚æµ‹å€¼ä¹‹é—´è¿›è¡Œä¼°è®¡ï¼‰å’Œå¤–æ¨æ³•ï¼ˆå¯¹è¶…å‡ºå·²çŸ¥è§‚æµ‹èŒƒå›´è¿›è¡Œé¢„æµ‹ï¼‰åœ¨å®è·µçš„éš¾åº¦ä¸Šå·®åˆ«å¾ˆå¤§ã€‚å› æ­¤ï¼Œå¯¹äºæ‰€æ‹¥æœ‰çš„åºåˆ—æ•°æ®ï¼Œåœ¨è®­ç»ƒæ—¶å§‹ç»ˆè¦å°Šé‡å…¶æ—¶é—´é¡ºåºï¼Œå³æœ€å¥½ä¸è¦åŸºäºæœªæ¥çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚
* åºåˆ—æ¨¡å‹çš„ä¼°è®¡éœ€è¦ä¸“é—¨çš„ç»Ÿè®¡å·¥å…·ï¼Œä¸¤ç§è¾ƒæµè¡Œçš„é€‰æ‹©æ˜¯è‡ªå›å½’æ¨¡å‹å’Œéšå˜é‡è‡ªå›å½’æ¨¡å‹ã€‚
* å¯¹äºæ—¶é—´æ˜¯å‘å‰æ¨è¿›çš„å› æœæ¨¡å‹ï¼Œæ­£å‘ä¼°è®¡é€šå¸¸æ¯”åå‘ä¼°è®¡æ›´å®¹æ˜“ã€‚
* å¯¹äºç›´åˆ°æ—¶é—´æ­¥$t$çš„è§‚æµ‹åºåˆ—ï¼Œå…¶åœ¨æ—¶é—´æ­¥$t+k$çš„é¢„æµ‹è¾“å‡ºæ˜¯â€œ$k$æ­¥é¢„æµ‹â€ã€‚éšç€æˆ‘ä»¬å¯¹é¢„æµ‹æ—¶é—´$k$å€¼çš„å¢åŠ ï¼Œä¼šé€ æˆè¯¯å·®çš„å¿«é€Ÿç´¯ç§¯å’Œé¢„æµ‹è´¨é‡çš„æé€Ÿä¸‹é™ã€‚

## ç»ƒä¹ 

1. æ”¹è¿›æœ¬èŠ‚å®éªŒä¸­çš„æ¨¡å‹ã€‚
    1. æ˜¯å¦åŒ…å«äº†è¿‡å»$4$ä¸ªä»¥ä¸Šçš„è§‚æµ‹ç»“æœï¼ŸçœŸå®å€¼éœ€è¦æ˜¯å¤šå°‘ä¸ªï¼Ÿ
    2. å¦‚æœæ²¡æœ‰å™ªéŸ³ï¼Œéœ€è¦å¤šå°‘ä¸ªè¿‡å»çš„è§‚æµ‹ç»“æœï¼Ÿæç¤ºï¼šæŠŠ$\sin$å’Œ$\cos$å†™æˆå¾®åˆ†æ–¹ç¨‹ã€‚
    3. å¯ä»¥åœ¨ä¿æŒç‰¹å¾æ€»æ•°ä¸å˜çš„æƒ…å†µä¸‹åˆå¹¶æ—§çš„è§‚å¯Ÿç»“æœå—ï¼Ÿè¿™èƒ½æé«˜æ­£ç¡®åº¦å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
    4. æ”¹å˜ç¥ç»ç½‘ç»œæ¶æ„å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚
2. ä¸€ä½æŠ•èµ„è€…æƒ³è¦æ‰¾åˆ°ä¸€ç§å¥½çš„è¯åˆ¸æ¥è´­ä¹°ã€‚ä»–æŸ¥çœ‹è¿‡å»çš„å›æŠ¥ï¼Œä»¥å†³å®šå“ªä¸€ç§å¯èƒ½æ˜¯è¡¨ç°è‰¯å¥½çš„ã€‚è¿™ä¸€ç­–ç•¥å¯èƒ½ä¼šå‡ºä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ
3. æ—¶é—´æ˜¯å‘å‰æ¨è¿›çš„å› æœæ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šé€‚ç”¨äºæ–‡æœ¬å‘¢ï¼Ÿ
4. ä¸¾ä¾‹è¯´æ˜ä»€ä¹ˆæ—¶å€™å¯èƒ½éœ€è¦éšå˜é‡è‡ªå›å½’æ¨¡å‹æ¥æ•æ‰æ•°æ®çš„åŠ¨åŠ›å­¦æ¨¡å‹ã€‚

[Discussions](https://discuss.d2l.ai/t/2091)


[^1]: Wu C Y, Ahmed A, Beutel A, et al. Recurrent recommender networks[C]//Proceedings of the tenth ACM international conference on web search and data mining. 2017: 495-503.
[^2]: Hu Y, Koren Y, Volinsky C. Collaborative filtering for implicit feedback datasets[C]//2008 Eighth IEEE international conference on data mining. Ieee, 2008: 263-272.
[^3]: Hoyer P, Janzing D, Mooij J M, et al. Nonlinear causal discovery with additive noise models[J]. Advances in neural information processing systems, 2008, 21.
[^4]: Peters J, Janzing D, SchÃ¶lkopf B. Elements of causal inference: foundations and learning algorithms[M]. The MIT Press, 2017.