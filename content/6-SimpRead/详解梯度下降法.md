
## 1 导言

梯度下降法是机器学习中一种常用到的算法，但其本身不是机器学习算法，而是一种求解的最优化算法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。  
机器学习的本质就是 “喂” 给模型数据，让模型不断地去学习，而这个学习的过程就是利用梯度下降法不断去优化的过程，目前最为常见的深度神经网络便是利用梯度的反向传播，反复更新模型参数直至收敛，从而达到优化模型的目的。  
对于最简单的线性模型，如 
$$h(\theta)={\theta}_0+{\theta}_1x_1+{\theta}_2x_2+{\theta}_3x_3+...+{\theta}_ix_i\\
$$

我们假设其损失函数为 
$$J(\theta)=\frac{1}{2}[ h_t(x)-y]^2\\
$$

那么梯度下降的基本形式就是

$$\theta_{n+1}=\theta_n-{\alpha}J^{'}(\theta)\\
$$

其中，$α$为学习率（learning_rate）。下一步便是要将损失函数最小化，需要对 $J(θ)$ 求导： 
$$J^{'}(\theta)=\frac{{\partial}J(\theta)}{\theta}=[h_{\theta}(x)-y]*h^{'}_{\theta}\\
$$

其中 $h^{'}_{\theta}=x$ ，所以不难得出
$$J^{'}(\theta)=[h_{\theta}(x)-y]*x \\
$$

即
$$\theta_{n+1}=\theta_n-{\alpha}[h_{\theta}(x^{(i)})-y^{(i)}]*x^{(i)}\\
$$

常见的梯度下降法包括**随机梯度下降法（SGD）**、**批梯度下降法**、**Momentum 梯度下降法**、**Nesterov Momentum 梯度下降法**、**AdaGrad 梯度下降法**、**RMSprop 梯度下降法**、**Adam 梯度下降法**。下面分别对各种梯度下降法进行介绍。

![](https://pic1.zhimg.com/v2-491a7ff6038630dae0e18e29cb7f5194_b.jpg)

## 2 随机梯度下降法

所谓随机梯度下降法，指的是每次随机从样本集中抽取一个样本对 $\theta$ 进行更新，更新公式为：

$$\theta_{n+1}=\theta_n-{\alpha}[h_{\theta}(x^{(i)})-y^{(i)}]*x^{(i)}\\
$$

这种算法如果要遍历整个样本集的话需要迭代很多次，且每次更新并不是向着最优的方向进行，所以每走一步都要 “很小心”，也就是说**随机梯度下降法的学习率α不能设置太大，不然容易出现在最优解附近 “震荡”，但始终无法更接近最优解的现象**。  
但从另一个角度来看，这种 “来回震荡” 的优化路线在损失函数局部极小值较多时，能够有效避免模型陷入局部最优解。

## 3 标准梯度下降法

标准梯度下降法则是计算样本集中损失函数的总和之后再对参数进行更新：

$$\theta_{n+1}=\theta_n-\frac{\alpha}{N}[h_{\theta}(x^{(i)})-y^{(i)}]*x^{(i)}
$$

这种算法是在遍历了整个样本集之后才对参数进行更新，因此它的下降方向是最优的方向，所以它就可以很理直气壮的走每一步。  
因此，这种算法的学习率 $\alpha$ 一般要比随机梯度下降法的大。但这种优化方法的缺点在于它每更新一次都需要遍历整个样本集，效率比较低，因为在很多时候整个样本集计算出的梯度和部分样本集计算出的梯度相差并不多。

## 4 批梯度下降法

每次随机从样本集中抽取 $M$ (batch_size) 个样本进行迭代：

$$\theta_{n+1}=\theta_n-\frac{\alpha}{M}[h_{\theta}(x^{(i)})-y^{(i)}]*x^{(i)}
$$

这种优化算法相比前两种来说，既能提高模型的准确度，又能提高算法的运行速度。

## 5 Momentum 梯度下降法

该算法又称被为动量梯度下降法，基本思想是：损失函数求最优解的过程可以看作小球从求解面（损失函数值在坐标系中呈现出的面）某处沿着该面下落直至该面最低处的过程，损失函数的梯度可以看作是施加给小球的力，通过力的作用有了速度，通过速度可以改变小球的位置。由力学定律 $F=ma$ 可知梯度与加速度成正比，加速度改变速度，便可以得到以下更新过程：

$$v={\mu}*v-{\alpha}J^{'}(\theta)
$$

$${\theta}_{n+1}={\theta}_n+v
$$

其中，$\mu$ 是动量系数，$\mu$ 值的大小，可以通过 trian-and-error 来确定，实践中常常设为 0.9。  
这种优化方式不会立刻改变梯度优化的方向，而是每一次计算后的方向为前一次优化的方向与本次计算得到的值的加权和，即梯度优化的方向是通过积累一点点改变的，而且越积累越大。这种优化方式的好处在于通过不同训练样本求得梯度时，始终都会增大最优方向上的梯度值，因此可以减少许多震荡。

## 6 Nesterov Momentum 梯度下降法

Nesterov Momentum 梯度下降法是对 Momentum 梯度下降法的一个改进，更新过程如下：
$$v={\mu}*v-{\alpha}J^{'}(\theta+\mu *v)
$$

$${\theta}_{n+1}={\theta}_n+v
$$

在 Momentum 梯度下降法中，已经求出了 $\mu ∗ v$ ，那么可以再 “向前看一步”，不是求解当前位置的梯度，而是求解 $\theta + \mu ∗ v$ 处的梯度。这个位置虽然不正确，但是要优于当前位置 $\theta$。

## 7 AdaGrad 梯度下降法

在梯度下降法中，学习率的选取十分重要，因为它关乎到优化过程每一步 “迈的大小”，迈的太大会导致在最优解附近来回震荡，迈的太小容易陷入局部最优解；且不同参数适用的学习率的大小不一样，有些参数可能已经接近最优，仅仅需要微调，需要比较小的学习率；而有些参数还需要大幅度调动。在这种场景下，AdaGrad 能够自适应不同的学习率。其更新过程如下：

$$G=G+J^{'}(\theta)^2
$$

$${\theta}_{n+1}={\theta}_n-\frac{\alpha}{\sqrt{G+\epsilon}}J^{'}(\theta)
$$

所以 RMSprop 的核心思想在于使用指数衰减滑动平均以丢弃遥远过去的历史。

## 8 Adam 梯度下降法

Adam 考虑了梯度以及梯度的平方，具有 AdaGrad 和 RMSprop 的优点。Adam 根据梯度的一阶估计和二阶估计，动态调整学习率。

$$m_t=\frac{m_{t-1}}{1-{\beta}^{t-1}_1} 
$$

$$v_t=\frac{v_{t-1}}{1-{\beta}^{t-1}_2}
$$

$${\theta}_{t+1}={\theta}_t-\frac{\eta}{\sqrt{v_t+\epsilon}}m_t$$

其中，$m(t)$ 为梯度的第一时刻平均值，$v(t)$ 为梯度的第二时刻非中心方差值，其中，$β_1$ 一般设为 0.9，$β_2$ 一般设为 0.9999，$\epsilon$ 一般设为 $10^{-8}$。 

这个方法不仅存储了 AdaDelta 先前平方梯度的指数衰减平均值，而且保持了先前梯度 M(t) 的指数衰减平均值，这一点与动量类似。

## 9 与其他无约束优化算法的比较

在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。  
梯度下降法和最小二乘法相比，**梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解**。  

如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。 但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。  
梯度下降法和牛顿法 / 拟牛顿法相比，两者都是迭代求解，**不过梯度下降法是梯度求解，而牛顿法 / 拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解**。  
相对而言，使用牛顿法 / 拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。