---
zotero-key: BF5KAJJN
zt-attachments:
  - "105"
title: "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with $1/n$ Parameters"
citekey: zhangFullyConnectedLayersQuaternions2021
date: "2021"
authors: Aston Zhang,Yi Tay,Shuai Zhang,Alvin Chan,Anh Tuan Luu,Siu Cheung Hui,Jie Fu
tags:
  - Computer Science - Artificial Intelligence
  - Computer Science - Computation and Language
  - Computer Science - Machine Learning
  - Computer Science - Computer Vision and Pattern Recognition
url: http://arxiv.org/abs/2102.08597
---
# Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with $1/n$ Parameters
>[!info] 
>- **Authors**:  [[Aston Zhang]] ,  [[Yi Tay]] ,  [[Shuai Zhang]] ,  [[Alvin Chan]] ,  [[Anh Tuan Luu]] ,  [[Siu Cheung Hui]] ,  [[Jie Fu]] >- **Date**: 2021>- **DOI**: >- **Groups**: DeepLearning>- **Tags**: #Computer-Science---Artificial-Intelligence, #Computer-Science---Computation-and-Language, #Computer-Science---Machine-Learning, #Computer-Science---Computer-Vision-and-Pattern-Recognition>- **Links**：[Zotero](zotero://select/library/items/BF5KAJJN), [attachment](<file:///home/senjl/Zotero/storage/CNYYLLHR/Zhang%20et%20al.%20-%202021%20-%20Beyond%20Fully-Connected%20Layers%20with%20Quaternions%20Pa.pdf>)
## Abstract
Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Speciﬁcally, “fully-connected layers with Quaternions” (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predeﬁned dimensions (4D, 8D, and 16D). This restricts the ﬂexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predeﬁned. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural ﬂexibility using arbitrarily 1/n learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural ﬂexibility and effectiveness of the proposed approach.
## Notes
  Comment: Published as a conference paper at the 9th International Conference on Learning Representations (ICLR 2021)
## Annotations

[Zotero](zotero://select/library/items/BF5KAJJN) [attachment](<file:///home/senjl/Zotero/storage/CNYYLLHR/Zhang%20et%20al.%20-%202021%20-%20Beyond%20Fully-Connected%20Layers%20with%20Quaternions%20Pa.pdf>)