---
zotero-key: SAXIPJFA
zt-attachments:
  - "206"
title: Data center TCP (DCTCP)
citekey: alizadehDataCenterTCP2010
date: "2010"
authors: Mohammad Alizadeh,Albert Greenberg
tags:
  - data center network
  - ECN
  - TCP
  - DCTCP
url: https://dl.acm.org/doi/10.1145/1851182.1851192
---
# Data center TCP (DCTCP)
>[!info] 
>- **Authors**:  Mohammad Alizadeh,  Albert Greenberg>- **Date**: 2010>- **DOI**: 10.1145/1851182.1851192>- **Groups**: Congestion-Control>- **Tags**: #data-center-network, #ECN, #TCP, #DCTCP>- **Links**：[Zotero](zotero://select/library/items/SAXIPJFA), [attachment](<file:///home/senjl/Zotero/storage/PKD4XAMV/Alizadeh%20et%20al_2010_Data%20center%20TCP%20(DCTCP).pdf>)
## Abstract
Cloud data centers host diverse applications, mixing workloads that require small predictable latency with others requiring large sustained throughput. In this environment, today's state-of-the-art TCP protocol falls short. We present measurements of a 6000 server production cluster and reveal impairments that lead to high application latencies, rooted in TCP's demands on the limited buffer space available in data center switches. For example, bandwidth hungry "background" flows build up queues at the switches, and thus impact the performance of latency sensitive "foreground" traffic. To address these problems, we propose DCTCP, a TCP-like protocol for data center networks. DCTCP leverages Explicit Congestion Notification (ECN) in the network to provide multi-bit feedback to the end hosts. We evaluate DCTCP at 1 and 10Gbps speeds using commodity, shallow buffered switches. We find DCTCP delivers the same or better throughput than TCP, while using 90% less buffer space. Unlike TCP, DCTCP also provides high burst tolerance and low latency for short flows. In handling workloads derived from operational measurements, we found DCTCP enables the applications to handle 10X the current background traffic, without impacting foreground traffic. Further, a 10X increase in foreground traffic does not cause any timeouts, thus largely eliminating incast problems.
## Notes

[@alizadehDataCenterTCP2010] 云数据中心承载的应用场景颇为复杂，其中既有需要较小的可预测延迟的负载，也有需要较大的持续吞吐的负载，因此如今即使最先进的TCP协议也无法满足需求。本文通过对6000台服务器生产集群的测量结果，揭示了导致应用延迟过高的缺陷，根源在于TCP对数据中心交换机中的有限缓冲空间的需求。例如对带宽要求极高的后台流量会在交换机上建立等待队列，从而影响对延迟敏感的“前台“流量的性能。

> [!note] Page 63state-of-the-art---sota，领域内最高水平的...
> ^YZZTEPIEaPKD4XAMVp1

  
[@alizadehDataCenterTCP2010] DCTCP提出了ECN（显式拥塞通知）来为网络中的终端主机提供多bit的反馈信息。我们在商用小缓冲区的交换机在1和10Gbps的速率下进行测试，发现DCTCP在使用的缓冲空间较TCP少了90%的情况下，还能获得相同甚至更好的吞吐量。并且相较于TCP，DCTCP还能对短流量提供高突发容忍度和低延时。在处理从运行测量中得到的工作负载时，DCTCP使应用程序能够处理10倍于当前的后台流量，同时还不会影响前台流量，甚至前台流量增加10倍也不会产生超时问题，这大大消除了播出问题。

> [!note] Page 63incast problems.---指多个发送方（服务器）向单个接收方（客户端）传输数据时，进入接收方缓冲区的数据包突然激增最终超过缓冲区，产生数据包丢失、延迟增加以及网络性能的整体下降。
> ^C5TTSBF7aPKD4XAMVp1

  
[@alizadehDataCenterTCP2010] 本文专注的应用是网络搜索引擎、零售电商、广告商、推荐系统等软实时（soft real-time）场景，这些应用会混合地产生长流量、短流量，因此对数据中心网络提出了三个要求：  
1\. 短流的低延时  
2\. 对高突发的容忍度  
3\. 对长流的高利用率

  
[@alizadehDataCenterTCP2010] 前两个需求（短流的低延时和突发的高容忍）源自于 **分区/聚合** （Partition/Aggregate）工作流模式（这也是许多应用采用的模式）。思路是将最终结果的real-time ddl转化为工作流中各个独立任务的延迟限时的目标。这些目标从10ms到100ms不等，如果任务在各自的ddl之前还未完成则会被取消，这就会影响到最终的结果。因此，应用对低延迟的需求直接影响到最终返回结果的质量，进而影响到盈利。减少网络延迟可以让应用的开发人员将更多时间投入到提高相关性和终端用户体验的算法中。  
第三个需求（对长流量的高效利用）源自于对这些应用内部数据结构持续更新的需求，因为这些数据的刷新同样影响着结果的质量。这三种需求同等重要。

  
[@alizadehDataCenterTCP2010] 本文主要做了两个贡献：  
1\. 测量并分析了一个月内从6000台服务器中产生的超过150TB的压缩流量，从中提取了由商用交换机组成网络的数据中心的应用的模式和需求（尤其是低延迟的需求）。我们从中认识到损害性能的因素，并将之与流量和交换机的属性建立联系。  
2\. 提出了DCTCP，它能够解决损害数据中心性能的因素以满足不同应用的需求。DCTCP 使用已经在现代商用交换机中提供的特性——ECN（显式拥塞通知）机制来实现。我们在提供ECN机制的商用交换机中对1和10Gbps速率的流量进行测量，发现DCTCP能够在我们的基准研究中提供10倍的增长。

> [!note] Page 63benchmark studies---基准研究
> ^VDP4TSQHaPKD4XAMVp1

  
[@alizadehDataCenterTCP2010] 我们的测量显示，数据中心的流量中TCP流量占99.91%。这些流量包括查询流量（大小在2KB~20KB之间）、延迟敏感的短消息流量（100KB~1MB）、吞吐敏感的长流量（1MB~100MB）。查询流量会受到在【32，13】文献中提到的存储网络的incast问题。  
  
除此之外，我们还发现与incast问题不相关的其他损害因素。由于长流量消耗了一部分甚至所有的交换区缓冲空间，导致查询和延迟敏感的短消息会经历较长延迟。  
从这些测量中能够学习到最关键的点在于，为了满足多样化的混合了短流和长流的情形下，交换机缓冲区占用率必须保持较低水平，同时还要保证长流量的高吞吐率。

  
[@alizadehDataCenterTCP2010] DCTCP 将 ECN 与新的源端控制方案相结合。它从 ECN 标记的单比特流中提取网络拥塞的多比特反馈。源端估算被标记数据包的比例，并将该估算作为拥塞程度的信号。这样，DCTCP 就能以极低的缓冲区占用率运行，同时仍能实现较高的吞吐量。

> [!note] Page 64a key requirement was that it be implementable with mechanisms in existing hardware---DCTCP的突出优势——能够在现有商用交换机的硬件基础上直接实现。
> ^JK8DRQ3YaPKD4XAMVp2

  
[@alizadehDataCenterTCP2010] DCTCP 是为数据中心环境设计的，并不一定适用于广域网。数据中心环境与广域网有很大不同。例如，  
\- 在没有队列的情况下，往返时间 (RTT) 可以小于 250μs  
\- 应用同时需要极高的带宽和极低的延迟。  
\- 通常情况下，几乎不存在统计多路复用：单个流量可能会主导某一特定路径。  
  
与此同时，数据中心环境也提供了某些便利：  
\- 网络在很大程度上是同质的，并处于单一的管理控制之下。因此，向后兼容性、增量部署和对传统协议的公平性都不是主要问题。  
\- 与外部互联网的连接通常是通过负载均衡器和应用代理来管理的，它们能有效地将内部流量与外部流量分开，因此与传统 TCP 协议的公平性问题无关。  
  
本文不讨论如何在内部流量和外部流量（至少一个端点在数据中心之外）之间分配数据中心带宽的问题。最简单的解决方案是使用以太网优先级（服务等级），在交换机上将内部流量和外部流量分开，并在数据中心严格针对内部流量进行 ECN 标记。

  
[@alizadehDataCenterTCP2010] TCP 文献浩如烟海，其中有两大类拥塞控制协议试图控制队列长度：  
1\. 基于延迟的协议将 RTT 测量值的增加作为队列延迟增长的标志，进而作为拥塞的标志。这些协议在很大程度上依赖于精确的 RTT 测量，而在数据中心的低延迟环境中，RTT 测量很容易受到噪声的影响。延迟的微小噪声波动与拥塞难以区分，因此算法可能会反应过度。  
2\. 主动队列管理（AQM）方法使用来自拥塞交换机的明确反馈。我们提出的算法就属于这一类。

> [!note] Page 64Delay-based protocols---利用RTT进行队列长度控制的协议
> ^LTEG7GJAaPKD4XAMVp2

> [!note] Page 64AQM---主动队列管理 Active Queue Management
> 🔤🔤
> ^U66QQ547aPKD4XAMVp2

> [!note] Page 64Partition/Aggregate---<b>分区/聚合 </b>结构的应用模式
> ^IBF2VUWKaPKD4XAMVp2

  
[@alizadehDataCenterTCP2010] 分区/聚合设计模式是许多大型网络应用程序的基础，架构如图二所示。**应用程序较高层的请求会被分割成若干块，然后分给较低层的处理人员。这些工作者的响应被聚合在一起，产生一个结果。**网络搜索、社交网络内容合成和广告选择都是以这种应用设计模式为基础的。  
  
对于类似的交互式软实时应用，延迟是关键指标，允许的总延迟时间由客户影响研究等因素决定。在减去典型的互联网和渲染延迟后，应用程序的 “后台 ”部分通常被分配在 230-300 毫秒之间。这一限制被称为 “全局服务水平协议”（all-up SLA）。

> [!note] Page 64all-up SLA---全局服务水平协议：all-up Service Level Agreement，指的是包含整个系统或服务的性能、可用性和可靠性指标的综合协议。
> 
> 以下是对全面 SLA 的详细描述：
> 
> 1. **全面覆盖**：
>    - 全面 SLA 涵盖服务交付的所有方面，包括硬件、软件、网络基础设施以及提供服务所涉及的任何其他组件。
>    - 它包括响应时间、吞吐量和延迟等性能指标。
> 
> 2. 2. **端到端责任**：
>    - 服务提供商对整个服务生命周期负责，从供应和维护到事件管理和解决。
>    - 这可确保最终用户体验到一致、可靠的服务性能。
> 
> 3. **统一衡量标准**：
>    - 与针对不同组件的单独 SLA（如服务器正常运行时间和网络延迟）不同，全功能 SLA 提供了一套反映服务整体性能和可用性的指标。
>    - 这些指标通常包括正常运行时间百分比、最长允许停机时间、响应时间和事件解决时间。
> 
> 4. **整体视图**：
>    - 全方位服务水平协议提供了服务健康状况和性能的整体视图，使客户更容易了解他们所获得的服务质量。
>    - 这种方法简化了服务提供商和客户的管理和监控。
> 
> 5. **责任**：
>    - 服务提供商有责任在服务的各个方面达到商定的性能标准。
>    - 这将导致更严格的内部流程和服务提供商组织内不同团队之间更好的协调，以确保遵守 SLA。
> 
> 6. **全面服务级别协议中的指标示例**：
>    - 可用性**： 服务正常运行的时间百分比（如 99.99% 的正常运行时间）。
>    - 性能**： 请求的平均响应时间、最大可接受延迟、吞吐率。
>    - 可靠性**： 服务中断的频率和持续时间、平均修复时间（MTTR）。
>    - 支持**： 支持请求的响应时间、事件的解决时间。
> ^ISDYZVXCaPKD4XAMVp2

  
[@alizadehDataCenterTCP2010] 多层的分区/聚合模式工作流程中，一层的滞后会延迟其他层的启动。此外，回答一个请求可能需要迭代地调用模式，聚合器会向下层的工作程序**发出连续请求**以准备响应（典型的迭代次数为 1 到 4 次，但也可能多达 20 次）。例如，在网络搜索中，一个查询可能会被发送给许多聚合器和工作员，每个聚合器和工作员负责索引的不同部分。根据回复，聚合器可能会改进查询并再次发送，以提高结果的相关性。  
因此，分区/聚合结构中部分的滞后实例可能会威胁到查询请求的的全局 SLA。事实上，我们发现延迟时间实际上与 SLA 的目标接近，因为**开发人员会利用所有可用的时间预算来尽可能计算出最佳结果**。  
为了防止对全局 SLA 的影响，工作节点被分配了严格的截止时间，通常为 10-100 毫秒。**如果某个节点错过了最后期限，计算就会在没有响应的情况下继续进行，从而降低计算结果的质量。**  
  
  
此外，就算是高百分位数工作者的延迟也很重要。例如，99.9% 的高延迟意味着 1000 个响应中至少有 1 个响应的结果质量较低或延迟时间较长（或两者兼而有之），这可能会影响到大量用户的用户粘性。因此，通常会对 99.9 百分位数的延迟进行跟踪，而 DDL 则与高百分位数相关。图 8 显示了一个生产监控工具的屏幕截图，其中跟踪了高百分位数。

  
[@alizadehDataCenterTCP2010] 在如此紧迫的期限内，数据中心内的网络延迟在应用设计中发挥着重要作用。许多应用程序发现，使用最先进的 TCP 协议也很难满足这些 DDL 的要求，因此开发人员通常会采用复杂的临时解决方案。例如，我们的应用程序会仔细控制每个工作者发送的数据量，并增加抖动。据报道，Facebook 甚至开发了自己的基于 UDP 的拥塞控制。

  
[@alizadehDataCenterTCP2010] 我们测量了三个生产集群中与网络搜索和其他服务相关的**工作负载**属性（上一个测量的是流量的类型）。测量结果有助于了解数据中心流量的性质，并为理解 TCP 性能不佳的原因和创建评估 DCTCP 的基准奠定基础。  
  
我们对 150 多个机架上的 6000 多台服务器进行了测量。这三个集群支持软实时 _查询流量_ ，集成了协调集群活动的 _紧急短消息流量_ 和 _持续的后台流量_ ，后者负责摄取和组织用于维持查询响应质量的海量数据（我们使用这些术语是为了便于解释和分析，开发人员并没有将流量分成简单的类集）。测量仪器会被动地收集 socket 级别的日志、选定的数据包级日志和描述延迟的应用程序级日志——在一个月内共收集了约 150TB 的压缩数据。

> [!note] Page 65Top of Rack switch---ToR交换机：rack指的是放服务器的架子，这个词的本义就是机架顶部的交换机。ToR 是浅层缓冲、共享内存交换机；每个交换机有 48 个 1Gbps 端口和两个 10Gbps 端口共享的 4MB 缓冲区。
> 
> 收集数据中，集群中的每个机架可容纳 44 台服务器。每个服务器通过 1Gbps 以太网连接到机架顶部交换机。
> ^2T4NZFYEaPKD4XAMVp3

  
[@alizadehDataCenterTCP2010] **查询流量**。集群中的查询流量遵循 “分区/聚合 ”模式。查询流量由非常短的、对延迟要求极高的流量组成，其模式如下：  
\- 高级聚合器（HLA）将查询分给大量中级聚合器（MLA），中级聚合器又将每个查询分给与 MLA 位于同一机架的其他 43 台服务器  
\- 服务器既是 MLA 又是 Worker，因此每台服务器在为某些查询充当聚合器的同时，也会为其他查询充当 Worker。  
  
图 3(a) 显示了查询到达中层聚合器的时间间隔的 CDF。查询流的大小非常有规律，从MLA到Worker的查询为 1.6KB，从Worker到MLA的响应为 1.6 到 2KB。

> [!note] Page 65HLA---高级聚合器：high-level aggregator
> 🔤n. 人体白细胞抗原；组织相容性抗原； 网络释义： 人类白细胞抗原(human leukocyte antigen)；人类白血球抗原；人白细胞抗原🔤
> ^IWCT7ERIaPKD4XAMVp3

> [!note] Page 65MLAs---中级聚合器：mid-level aggregators
> 🔤🔤
> ^2G525DJTaPKD4XAMVp3

> [!note] Page 65CDF---累积分布函数：Cumulative Distribution Function
> CDF 是一种统计工具，用于描述随机变量取值小于或等于某一阈值的概率。其实就是概率论中的<b>分布函数</b>。
> 🔤🔤
> ^ERXPHHZTaPKD4XAMVp3

  
[@alizadehDataCenterTCP2010] **后台流量**。与查询流量同时存在的还有复杂的后台流量，包括大流量和小流量。图 4 显示了后台流量大小的 PDF 值，图示说明了大多数后台流量都很小，但后台流量中的大部分字节流都是大流量的一部分。  
后台流量中最主要的是大型流量（1MB 至 50MB），即向worker复制新数据的更新流量，以及时间敏感的短消息流量（50KB 至 1MB）——用于更新工作者控制状态的流量。图 3(b) 显示了新后台流的到达间隔时间。  
  
后台流的到达间隔时间反映了支持应用程序的多种不同服务的叠加和多样性：  
(1) 到达间隔时间的方差非常大，尾部非常重；  
(2) 出现嵌入式峰值，例如 0ms 的到达间隔时间解释了直到第 50 百分位数的 CDF 曲线；  
(3) 周期性地出现相对较多的流出流，这是由于worker周期性地轮询大量对等设备以寻找更新文件。

> [!note] Page 65PDF---概率密度函数：Probability Density Function
> 🔤n. 可移植文档格式； 网络释义： 下载；文件格式；可移植文档格式(Portable Document Format)🔤
> ^RQG3IU77aPKD4XAMVp3

  
[@alizadehDataCenterTCP2010] **流量并发和大小**。图 5 显示了MLA或worker并发参与的流量数的分布函数（定义为 50 毫秒窗口内活动的流量数）。当考虑所有流量时，并发流量的中位数为 36，这是由于 “分区/聚合 ”流量模式的广度造成的，其中每个服务器与 43 个其他服务器通信。第 99.99 百分位数超过 1,600 个，有一台服务器的中位数为 1,200 个连接。  
  
如果**只考虑大流量（大于 1MB），统计多路复用程度非常低**——并发大流量的中位数为 1，第 75 百分位数为 2。然而，这些流量足够大，足以持续数个 RTT，并可能因队列堆积而消耗大量缓冲空间。  
  
总之，**对吞吐量敏感的大流量、对延迟敏感的短流量和突发查询流量在数据中心网络中并存**。

  
[@alizadehDataCenterTCP2010] 与大多数商品交换机一样，这些集群中的交换机也是**共享内存交换机**，旨在通过使用所有交换机端口都可用的逻辑上通用的数据包缓冲区来利用统计多路复用的增益。  
到达某个接口的数据包被存储到所有接口共享的高速多端口内存中。共享池中的内存由 MMU（Memory Management Unit）动态分配给数据包。MMU 通过动态调整任何一个接口可占用的最大内存量，尽量给每个接口分配其所需的内存，同时防止不公平现象。**如果一个数据包必须排队等待传出接口，但该接口已达到其最大内存分配量或共享池本身已耗尽，那么该数据包就会被丢弃**。  
  
建立大型多端口内存的成本非常昂贵，因此大多数廉价交换机都采用浅层缓冲，而数据包缓冲区是最稀缺的资源。浅层数据包缓冲区会导致三种特定的性能损害，我们将在下文中讨论。

  
[@alizadehDataCenterTCP2010] 如图 6(a) 所示，如果许多数据流在短时间内汇聚到交换机的同一接口上，数据包可能会耗尽交换机内存或该接口允许的最大缓冲区，从而导致数据包丢失。即使流量很小，也会出现这种情况。使用 “分区/聚合” 的设计模式就会产生这种流量模式，因为数据请求会同步worker的响应，并在连接到聚合器的交换机端口队列中产生 incast 问题。  
  
迄今为止发表的 incast 研究\[32, 13\]涉及精心构建的测试实验室场景。我们发现，在生产环境中确实会出现类似 incast 的问题，而且这些问题非常重要——不仅会降低性能，更重要的是会降低用户体验。问题在于，**发生 incast 的响应几乎肯定会错过聚合器的 DDL ，并被排除在最终结果之外**。  
  
我们通过数据包级监控捕获了 incast 问题的实例。图 7 显示了观察到的实例的时间轴。由于该应用中每个响应的大小仅为 2KB（2 个数据包），数据包的丢失几乎必然导致 TCP 超时。在我们的网络协议栈中，RTOmin 设置为 300 毫秒。因此，只要发生超时，响应就会错过聚合器的 DDL 。

> [!note] Page 66RT Omin---<b>重传超时</b>：The Retransmission Timeout (RTO) is a key parameter in TCP (Transmission Control Protocol) that determines how long the sender waits for an acknowledgment (ACK) of a transmitted packet before retransmitting the packet.
> ^4J4MX4IJaPKD4XAMVp4

  
[@alizadehDataCenterTCP2010] 为了避免 Worker 响应超时，开发人员对应用代码进行了两项重大修改：  
  
1\. 首先，他们特意将响应的大小限制在 2KB 以提高所有响应都适合交换机内存的几率。  
2\. 其次，开发人员添加了应用级抖动\[11\]，通过随机延迟一定时间（通常平均值为 10ms）来使响应不同步。抖动的问题在于，它通过避免超时，缩短了较高百分位数的响应时间，但代价是增加了响应时间的中位数（由于增加了延迟）。图 8 就生动地说明了这一点。  
  
降低最小超时重传的时间能够减少超时的影响，但是这并不能解决其他关键的延迟产生的根源。

  
[@alizadehDataCenterTCP2010] 持久的、贪婪的 TCP 流量会导致瓶颈队列的长度不断增加，直到数据包被丢弃，从而形成我们熟悉的锯齿模式（图 1）。当长流量和短流量穿越同一队列时，如图 6(b) 所示，会出现两种损害：  
1\. 首先，短数据流的数据包丢失会导致上述的incast问题  
2\. 其次，队列堆积（queue buildup）也会造成损害：即使没有数据包丢失，**短数据流也会因为排在大数据流数据包后面而增加延迟**（HOF阻塞问题）。由于集群中的每个worker都要处理查询流量和后台流量，因此这种流量模式出现的频率非常高。  
  
仔细观察图 7 可以发现，响应的到达时间分布在 12ms 左右。由于所有响应的总大小仅为 43 × 2KB = 86KB，在 1Gbps 的速度下大约需要 1 毫秒的传输时间，因此在这种传输中出现任何传输损失都是令人难以接受的。然而，关键问题在于其他流量（后台流量）对队列的占用，当长流量和短流量同时出现时，就会产生损耗。

  
[@alizadehDataCenterTCP2010] 为了确定长流量会影响查询响应的延迟，我们测量了worker和聚合器之间的 RTT：这是worker发送响应到从聚合器收到 TCP ACK 的时间，在图 7 中标注为 “RTT+队列”。我们测得，在没有队列的情况下，机架内的 RTT 约为 100μs，而机架间的 RTT 不到 250μs。这意味着 **“RTT+队列 ”可以很好地衡量在汇聚器收集响应期间前往汇聚器的数据包队列长度**。图 9 中的 CDF 是 19K 次测量的队列长度分布。它显示，90% 的时间响应数据包的队列长度小于 1 毫秒，10% 的时间响应数据包的队列长度在 1 到 14 毫秒之间（14 毫秒是动态缓冲区的最大值）。  
  
这表明，查询流确实出现了排队延迟。此外，需要注意的是，回答一个请求可能需要多次迭代，这就放大了这种延迟的影响。  
  
请注意，这种延迟与incast问题无关——没有数据包丢失，因此减少最小超时重传时间不会有任何帮助。此外，甚至不需要有很多同步短流。**既然延迟是由队列造成的，那么唯一的解决办法就是减小队列的规模。**

  
[@alizadehDataCenterTCP2010] 考虑到数据中心中长流量和短流量的混合情况，一个端口上的短流量受到许多其他端口上活动的影响是非常常见的，如图 6(c) 所示。事实上，在这种流量模式下，**短流量的损失率取决于穿越其他端口的长流量的数量**。这是因为不同端口上的活动是由共享内存池耦合的。  
  
贪婪的 TCP 长流量会在其端口上建立队列。由于缓冲空间是一种共享资源，队列的建立减少了可用于吸收来自分区/聚合流量的突发流量的缓冲空间。我们将这种损害称为缓冲区压力。其结果就是数据包丢失和超时，就像incast我呢提一样，但不需要同步进入端口流（超出缓冲区限制）。

> [!note] Page 67buffer pressure---由于共享的缓冲区资源的稀缺，当TCP在端口上建立连接时，队列的建立占用了有限的缓冲区资源，于是增加了<b>缓冲区的压力</b>。
> ^H7IJK7QJaPKD4XAMVp5

  
[@alizadehDataCenterTCP2010] DCTCP 的设计灵感来自第 2.3 节所述的导致数据中心性能损伤的因素，其目标是利用商用浅层缓冲交换机实现高突发容限、低延迟和高吞吐量。为此，DCTCP 被设计为**在不损失吞吐量的情况下，以较小的队列占用率运行**。  
  
DCTCP 主要通过根据拥塞程度对拥塞做出反应来实现这些目标。DCTCP 在交换机上使用一种简单的标记方案，**一旦缓冲区占用率超过一个固定的小阈值，就会设置数据包的拥塞经验（CE）编码点**。DCTCP 源会据此做出反应，将窗口缩小一个系数，该系数取决于被标记数据包的比例：被标记数据包的比例越大，缩小窗口的系数越大。（意即，拥塞情况越严重，就越需要缩小发送窗口）  
  
由于 DCTCP 只要求网络提供单比特反馈，因此我们可以重新使用现代 TCP 协议栈和交换机中已有的 ECN 机制。  
  
基于延迟的拥塞控制算法 \[5, 31\] 也采用了根据拥塞程度按比例做出反应的思想。事实上，我们可以将路径延迟信息视为隐含的多比特反馈。然而，在数据传输速率极高、延迟极低的网络结构中，感知浅缓冲交换机中的队列堆积可能会产生极大的噪声。例如，10 个数据包的积压在 1Gbps 速率下会造成 120μs 的队列延迟，而在 10Gbps 速率下仅会造成 12μs 的队列延迟。要精确测量队列延迟的这种微小增长，对当今的服务器来说是一项艰巨的任务。

> [!note] Page 67Congestion Experienced (CE)---用于标记数据包，当含有CE标记的数据包的比例越大，缩小发送窗口的程度也越大。
> ^8YNWP2KUaPKD4XAMVp5

  
[@alizadehDataCenterTCP2010] 在没有大规模统计多路复用的情况下，尤其需要根据拥塞程度做出相应的反应。标准 TCP 在收到 ECN 通知时，会将窗口大小缩小 2 倍。实际上，TCP 反应的是拥塞的存在，而不是拥塞的程度。窗口减半会导致链路输入速率与可用容量之间出现严重不匹配。在高速数据中心环境中，只有少量流量共享缓冲区（第 2.2 节），这会导致缓冲区溢出和吞吐量损失。

  
[@alizadehDataCenterTCP2010] DCTCP 算法有三个主要组成部分：  
  
(1) **_交换机的简单标记_**： DCTCP 只有一个参数，即标记阈值 K。如果数据包在到达时队列占用率大于 K，则用 CE 编码点标记；否则，就不标记。这种方案可确保信源端迅速收到队列超负荷的通知。大多数现代交换机实现的 RED 标记方案都可以重新用于 DCTCP。我们只需**将低阈值和高阈值都设为 K，并根据瞬时队列长度而不是平均队列长度进行标记**。  
  
  
(2) **_接收机上的 ECN-Echo_**： DCTCP 接收方与 TCP 接收方的唯一区别在于将 CE 编码点的信息回送给发送端的方式。RFC 3168 规定，接收方在一系列 ACK 数据包中设置 ECN-Echo 标志，直到收到发送方（通过 CWR 标志）确认已收到拥塞通知。然而，**DCTCP 接收方会尝试将被标记数据包的确切序列准确地传送回发送方**。最简单的方法是对每个数据包进行 ACK，只有当数据包有标记的 CE 编码点时，才设置 ECN-Echo 标志。  
不过，由于多种原因，使用延迟 ACK （累积ACK）是很重要的，其中包括减少数据发送方的负载。要使用延迟 ACK（每连续接收 m 个数据包才发送一个累积 ACK），DCTCP 接收方使用图 10 所示的两个状态机来决定是否设置 ECN Echo 位。这两个状态对应于最后收到的数据包是否标记了 CE 编码点。由于发送方知道每个 ACK 包含多少个数据包，因此可以准确地重建接收方看到的标记运行。  
  
  
(3) **_发送端控制器_**： 发送方维护一个被标记数据包的比例的估计值，称为 α，每个数据窗口（大约一个 RTT）更新一次，具体如下：  
α ← (1 - g) × α + g × F，  
  
其中，F 是上一个数据窗口中被标记的数据包的比例，0 < g < 1 是在估计 α 时新样本与过去样本的权重。鉴于当队列长度大于 K 时，发送者会收到每个数据包的标记，而当队列长度小于 K 时，发送者不会收到任何标记，此式意味着 **α 估算了队列大小大于 K 的概率**。

> [!note] Page 67RED---随机早期检测：Random Early Detection is a congestion avoidance mechanism used in network routers and switches. RED aims to detect and mitigate congestion before it becomes severe by proactively managing packet queues.
> 🔤n. 红色；红葡萄酒；左翼人士；激进分子； adj. 红的；红色的；充血的；布满血丝的； 网络释义： 红(redness)；赤焰战场；红颜色🔤
> ^MJLWGGIVaPKD4XAMVp5

> [!note] Page 67CWR---拥塞窗口减小：Congestion Window Reduced is a control flag used in TCP (Transmission Control Protocol) to signal that the sender has received and responded to an Explicit Congestion Notification (ECN) from the network.
> 
> The CWR flag ensures that both the sender and the receiver are synchronized in their understanding of the network congestion state.
> 🔤🔤
> ^LHC5WG24aPKD4XAMVp5

  
[@alizadehDataCenterTCP2010] 先前关于小缓冲区系统中拥塞控制的工作已经观察到，**在高线路速率下，队列大小波动变得非常快，以至于无法控制队列大小，只能控制其分布**。α 的物理意义与这一观察结果一致：它表示瓶颈链路上队列大小分布的一个点。  
  
DCTCP 发送方和 TCP 发送方之间的唯一区别在于，每个发送方对接收到设置了ECN Echo标志的ACK的反应。TCP的其他功能，如缓慢启动、避免拥塞的增加或从丢失的数据包中恢复等机制，都保持不变。虽然TCP总是将其窗口大小减少2倍，以响应标记的ACK，但DCTCP使用与 α 相关的下式来计算窗口：  
  
cwnd⬅cwnd×(1-α/2)  
因此，当α接近0（低拥塞）时，窗口仅略微减小。换言之，一旦队列超过K，DCTCP发送方就会开始慢慢减少其窗口。这就是DCTCP在保持低队列长度的同时确保高吞吐量的方式。当拥塞很高时（α=1），DCTCP将窗口减半，就像TCP一样。

  
[@alizadehDataCenterTCP2010] DCTCP 缓解 §2.3 节中讨论的三种损伤的方式如下：  
\- **_队列建立_**：一旦接口上的队列长度超过K，DCTCP 发送端就会开始响应。这**减少了发生拥塞的交换机端口上的排队延迟，从而最大限度地减少了长流量对小流量完成时间的影响**。此外，更多的缓冲区空间可用作吸收瞬时的微小突发，极大地减轻了可能导致超时的昂贵数据包丢失。  
\- **_缓冲区压力_**：DCTCP还解决了缓冲区压力问题，因为拥塞端口的队列长度不会增长得太大。因此，在共享内存交换机中，少数拥塞端口不会耗尽缓冲区资源，损害通过其他端口的流。  
  
\- **_Incast 问题_**：Incast场景是最难处理的，因为是大量同步的小流量冲击同一队列。如果小流的数量过多，那么即使每个流中只有1个数据包也会淹没同步突发上的缓冲区，那么DCTCP或任何不试图调度流量的拥塞控制方案都无法避免数据包丢失。然而，在实践中，每个流都有几个数据包要传输，并且它们的窗口建立在多个RTT上。通常是在随后的RTT中突发导致丢包。由于DCTCP很早就开始标记（并且基于瞬时队列长度），因此DCTCP源端在前一个或两个RTT期间接收到足够的标记，以控制后续突发的大小。这样可以防止缓冲区溢出和由此产生的超时。
## Annotations

[Zotero](zotero://select/library/items/SAXIPJFA) [attachment](<file:///home/senjl/Zotero/storage/PKD4XAMV/Alizadeh%20et%20al_2010_Data%20center%20TCP%20(DCTCP).pdf>)> [!note] Page 63![[6-Scholar/ZtImgExcerpt/SRV67EIK.png]]---云数据中心承载的应用场景颇为复杂，其中既有需要较小的可预测延迟的负载，也有需要较大的持续吞吐的负载，因此如今即使最先进的TCP协议也无法满足需求。本文通过对6000台服务器生产集群的测量结果，揭示了导致应用延迟过高的缺陷，根源在于TCP对数据中心交换机中的有限缓冲空间的需求。例如对带宽要求极高的后台流量会在交换机上建立等待队列，从而影响对延迟敏感的“前台“流量的性能。
> ^SRV67EIKaPKD4XAMVp1> [!note] Page 63state-of-the-art---sota，领域内最高水平的...
> ^YZZTEPIEaPKD4XAMVp1> [!note] Page 63![[6-Scholar/ZtImgExcerpt/TKMX9CFZ.png]]---DCTCP提出了ECN（显式拥塞通知）来为网络中的终端主机提供多bit的反馈信息。我们在商用小缓冲区的交换机在1和10Gbps的速率下进行测试，发现DCTCP在使用的缓冲空间较TCP少了90%的情况下，还能获得相同甚至更好的吞吐量。并且相较于TCP，DCTCP还能对短流量提供高突发容忍度和低延时。在处理从运行测量中得到的工作负载时，DCTCP使应用程序能够处理10倍于当前的后台流量，同时还不会影响前台流量，甚至前台流量增加10倍也不会产生超时问题，这大大消除了播出问题。
> ^TKMX9CFZaPKD4XAMVp1> [!note] Page 63incast problems.---指多个发送方（服务器）向单个接收方（客户端）传输数据时，进入接收方缓冲区的数据包突然激增最终超过缓冲区，产生数据包丢失、延迟增加以及网络性能的整体下降。
> ^C5TTSBF7aPKD4XAMVp1> [!note] Page 63![[6-Scholar/ZtImgExcerpt/N7EVGA9K.png]]---本文专注的应用是网络搜索引擎、零售电商、广告商、推荐系统等软实时（soft real-time）场景，这些应用会混合地产生长流量、短流量，因此对数据中心网络提出了三个要求：
> 1. 短流的低延时
> 2. 对高突发的容忍度
> 3. 对长流的高利用率
> ^N7EVGA9KaPKD4XAMVp1> [!note] Page 63![[6-Scholar/ZtImgExcerpt/C9L5LK9F.png]]---前两个需求（短流的低延时和突发的高容忍）源自于 <b>分区/聚合 </b>（Partition/Aggregate）工作流模式（这也是许多应用采用的模式）。思路是将最终结果的real-time ddl转化为工作流中各个独立任务的延迟限时的目标。这些目标从10ms到100ms不等，如果任务在各自的ddl之前还未完成则会被取消，这就会影响到最终的结果。因此，应用对低延迟的需求直接影响到最终返回结果的质量，进而影响到盈利。减少网络延迟可以让应用的开发人员将更多时间投入到提高相关性和终端用户体验的算法中。
> 第三个需求（对长流量的高效利用）源自于对这些应用内部数据结构持续更新的需求，因为这些数据的刷新同样影响着结果的质量。这三种需求同等重要。
> ^C9L5LK9FaPKD4XAMVp1> [!note] Page 63![[6-Scholar/ZtImgExcerpt/P26Y4B7T.png]]---本文主要做了两个贡献：
> 1. 测量并分析了一个月内从6000台服务器中产生的超过150TB的压缩流量，从中提取了由商用交换机组成网络的数据中心的应用的模式和需求（尤其是低延迟的需求）。我们从中认识到损害性能的因素，并将之与流量和交换机的属性建立联系。
> 2. 提出了DCTCP，它能够解决损害数据中心性能的因素以满足不同应用的需求。DCTCP 使用已经在现代商用交换机中提供的特性——ECN（显式拥塞通知）机制来实现。我们在提供ECN机制的商用交换机中对1和10Gbps速率的流量进行测量，发现DCTCP能够在我们的基准研究中提供10倍的增长。
> ^P26Y4B7TaPKD4XAMVp1> [!note] Page 63benchmark studies---基准研究
> ^VDP4TSQHaPKD4XAMVp1> [!note] Page 63![[6-Scholar/ZtImgExcerpt/2N3CE8KS.png]]---我们的测量显示，数据中心的流量中TCP流量占99.91%。这些流量包括查询流量（大小在2KB~20KB之间）、延迟敏感的短消息流量（100KB~1MB）、吞吐敏感的长流量（1MB~100MB）。查询流量会受到在【32，13】文献中提到的存储网络的incast问题。
> 
> 除此之外，我们还发现与incast问题不相关的其他损害因素。由于长流量消耗了一部分甚至所有的交换区缓冲空间，导致查询和延迟敏感的短消息会经历较长延迟。
> 从这些测量中能够学习到最关键的点在于，为了满足多样化的混合了短流和长流的情形下，交换机缓冲区占用率必须保持较低水平，同时还要保证长流量的高吞吐率。
> ^2N3CE8KSaPKD4XAMVp1> [!note] Page 64![[6-Scholar/ZtImgExcerpt/HS445CGJ.png]]---DCTCP 将 ECN 与新的源端控制方案相结合。它从 ECN 标记的单比特流中提取网络拥塞的多比特反馈。源端估算被标记数据包的比例，并将该估算作为拥塞程度的信号。这样，DCTCP 就能以极低的缓冲区占用率运行，同时仍能实现较高的吞吐量。
> ^HS445CGJaPKD4XAMVp2> [!note] Page 64a key requirement was that it be implementable with mechanisms in existing hardware---DCTCP的突出优势——能够在现有商用交换机的硬件基础上直接实现。
> ^JK8DRQ3YaPKD4XAMVp2> [!note] Page 64![[6-Scholar/ZtImgExcerpt/GSYIUGWB.png]]---DCTCP 是为数据中心环境设计的，并不一定适用于广域网。数据中心环境与广域网有很大不同。例如，
> - 在没有队列的情况下，往返时间 (RTT) 可以小于 250μs
> - 应用同时需要极高的带宽和极低的延迟。
> - 通常情况下，几乎不存在统计多路复用：单个流量可能会主导某一特定路径。
> 
> 与此同时，数据中心环境也提供了某些便利：
> - 网络在很大程度上是同质的，并处于单一的管理控制之下。因此，向后兼容性、增量部署和对传统协议的公平性都不是主要问题。
> - 与外部互联网的连接通常是通过负载均衡器和应用代理来管理的，它们能有效地将内部流量与外部流量分开，因此与传统 TCP 协议的公平性问题无关。
> 
> 本文不讨论如何在内部流量和外部流量（至少一个端点在数据中心之外）之间分配数据中心带宽的问题。最简单的解决方案是使用以太网优先级（服务等级），在交换机上将内部流量和外部流量分开，并在数据中心严格针对内部流量进行 ECN 标记。
> ^GSYIUGWBaPKD4XAMVp2> [!note] Page 64![[6-Scholar/ZtImgExcerpt/UQ4BDIIB.png]]---TCP 文献浩如烟海，其中有两大类拥塞控制协议试图控制队列长度：
> 1. 基于延迟的协议将 RTT 测量值的增加作为队列延迟增长的标志，进而作为拥塞的标志。这些协议在很大程度上依赖于精确的 RTT 测量，而在数据中心的低延迟环境中，RTT 测量很容易受到噪声的影响。延迟的微小噪声波动与拥塞难以区分，因此算法可能会反应过度。
> 2. 主动队列管理（AQM）方法使用来自拥塞交换机的明确反馈。我们提出的算法就属于这一类。
> ^UQ4BDIIBaPKD4XAMVp2> [!note] Page 64Delay-based protocols---利用RTT进行队列长度控制的协议
> ^LTEG7GJAaPKD4XAMVp2> [!note] Page 64AQM---主动队列管理 Active Queue Management
> 🔤🔤
> ^U66QQ547aPKD4XAMVp2> [!note] Page 64Partition/Aggregate---<b>分区/聚合 </b>结构的应用模式
> ^IBF2VUWKaPKD4XAMVp2> [!note] Page 64![[6-Scholar/ZtImgExcerpt/PAQK4VXA.png]]---分区/聚合设计模式是许多大型网络应用程序的基础，架构如图二所示。<b>应用程序较高层的请求会被分割成若干块，然后分给较低层的处理人员。这些工作者的响应被聚合在一起，产生一个结果。</b>网络搜索、社交网络内容合成和广告选择都是以这种应用设计模式为基础的。
> 
> 对于类似的交互式软实时应用，延迟是关键指标，允许的总延迟时间由客户影响研究等因素决定。在减去典型的互联网和渲染延迟后，应用程序的 “后台 ”部分通常被分配在 230-300 毫秒之间。这一限制被称为 “全局服务水平协议”（all-up SLA）。
> ^PAQK4VXAaPKD4XAMVp2> [!note] Page 64all-up SLA---全局服务水平协议：all-up Service Level Agreement，指的是包含整个系统或服务的性能、可用性和可靠性指标的综合协议。
> 
> 以下是对全面 SLA 的详细描述：
> 
> 1. **全面覆盖**：
>    - 全面 SLA 涵盖服务交付的所有方面，包括硬件、软件、网络基础设施以及提供服务所涉及的任何其他组件。
>    - 它包括响应时间、吞吐量和延迟等性能指标。
> 
> 2. 2. **端到端责任**：
>    - 服务提供商对整个服务生命周期负责，从供应和维护到事件管理和解决。
>    - 这可确保最终用户体验到一致、可靠的服务性能。
> 
> 3. **统一衡量标准**：
>    - 与针对不同组件的单独 SLA（如服务器正常运行时间和网络延迟）不同，全功能 SLA 提供了一套反映服务整体性能和可用性的指标。
>    - 这些指标通常包括正常运行时间百分比、最长允许停机时间、响应时间和事件解决时间。
> 
> 4. **整体视图**：
>    - 全方位服务水平协议提供了服务健康状况和性能的整体视图，使客户更容易了解他们所获得的服务质量。
>    - 这种方法简化了服务提供商和客户的管理和监控。
> 
> 5. **责任**：
>    - 服务提供商有责任在服务的各个方面达到商定的性能标准。
>    - 这将导致更严格的内部流程和服务提供商组织内不同团队之间更好的协调，以确保遵守 SLA。
> 
> 6. **全面服务级别协议中的指标示例**：
>    - 可用性**： 服务正常运行的时间百分比（如 99.99% 的正常运行时间）。
>    - 性能**： 请求的平均响应时间、最大可接受延迟、吞吐率。
>    - 可靠性**： 服务中断的频率和持续时间、平均修复时间（MTTR）。
>    - 支持**： 支持请求的响应时间、事件的解决时间。
> ^ISDYZVXCaPKD4XAMVp2> [!note] Page 64![[6-Scholar/ZtImgExcerpt/U4PRC92H.png]]---多层的分区/聚合模式工作流程中，一层的滞后会延迟其他层的启动。此外，回答一个请求可能需要迭代地调用模式，聚合器会向下层的工作程序<b>发出连续请求</b>以准备响应（典型的迭代次数为 1 到 4 次，但也可能多达 20 次）。例如，在网络搜索中，一个查询可能会被发送给许多聚合器和工作员，每个聚合器和工作员负责索引的不同部分。根据回复，聚合器可能会改进查询并再次发送，以提高结果的相关性。
> 因此，分区/聚合结构中部分的滞后实例可能会威胁到查询请求的的全局 SLA。事实上，我们发现延迟时间实际上与 SLA 的目标接近，因为<b>开发人员会利用所有可用的时间预算来尽可能计算出最佳结果</b>。
> 为了防止对全局 SLA 的影响，工作节点被分配了严格的截止时间，通常为 10-100 毫秒。<b>如果某个节点错过了最后期限，计算就会在没有响应的情况下继续进行，从而降低计算结果的质量。</b>
> 
> 
> 此外，就算是高百分位数工作者的延迟也很重要。例如，99.9% 的高延迟意味着 1000 个响应中至少有 1 个响应的结果质量较低或延迟时间较长（或两者兼而有之），这可能会影响到大量用户的用户粘性。因此，通常会对 99.9 百分位数的延迟进行跟踪，而 DDL 则与高百分位数相关。图 8 显示了一个生产监控工具的屏幕截图，其中跟踪了高百分位数。
> ^U4PRC92HaPKD4XAMVp2> [!note] Page 65![[6-Scholar/ZtImgExcerpt/HRL9BF64.png]]---在如此紧迫的期限内，数据中心内的网络延迟在应用设计中发挥着重要作用。许多应用程序发现，使用最先进的 TCP 协议也很难满足这些 DDL 的要求，因此开发人员通常会采用复杂的临时解决方案。例如，我们的应用程序会仔细控制每个工作者发送的数据量，并增加抖动。据报道，Facebook 甚至开发了自己的基于 UDP 的拥塞控制。
> ^HRL9BF64aPKD4XAMVp3> [!note] Page 65![[6-Scholar/ZtImgExcerpt/U675KKBU.png]]---我们测量了三个生产集群中与网络搜索和其他服务相关的<b>工作负载</b>属性（上一个测量的是流量的类型）。测量结果有助于了解数据中心流量的性质，并为理解 TCP 性能不佳的原因和创建评估 DCTCP 的基准奠定基础。
> 
> 我们对 150 多个机架上的 6000 多台服务器进行了测量。这三个集群支持软实时<i> 查询流量 </i>，集成了协调集群活动的<i> 紧急短消息流量</i> 和<i> 持续的后台流量 </i>，后者负责摄取和组织用于维持查询响应质量的海量数据（我们使用这些术语是为了便于解释和分析，开发人员并没有将流量分成简单的类集）。测量仪器会被动地收集 socket 级别的日志、选定的数据包级日志和描述延迟的应用程序级日志——在一个月内共收集了约 150TB 的压缩数据。
> ^U675KKBUaPKD4XAMVp3> [!note] Page 65Top of Rack switch---ToR交换机：rack指的是放服务器的架子，这个词的本义就是机架顶部的交换机。ToR 是浅层缓冲、共享内存交换机；每个交换机有 48 个 1Gbps 端口和两个 10Gbps 端口共享的 4MB 缓冲区。
> 
> 收集数据中，集群中的每个机架可容纳 44 台服务器。每个服务器通过 1Gbps 以太网连接到机架顶部交换机。
> ^2T4NZFYEaPKD4XAMVp3> [!note] Page 65![[6-Scholar/ZtImgExcerpt/59PN6K42.png]]---<b>查询流量</b>。集群中的查询流量遵循 “分区/聚合 ”模式。查询流量由非常短的、对延迟要求极高的流量组成，其模式如下：
> - 高级聚合器（HLA）将查询分给大量中级聚合器（MLA），中级聚合器又将每个查询分给与 MLA 位于同一机架的其他 43 台服务器
> - 服务器既是 MLA 又是 Worker，因此每台服务器在为某些查询充当聚合器的同时，也会为其他查询充当 Worker。
> 
> 图 3(a) 显示了查询到达中层聚合器的时间间隔的 CDF。查询流的大小非常有规律，从MLA到Worker的查询为 1.6KB，从Worker到MLA的响应为 1.6 到 2KB。
> ^59PN6K42aPKD4XAMVp3> [!note] Page 65HLA---高级聚合器：high-level aggregator
> 🔤n. 人体白细胞抗原；组织相容性抗原； 网络释义： 人类白细胞抗原(human leukocyte antigen)；人类白血球抗原；人白细胞抗原🔤
> ^IWCT7ERIaPKD4XAMVp3> [!note] Page 65MLAs---中级聚合器：mid-level aggregators
> 🔤🔤
> ^2G525DJTaPKD4XAMVp3> [!note] Page 65CDF---累积分布函数：Cumulative Distribution Function
> CDF 是一种统计工具，用于描述随机变量取值小于或等于某一阈值的概率。其实就是概率论中的<b>分布函数</b>。
> 🔤🔤
> ^ERXPHHZTaPKD4XAMVp3> [!note] Page 65![[6-Scholar/ZtImgExcerpt/78GGVML5.png]]---<b>后台流量</b>。与查询流量同时存在的还有复杂的后台流量，包括大流量和小流量。图 4 显示了后台流量大小的 PDF 值，图示说明了大多数后台流量都很小，但后台流量中的大部分字节流都是大流量的一部分。
> 后台流量中最主要的是大型流量（1MB 至 50MB），即向worker复制新数据的更新流量，以及时间敏感的短消息流量（50KB 至 1MB）——用于更新工作者控制状态的流量。图 3(b) 显示了新后台流的到达间隔时间。
> 
> 后台流的到达间隔时间反映了支持应用程序的多种不同服务的叠加和多样性：
> (1) 到达间隔时间的方差非常大，尾部非常重；
> (2) 出现嵌入式峰值，例如 0ms 的到达间隔时间解释了直到第 50 百分位数的 CDF 曲线；
> (3) 周期性地出现相对较多的流出流，这是由于worker周期性地轮询大量对等设备以寻找更新文件。
> ^78GGVML5aPKD4XAMVp3> [!note] Page 65PDF---概率密度函数：Probability Density Function
> 🔤n. 可移植文档格式； 网络释义： 下载；文件格式；可移植文档格式(Portable Document Format)🔤
> ^RQG3IU77aPKD4XAMVp3> [!note] Page 65![[6-Scholar/ZtImgExcerpt/GWJFZJP7.png]]---<b>流量并发和大小</b>。图 5 显示了MLA或worker并发参与的流量数的分布函数（定义为 50 毫秒窗口内活动的流量数）。当考虑所有流量时，并发流量的中位数为 36，这是由于 “分区/聚合 ”流量模式的广度造成的，其中每个服务器与 43 个其他服务器通信。第 99.99 百分位数超过 1,600 个，有一台服务器的中位数为 1,200 个连接。
> 
> 如果<b>只考虑大流量（大于 1MB），统计多路复用程度非常低</b>——并发大流量的中位数为 1，第 75 百分位数为 2。然而，这些流量足够大，足以持续数个 RTT，并可能因队列堆积而消耗大量缓冲空间。
> 
> 总之，<b>对吞吐量敏感的大流量、对延迟敏感的短流量和突发查询流量在数据中心网络中并存</b>。
> ^GWJFZJP7aPKD4XAMVp3> [!note] Page 65![[6-Scholar/ZtImgExcerpt/HXV2W357.png]]---与大多数商品交换机一样，这些集群中的交换机也是<b>共享内存交换机</b>，旨在通过使用所有交换机端口都可用的逻辑上通用的数据包缓冲区来利用统计多路复用的增益。
> 到达某个接口的数据包被存储到所有接口共享的高速多端口内存中。共享池中的内存由 MMU（Memory Management Unit）动态分配给数据包。MMU 通过动态调整任何一个接口可占用的最大内存量，尽量给每个接口分配其所需的内存，同时防止不公平现象。<b>如果一个数据包必须排队等待传出接口，但该接口已达到其最大内存分配量或共享池本身已耗尽，那么该数据包就会被丢弃</b>。
> 
> 建立大型多端口内存的成本非常昂贵，因此大多数廉价交换机都采用浅层缓冲，而数据包缓冲区是最稀缺的资源。浅层数据包缓冲区会导致三种特定的性能损害，我们将在下文中讨论。
> ^HXV2W357aPKD4XAMVp3> [!note] Page 66![[6-Scholar/ZtImgExcerpt/U46M89FZ.png]]---如图 6(a) 所示，如果许多数据流在短时间内汇聚到交换机的同一接口上，数据包可能会耗尽交换机内存或该接口允许的最大缓冲区，从而导致数据包丢失。即使流量很小，也会出现这种情况。使用 “分区/聚合” 的设计模式就会产生这种流量模式，因为数据请求会同步worker的响应，并在连接到聚合器的交换机端口队列中产生 incast 问题。
> 
> 迄今为止发表的 incast 研究[32, 13]涉及精心构建的测试实验室场景。我们发现，在生产环境中确实会出现类似 incast 的问题，而且这些问题非常重要——不仅会降低性能，更重要的是会降低用户体验。问题在于，<b>发生 incast 的响应几乎肯定会错过聚合器的 DDL ，并被排除在最终结果之外</b>。
> 
> 我们通过数据包级监控捕获了 incast 问题的实例。图 7 显示了观察到的实例的时间轴。由于该应用中每个响应的大小仅为 2KB（2 个数据包），数据包的丢失几乎必然导致 TCP 超时。在我们的网络协议栈中，RTOmin 设置为 300 毫秒。因此，只要发生超时，响应就会错过聚合器的 DDL 。
> ^U46M89FZaPKD4XAMVp4> [!note] Page 66RT Omin---<b>重传超时</b>：The Retransmission Timeout (RTO) is a key parameter in TCP (Transmission Control Protocol) that determines how long the sender waits for an acknowledgment (ACK) of a transmitted packet before retransmitting the packet.
> ^4J4MX4IJaPKD4XAMVp4> [!note] Page 66![[6-Scholar/ZtImgExcerpt/QY45NHEG.png]]---为了避免 Worker 响应超时，开发人员对应用代码进行了两项重大修改：
> 
> 1. 首先，他们特意将响应的大小限制在 2KB 以提高所有响应都适合交换机内存的几率。
> 2. 其次，开发人员添加了应用级抖动[11]，通过随机延迟一定时间（通常平均值为 10ms）来使响应不同步。抖动的问题在于，它通过避免超时，缩短了较高百分位数的响应时间，但代价是增加了响应时间的中位数（由于增加了延迟）。图 8 就生动地说明了这一点。
> 
> 降低最小超时重传的时间能够减少超时的影响，但是这并不能解决其他关键的延迟产生的根源。
> ^QY45NHEGaPKD4XAMVp4> [!note] Page 66![[6-Scholar/ZtImgExcerpt/SXYVW4JW.png]]---持久的、贪婪的 TCP 流量会导致瓶颈队列的长度不断增加，直到数据包被丢弃，从而形成我们熟悉的锯齿模式（图 1）。当长流量和短流量穿越同一队列时，如图 6(b) 所示，会出现两种损害：
> 1. 首先，短数据流的数据包丢失会导致上述的incast问题
> 2. 其次，队列堆积（queue buildup）也会造成损害：即使没有数据包丢失，<b>短数据流也会因为排在大数据流数据包后面而增加延迟</b>（HOF阻塞问题）。由于集群中的每个worker都要处理查询流量和后台流量，因此这种流量模式出现的频率非常高。
> 
> 仔细观察图 7 可以发现，响应的到达时间分布在 12ms 左右。由于所有响应的总大小仅为 43 × 2KB = 86KB，在 1Gbps 的速度下大约需要 1 毫秒的传输时间，因此在这种传输中出现任何传输损失都是令人难以接受的。然而，关键问题在于其他流量（后台流量）对队列的占用，当长流量和短流量同时出现时，就会产生损耗。
> ^SXYVW4JWaPKD4XAMVp4> [!note] Page 66![[6-Scholar/ZtImgExcerpt/5C778I56.png]]---为了确定长流量会影响查询响应的延迟，我们测量了worker和聚合器之间的 RTT：这是worker发送响应到从聚合器收到 TCP ACK 的时间，在图 7 中标注为 “RTT+队列”。我们测得，在没有队列的情况下，机架内的 RTT 约为 100μs，而机架间的 RTT 不到 250μs。这意味着 <b>“RTT+队列 ”可以很好地衡量在汇聚器收集响应期间前往汇聚器的数据包队列长度</b>。图 9 中的 CDF 是 19K 次测量的队列长度分布。它显示，90% 的时间响应数据包的队列长度小于 1 毫秒，10% 的时间响应数据包的队列长度在 1 到 14 毫秒之间（14 毫秒是动态缓冲区的最大值）。
> 
> 这表明，查询流确实出现了排队延迟。此外，需要注意的是，回答一个请求可能需要多次迭代，这就放大了这种延迟的影响。
> 
> 请注意，这种延迟与incast问题无关——没有数据包丢失，因此减少最小超时重传时间不会有任何帮助。此外，甚至不需要有很多同步短流。<b>既然延迟是由队列造成的，那么唯一的解决办法就是减小队列的规模。</b>
> ^5C778I56aPKD4XAMVp4> [!note] Page 67![[6-Scholar/ZtImgExcerpt/NKLDYYNK.png]]---考虑到数据中心中长流量和短流量的混合情况，一个端口上的短流量受到许多其他端口上活动的影响是非常常见的，如图 6(c) 所示。事实上，在这种流量模式下，<b>短流量的损失率取决于穿越其他端口的长流量的数量</b>。这是因为不同端口上的活动是由共享内存池耦合的。
> 
> 贪婪的 TCP 长流量会在其端口上建立队列。由于缓冲空间是一种共享资源，队列的建立减少了可用于吸收来自分区/聚合流量的突发流量的缓冲空间。我们将这种损害称为缓冲区压力。其结果就是数据包丢失和超时，就像incast我呢提一样，但不需要同步进入端口流（超出缓冲区限制）。
> ^NKLDYYNKaPKD4XAMVp5> [!note] Page 67buffer pressure---由于共享的缓冲区资源的稀缺，当TCP在端口上建立连接时，队列的建立占用了有限的缓冲区资源，于是增加了<b>缓冲区的压力</b>。
> ^H7IJK7QJaPKD4XAMVp5> [!note] Page 67![[6-Scholar/ZtImgExcerpt/I3JQPTYK.png]]---DCTCP 的设计灵感来自第 2.3 节所述的导致数据中心性能损伤的因素，其目标是利用商用浅层缓冲交换机实现高突发容限、低延迟和高吞吐量。为此，DCTCP 被设计为<b>在不损失吞吐量的情况下，以较小的队列占用率运行</b>。
> 
> DCTCP 主要通过根据拥塞程度对拥塞做出反应来实现这些目标。DCTCP 在交换机上使用一种简单的标记方案，<b>一旦缓冲区占用率超过一个固定的小阈值，就会设置数据包的拥塞经验（CE）编码点</b>。DCTCP 源会据此做出反应，将窗口缩小一个系数，该系数取决于被标记数据包的比例：被标记数据包的比例越大，缩小窗口的系数越大。（意即，拥塞情况越严重，就越需要缩小发送窗口）
> 
> 由于 DCTCP 只要求网络提供单比特反馈，因此我们可以重新使用现代 TCP 协议栈和交换机中已有的 ECN 机制。
> 
> 基于延迟的拥塞控制算法 [5, 31] 也采用了根据拥塞程度按比例做出反应的思想。事实上，我们可以将路径延迟信息视为隐含的多比特反馈。然而，在数据传输速率极高、延迟极低的网络结构中，感知浅缓冲交换机中的队列堆积可能会产生极大的噪声。例如，10 个数据包的积压在 1Gbps 速率下会造成 120μs 的队列延迟，而在 10Gbps 速率下仅会造成 12μs 的队列延迟。要精确测量队列延迟的这种微小增长，对当今的服务器来说是一项艰巨的任务。
> ^I3JQPTYKaPKD4XAMVp5> [!note] Page 67Congestion Experienced (CE)---用于标记数据包，当含有CE标记的数据包的比例越大，缩小发送窗口的程度也越大。
> ^8YNWP2KUaPKD4XAMVp5> [!note] Page 67![[6-Scholar/ZtImgExcerpt/355YSTWH.png]]---在没有大规模统计多路复用的情况下，尤其需要根据拥塞程度做出相应的反应。标准 TCP 在收到 ECN 通知时，会将窗口大小缩小 2 倍。实际上，TCP 反应的是拥塞的存在，而不是拥塞的程度。窗口减半会导致链路输入速率与可用容量之间出现严重不匹配。在高速数据中心环境中，只有少量流量共享缓冲区（第 2.2 节），这会导致缓冲区溢出和吞吐量损失。
> ^355YSTWHaPKD4XAMVp5> [!note] Page 67![[6-Scholar/ZtImgExcerpt/K6J8QBHW.png]]---DCTCP 算法有三个主要组成部分：
> 
> (1) <i><b>交换机的简单标记</b></i>： DCTCP 只有一个参数，即标记阈值 K。如果数据包在到达时队列占用率大于 K，则用 CE 编码点标记；否则，就不标记。这种方案可确保信源端迅速收到队列超负荷的通知。大多数现代交换机实现的 RED 标记方案都可以重新用于 DCTCP。我们只需<b>将低阈值和高阈值都设为 K，并根据瞬时队列长度而不是平均队列长度进行标记</b>。
> 
> 
> (2) <i><b>接收机上的 ECN-Echo</b></i>： DCTCP 接收方与 TCP 接收方的唯一区别在于将 CE 编码点的信息回送给发送端的方式。RFC 3168 规定，接收方在一系列 ACK 数据包中设置 ECN-Echo 标志，直到收到发送方（通过 CWR 标志）确认已收到拥塞通知。然而，<b>DCTCP 接收方会尝试将被标记数据包的确切序列准确地传送回发送方</b>。最简单的方法是对每个数据包进行 ACK，只有当数据包有标记的 CE 编码点时，才设置 ECN-Echo 标志。
> 不过，由于多种原因，使用延迟 ACK （累积ACK）是很重要的，其中包括减少数据发送方的负载。要使用延迟 ACK（每连续接收 m 个数据包才发送一个累积 ACK），DCTCP 接收方使用图 10 所示的两个状态机来决定是否设置 ECN Echo 位。这两个状态对应于最后收到的数据包是否标记了 CE 编码点。由于发送方知道每个 ACK 包含多少个数据包，因此可以准确地重建接收方看到的标记运行。
> 
> 
> (3) <i><b>发送端控制器</b></i>： 发送方维护一个被标记数据包的比例的估计值，称为 α，每个数据窗口（大约一个 RTT）更新一次，具体如下：
> α ← (1 - g) × α + g × F，
> 
> 其中，F 是上一个数据窗口中被标记的数据包的比例，0 < g < 1 是在估计 α 时新样本与过去样本的权重。鉴于当队列长度大于 K 时，发送者会收到每个数据包的标记，而当队列长度小于 K 时，发送者不会收到任何标记，此式意味着 <b>α 估算了队列大小大于 K 的概率</b>。
> ^K6J8QBHWaPKD4XAMVp5> [!note] Page 67RED---随机早期检测：Random Early Detection is a congestion avoidance mechanism used in network routers and switches. RED aims to detect and mitigate congestion before it becomes severe by proactively managing packet queues.
> 🔤n. 红色；红葡萄酒；左翼人士；激进分子； adj. 红的；红色的；充血的；布满血丝的； 网络释义： 红(redness)；赤焰战场；红颜色🔤
> ^MJLWGGIVaPKD4XAMVp5> [!note] Page 67CWR---拥塞窗口减小：Congestion Window Reduced is a control flag used in TCP (Transmission Control Protocol) to signal that the sender has received and responded to an Explicit Congestion Notification (ECN) from the network.
> 
> The CWR flag ensures that both the sender and the receiver are synchronized in their understanding of the network congestion state.
> 🔤🔤
> ^LHC5WG24aPKD4XAMVp5> [!note] Page 68![[6-Scholar/ZtImgExcerpt/V2757BGK.png]]---先前关于小缓冲区系统中拥塞控制的工作已经观察到，<b>在高线路速率下，队列大小波动变得非常快，以至于无法控制队列大小，只能控制其分布</b>。α 的物理意义与这一观察结果一致：它表示瓶颈链路上队列大小分布的一个点。
> 
> DCTCP 发送方和 TCP 发送方之间的唯一区别在于，每个发送方对接收到设置了ECN Echo标志的ACK的反应。TCP的其他功能，如缓慢启动、避免拥塞的增加或从丢失的数据包中恢复等机制，都保持不变。虽然TCP总是将其窗口大小减少2倍，以响应标记的ACK，但DCTCP使用与 α 相关的下式来计算窗口：
> 
> cwnd⬅cwnd×(1-α/2)
> 因此，当α接近0（低拥塞）时，窗口仅略微减小。换言之，一旦队列超过K，DCTCP发送方就会开始慢慢减少其窗口。这就是DCTCP在保持低队列长度的同时确保高吞吐量的方式。当拥塞很高时（α=1），DCTCP将窗口减半，就像TCP一样。
> ^V2757BGKaPKD4XAMVp6> [!note] Page 68![[6-Scholar/ZtImgExcerpt/TUHCM6YZ.png]]---DCTCP 缓解 §2.3 节中讨论的三种损伤的方式如下：
> - <i><b>队列建立</b></i>：一旦接口上的队列长度超过K，DCTCP 发送端就会开始响应。这<b>减少了发生拥塞的交换机端口上的排队延迟，从而最大限度地减少了长流量对小流量完成时间的影响</b>。此外，更多的缓冲区空间可用作吸收瞬时的微小突发，极大地减轻了可能导致超时的昂贵数据包丢失。
> - <i><b>缓冲区压力</b></i>：DCTCP还解决了缓冲区压力问题，因为拥塞端口的队列长度不会增长得太大。因此，在共享内存交换机中，少数拥塞端口不会耗尽缓冲区资源，损害通过其他端口的流。
> 
> - <i><b>Incast 问题</b></i>：Incast场景是最难处理的，因为是大量同步的小流量冲击同一队列。如果小流的数量过多，那么即使每个流中只有1个数据包也会淹没同步突发上的缓冲区，那么DCTCP或任何不试图调度流量的拥塞控制方案都无法避免数据包丢失。然而，在实践中，每个流都有几个数据包要传输，并且它们的窗口建立在多个RTT上。通常是在随后的RTT中突发导致丢包。由于DCTCP很早就开始标记（并且基于瞬时队列长度），因此DCTCP源端在前一个或两个RTT期间接收到足够的标记，以控制后续突发的大小。这样可以防止缓冲区溢出和由此产生的超时。
> ^TUHCM6YZaPKD4XAMVp6